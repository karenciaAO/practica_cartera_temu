{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c536c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/temuapi/lib/python3.11/site-packages (2.3.2)\n",
      "Requirement already satisfied: pyxlsb in /opt/anaconda3/envs/temuapi/lib/python3.11/site-packages (1.0.10)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/envs/temuapi/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/temuapi/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/temuapi/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/temuapi/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/temuapi/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyxlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3fd548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20a624f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Cargando Excel .xlsb ...\n",
      "Hojas encontradas: ['DataFramePrueba', 'diccionario']\n",
      "\n",
      ">>> SHAPE crudo: (146939, 30)\n",
      ">>> Columnas (primeras 30): ['IdentificadorCliente', 'FechaEvento', 'UsabilidadCupo', 'CategoriaPrincipalCredito', 'DiasMaximosMoraCreditosGenerados', 'NumeroCreditosGPrevius', 'NumeroCreditosGCanalFPrevius', 'NumeroCreditosGEstadoActivosPrevius', 'NumeroCreditosGEstadoPagadosPrevius', 'NumeroCreditosGCanalVPrevius', 'NumeroCreditosLPrevius', 'NumeroCreditosLEstadoActivosPrevius', 'NumeroCreditosLEstadoPagadosPrevius', 'FechaVinculacionCliente', 'FechaPrimerUso', 'FechaUltimoUso', 'TotalPagosEfectuadosGlobalmentePrevius', 'TotalPagosEfectuadosLocalmentePrevius', 'CodigoAlmacenEntregaTC', 'CodigoMunicipioEntregaTC', 'TipoMunicipioEntregaTC', 'CanalMunicipioEntregaTC', 'NumeroIntentosFallidos', 'CupoAprobado', 'UsoAppWeb', 'ScoreCrediticio', 'Genero', 'Edad', 'DiasMora', 'PerdidaCartera']\n",
      "\n",
      ">>> Chequeos de integridad (fechas):\n",
      " - PrimerUso < Vinculacion: 20325 (13.8%)\n",
      " - UltimoUso  > Evento    : 0 (0.0%)\n",
      " - PrimerUso NaT           : 35874 (24.4%)\n",
      " - UltimoUso  NaT          : 35874 (24.4%)\n",
      "\n",
      "  Ejemplos PrimerUso<Vinculacion:\n",
      "  IdentificadorCliente FechaPrimerUso_dt_raw FechaVinculacionCliente_dt\n",
      "                    1            2006-12-14                 2019-09-11\n",
      "                    2            2008-07-21                 2023-08-19\n",
      "                    3            2008-04-03                 2019-03-21\n",
      "                    4            2009-07-25                 2022-05-07\n",
      "                    7            2006-12-24                 2022-08-19\n",
      "\n",
      ">>> Splits temporales:\n",
      "  train_in = 94041 filas (<= 2023-05-13)\n",
      "  valid_in = 23510 filas (<= 2023-07-29)\n",
      "  holdout  = 29388 filas (>  2023-07-29)\n",
      "\n",
      "======================\n",
      ">>> EXPERIMENTO: clip_a_vinc__clip_ultimo (PrimerUso=clip_a_vinc, UltimoUsoClipEvento=True)\n",
      ">>> Poda de columnas casi-constantes: ['Flag_UltimoUsoPosteriorEvento', 'Flag_UsabilidadCupo_NaN', 'Flag_MesesDesdeVinculacion_NaN', 'Flag_MesesDesdePrimerUso_NaN', 'Flag_DiasDesdeUltimoUso_NaN']\n",
      "[VALID[clip_a_vinc__clip_ultimo]] ROC-AUC=0.870 | PR-AUC=0.756 | Brier=0.117\n",
      "[VALID[clip_a_vinc__clip_ultimo]] Base rate y=1: 0.270 | mean(p1)=0.270\n",
      "[VALID[clip_a_vinc__clip_ultimo]] best-F1=0.670 @ thr=0.320\n",
      "[VALID[clip_a_vinc__clip_ultimo]] ConfMatrix @thr=0.320:\n",
      " [[14560  2608]\n",
      " [ 1832  4510]]\n",
      "[VALID[clip_a_vinc__clip_ultimo]] Report @thr=0.320:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.888     0.848     0.868     17168\n",
      "           1      0.634     0.711     0.670      6342\n",
      "\n",
      "    accuracy                          0.811     23510\n",
      "   macro avg      0.761     0.780     0.769     23510\n",
      "weighted avg      0.820     0.811     0.814     23510\n",
      "\n",
      "[TRAIN[clip_a_vinc__clip_ultimo]] ROC-AUC=0.884 | PR-AUC=0.747 | Brier=0.099\n",
      "[TRAIN[clip_a_vinc__clip_ultimo]] Base rate y=1: 0.220 | mean(p1)=0.219\n",
      "[TRAIN[clip_a_vinc__clip_ultimo]] best-F1=0.659 @ thr=0.343\n",
      "[TRAIN[clip_a_vinc__clip_ultimo]] ConfMatrix @thr=0.343:\n",
      " [[65825  7519]\n",
      " [ 6828 13869]]\n",
      "[TRAIN[clip_a_vinc__clip_ultimo]] Report @thr=0.343:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.906     0.897     0.902     73344\n",
      "           1      0.648     0.670     0.659     20697\n",
      "\n",
      "    accuracy                          0.847     94041\n",
      "   macro avg      0.777     0.784     0.780     94041\n",
      "weighted avg      0.849     0.847     0.848     94041\n",
      "\n",
      "\n",
      "======================\n",
      ">>> EXPERIMENTO: poner_na__clip_ultimo (PrimerUso=poner_na, UltimoUsoClipEvento=True)\n",
      ">>> Poda de columnas casi-constantes: ['Flag_UltimoUsoPosteriorEvento', 'Flag_UsabilidadCupo_NaN', 'Flag_MesesDesdeVinculacion_NaN', 'Flag_MesesDesdePrimerUso_NaN', 'Flag_DiasDesdeUltimoUso_NaN']\n",
      "[VALID[poner_na__clip_ultimo]] ROC-AUC=0.871 | PR-AUC=0.755 | Brier=0.117\n",
      "[VALID[poner_na__clip_ultimo]] Base rate y=1: 0.270 | mean(p1)=0.270\n",
      "[VALID[poner_na__clip_ultimo]] best-F1=0.671 @ thr=0.333\n",
      "[VALID[poner_na__clip_ultimo]] ConfMatrix @thr=0.333:\n",
      " [[14739  2429]\n",
      " [ 1910  4432]]\n",
      "[VALID[poner_na__clip_ultimo]] Report @thr=0.333:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.885     0.859     0.872     17168\n",
      "           1      0.646     0.699     0.671      6342\n",
      "\n",
      "    accuracy                          0.815     23510\n",
      "   macro avg      0.766     0.779     0.772     23510\n",
      "weighted avg      0.821     0.815     0.818     23510\n",
      "\n",
      "[TRAIN[poner_na__clip_ultimo]] ROC-AUC=0.884 | PR-AUC=0.745 | Brier=0.099\n",
      "[TRAIN[poner_na__clip_ultimo]] Base rate y=1: 0.220 | mean(p1)=0.219\n",
      "[TRAIN[poner_na__clip_ultimo]] best-F1=0.661 @ thr=0.333\n",
      "[TRAIN[poner_na__clip_ultimo]] ConfMatrix @thr=0.333:\n",
      " [[66343  7001]\n",
      " [ 7041 13656]]\n",
      "[TRAIN[poner_na__clip_ultimo]] Report @thr=0.333:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.904     0.905     0.904     73344\n",
      "           1      0.661     0.660     0.660     20697\n",
      "\n",
      "    accuracy                          0.851     94041\n",
      "   macro avg      0.783     0.782     0.782     94041\n",
      "weighted avg      0.851     0.851     0.851     94041\n",
      "\n",
      "\n",
      "======================\n",
      ">>> EXPERIMENTO: mantener__clip_ultimo (PrimerUso=mantener, UltimoUsoClipEvento=True)\n",
      ">>> Poda de columnas casi-constantes: ['Flag_UltimoUsoPosteriorEvento', 'Flag_UsabilidadCupo_NaN', 'Flag_MesesDesdeVinculacion_NaN', 'Flag_MesesDesdePrimerUso_NaN', 'Flag_DiasDesdeUltimoUso_NaN']\n",
      "[VALID[mantener__clip_ultimo]] ROC-AUC=0.871 | PR-AUC=0.757 | Brier=0.117\n",
      "[VALID[mantener__clip_ultimo]] Base rate y=1: 0.270 | mean(p1)=0.270\n",
      "[VALID[mantener__clip_ultimo]] best-F1=0.671 @ thr=0.322\n",
      "[VALID[mantener__clip_ultimo]] ConfMatrix @thr=0.322:\n",
      " [[14489  2679]\n",
      " [ 1798  4544]]\n",
      "[VALID[mantener__clip_ultimo]] Report @thr=0.322:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.890     0.844     0.866     17168\n",
      "           1      0.629     0.716     0.670      6342\n",
      "\n",
      "    accuracy                          0.810     23510\n",
      "   macro avg      0.759     0.780     0.768     23510\n",
      "weighted avg      0.819     0.810     0.813     23510\n",
      "\n",
      "[TRAIN[mantener__clip_ultimo]] ROC-AUC=0.884 | PR-AUC=0.746 | Brier=0.099\n",
      "[TRAIN[mantener__clip_ultimo]] Base rate y=1: 0.220 | mean(p1)=0.219\n",
      "[TRAIN[mantener__clip_ultimo]] best-F1=0.659 @ thr=0.363\n",
      "[TRAIN[mantener__clip_ultimo]] ConfMatrix @thr=0.363:\n",
      " [[67497  5847]\n",
      " [ 7641 13056]]\n",
      "[TRAIN[mantener__clip_ultimo]] Report @thr=0.363:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.898     0.920     0.909     73344\n",
      "           1      0.691     0.631     0.659     20697\n",
      "\n",
      "    accuracy                          0.857     94041\n",
      "   macro avg      0.794     0.776     0.784     94041\n",
      "weighted avg      0.853     0.857     0.854     94041\n",
      "\n",
      "\n",
      "====================== RESULTADOS DE EXPERIMENTOS (ordenado por PR-AUC VALID) ======================\n",
      "                     exp   primeruso  clip_ultimo  valid_pr_auc  valid_roc_auc  valid_brier  train_pr_auc  train_roc_auc\n",
      "   mantener__clip_ultimo    mantener         True      0.756547       0.870612     0.117141      0.745563       0.884077\n",
      "clip_a_vinc__clip_ultimo clip_a_vinc         True      0.755922       0.870066     0.117397      0.747341       0.883777\n",
      "   poner_na__clip_ultimo    poner_na         True      0.755392       0.870729     0.117097      0.744647       0.883514\n",
      "\n",
      ">>> Variante ganadora por VALID: mantener__clip_ultimo (PR-AUC=0.757 | ROC-AUC=0.871)\n",
      ">>> Poda de columnas casi-constantes: ['Flag_UltimoUsoPosteriorEvento', 'Flag_UsabilidadCupo_NaN', 'Flag_MesesDesdeVinculacion_NaN', 'Flag_MesesDesdePrimerUso_NaN', 'Flag_DiasDesdeUltimoUso_NaN']\n",
      "\n",
      ">>> Entrenando ganador en train_in y calibrando en valid_in; evaluando en holdout (nunca visto).\n",
      "[HOLDOUT (final)] ROC-AUC=0.829 | PR-AUC=0.542 | Brier=0.142\n",
      "[HOLDOUT (final)] Base rate y=1: 0.207 | mean(p1)=0.294\n",
      "[HOLDOUT (final)] best-F1=0.562 @ thr=0.361\n",
      "[HOLDOUT (final)] ConfMatrix @thr=0.361:\n",
      " [[18684  4607]\n",
      " [ 1910  4187]]\n",
      "[HOLDOUT (final)] Report @thr=0.361:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.907     0.802     0.851     23291\n",
      "           1      0.476     0.687     0.562      6097\n",
      "\n",
      "    accuracy                          0.778     29388\n",
      "   macro avg      0.692     0.744     0.707     29388\n",
      "weighted avg      0.818     0.778     0.792     29388\n",
      "\n",
      "\n",
      "[GANANCIA/LIFT por top-k%]\n",
      "  top_%  n_alertas  morosos_detectados  tasa_moros_topk  lift_vs_base\n",
      "     1        293                 211            0.720          3.47\n",
      "     2        587                 430            0.733          3.53\n",
      "     5       1469                1099            0.748          3.61\n",
      "    10       2938                1932            0.658          3.17\n",
      "    20       5877                3090            0.526          2.53\n",
      "\n",
      "[BARRIDO UMBRALES] (clase=1) primeras filas:\n",
      "  thr  prec_1  rec_1   TP    FP  FN    TN\n",
      "0.01   0.213  0.999 6093 22458   4   833\n",
      "0.02   0.216  0.999 6089 22055   8  1236\n",
      "0.03   0.231  0.994 6060 20148  37  3143\n",
      "0.04   0.250  0.987 6019 18050  78  5241\n",
      "0.05   0.250  0.987 6019 18049  78  5242\n",
      "0.06   0.271  0.975 5946 15967 151  7324\n",
      "0.07   0.272  0.974 5941 15874 156  7417\n",
      "0.08   0.286  0.965 5883 14722 214  8569\n",
      "0.09   0.311  0.943 5752 12761 345 10530\n",
      "0.10   0.311  0.943 5752 12760 345 10531\n",
      "\n",
      "[Objetivo negocio] PRECISIÓN≥0.80 @thr=1.000\n",
      "  TP=12 FP=5 FN=6085 TN=23286\n",
      "  precision1=0.706 recall1=0.002\n",
      "  %alertados=0.001   %morosos_detectados=0.000\n",
      "\n",
      "[Referencia] Máx-F1 @thr≈0.361 | precision=0.476 | recall=0.687\n",
      "\n",
      "[HOLDOUT_final] Calibración por deciles (p_mean ~ y_rate):\n",
      "            index   p_mean   y_rate    n\n",
      "(-0.001, 0.0277] 0.018355 0.011635 3180\n",
      "(0.0277, 0.0511] 0.043470 0.026555 4293\n",
      "(0.0511, 0.0873] 0.083680 0.057025 3402\n",
      " (0.0873, 0.118] 0.116821 0.061404  912\n",
      "  (0.118, 0.169] 0.147133 0.101055 2939\n",
      "   (0.169, 0.25] 0.231261 0.169288 3243\n",
      "   (0.25, 0.351] 0.302756 0.252668 2624\n",
      "  (0.351, 0.534] 0.433171 0.379801 3020\n",
      "   (0.534, 0.83] 0.712729 0.392905 3016\n",
      "     (0.83, 1.0] 0.929198 0.672345 2759\n",
      "\n",
      "[EV] Mejor umbral por valor económico: thr=0.140 | EV=8976.0 | TP=5578.0 FP=10741.0 FN=519.0 TN=12550.0\n",
      " Tip: ajusta COST_FP, COST_FN, BENEFIT_TP según tu negocio y re-ejecuta.\n",
      "\n",
      ">>> Importancias por permutación (holdout) — top 20\n",
      "                               feature  importance_mean  importance_std\n",
      "   NumeroCreditosGEstadoActivosPrevius         0.108647        0.002517\n",
      "   NumeroCreditosLEstadoActivosPrevius         0.077453        0.003501\n",
      "   NumeroCreditosGEstadoPagadosPrevius         0.047953        0.001302\n",
      "                creditos_activos_ratio         0.047895        0.002012\n",
      "   NumeroCreditosLEstadoPagadosPrevius         0.037080        0.001236\n",
      "                        UsabilidadCupo         0.029789        0.001433\n",
      "                       ScoreCrediticio         0.022705        0.000946\n",
      "                    DiasDesdeUltimoUso         0.009488        0.000501\n",
      "TotalPagosEfectuadosGlobalmentePrevius         0.009160        0.000719\n",
      " TotalPagosEfectuadosLocalmentePrevius         0.006975        0.000419\n",
      "                           ScoreBucket         0.005970        0.000448\n",
      "                 MesesDesdeVinculacion         0.005473        0.000489\n",
      "                TipoMunicipioEntregaTC         0.003923        0.000176\n",
      "                             UsoAppWeb         0.003559        0.000172\n",
      "                                  Edad         0.001568        0.000461\n",
      "             CategoriaPrincipalCredito         0.001511        0.000414\n",
      "                    Flag_PrimerUsoTemu         0.001441        0.000182\n",
      "                                Genero         0.000739        0.000150\n",
      "                NumeroIntentosFallidos         0.000401        0.000045\n",
      "       Flag_NumeroCreditosGPrevius_NaN         0.000328        0.000116\n",
      "SHAP no disponible/omitido: No module named 'shap'\n",
      "\n",
      ">>> Artefactos guardados en /Users/karenaraque/Desktop/practica_cartera_temu/artifacts_modelo\n",
      ">>> Snippet FastAPI guardado en artifacts_modelo/app_fastapi_snippet.py\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PRUEBA TÉCNICA — CIENTÍFICO DE DATOS (SISTECREDITO)\n",
    "# MODELO DE RIESGO TEMU — VALIDACIÓN TEMPORAL ESTRICTA (SIN FUGA)\n",
    "# ============================================================\n",
    "# Requiere: pandas, numpy, scikit-learn>=1.1, joblib, pyxlsb\n",
    "# Opcional (explicabilidad): shap\n",
    "# ------------------------------------------------------------\n",
    "# Entradas esperadas (hoja Excel .xlsb):\n",
    "#   - 'DataFramePrueba' con columnas (incluye target 'PerdidaCartera')\n",
    "#   - 'diccionario' (descripciones de variables)\n",
    "# ============================================================\n",
    "\n",
    "import warnings, os, sys, json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    classification_report, confusion_matrix, precision_recall_curve\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import joblib\n",
    "\n",
    "# ============================================================\n",
    "# 0) CONFIGURACIÓN GENERAL\n",
    "# ============================================================\n",
    "FILE_PATH  = \"/Users/karenaraque/Desktop/practica_cartera_temu/data/DataFramePrueba 2025_08.xlsb\"\n",
    "SHEET_DATA = \"DataFramePrueba\"\n",
    "SHEET_DICT = \"diccionario\"\n",
    "\n",
    "TARGET = \"PerdidaCartera\"    # 0/1\n",
    "RANDOM_STATE = 42\n",
    "TOP_K_LIST = [1, 2, 5, 10, 20]\n",
    "GOAL_PRECISION = 0.80  # objetivo negocio (precision en morosos)\n",
    "\n",
    "# Costeo opcional para EV (puedes editar). Unidades monetarias del negocio.\n",
    "COST_FP   = 1.0   # costo de intervenir a un no-moroso\n",
    "COST_FN   = 5.0   # pérdida si NO intervienes a un moroso\n",
    "BENEFIT_TP= 4.0   # beneficio por intervenir a un moroso (recuperación)\n",
    "COST_TN   = 0.0   # usualmente 0\n",
    "\n",
    "ARTIF_DIR = Path(\"./artifacts_modelo\"); ARTIF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "# ============================================================\n",
    "# 1) INGESTA Y SANITY CHECKS\n",
    "# ============================================================\n",
    "print(\">>> Cargando Excel .xlsb ...\")\n",
    "try:\n",
    "    excel_hojas = pd.ExcelFile(FILE_PATH, engine=\"pyxlsb\")\n",
    "    print(\"Hojas encontradas:\", excel_hojas.sheet_names)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"No pude abrir el archivo .xlsb en {FILE_PATH}: {e}\")\n",
    "\n",
    "clientes = pd.read_excel(FILE_PATH, sheet_name=SHEET_DATA, engine=\"pyxlsb\")\n",
    "try:\n",
    "    diccionario = pd.read_excel(FILE_PATH, sheet_name=SHEET_DICT, engine=\"pyxlsb\")\n",
    "except Exception:\n",
    "    diccionario = None\n",
    "    print(\"Aviso: hoja 'diccionario' no disponible, continuo sin ella.\")\n",
    "\n",
    "print(\"\\n>>> SHAPE crudo:\", clientes.shape)\n",
    "print(\">>> Columnas (primeras 30):\", list(clientes.columns)[:30])\n",
    "\n",
    "# ============================================================\n",
    "# 2) SANITY CHECKS de fechas (solo reporta, no transforma aún)\n",
    "# ============================================================\n",
    "def parse_dates_raw(df):\n",
    "    out = df.copy()\n",
    "    # FechaEvento se asume ISO-like (string). Intentamos parsear con UTC seguro.\n",
    "    out['FechaEvento_dt'] = pd.to_datetime(out['FechaEvento'], errors='coerce', utc=True).dt.tz_convert(None)\n",
    "\n",
    "    # Excel serial dates para otras fechas:\n",
    "    out['FechaVinculacionCliente_dt'] = pd.to_datetime(out['FechaVinculacionCliente'], errors='coerce',\n",
    "                                                       origin='1899-12-30', unit='D')\n",
    "    out['FechaUltimoUso_dt_raw']      = pd.to_datetime(out['FechaUltimoUso'], errors='coerce',\n",
    "                                                       origin='1899-12-30', unit='D')\n",
    "    # PrimerUso a veces viene en 1904-epoch en algunos exportes\n",
    "    fpu = pd.to_datetime(out['FechaPrimerUso'], errors='coerce', origin='1899-12-30', unit='D')\n",
    "    fpu_alt = pd.to_datetime(out['FechaPrimerUso'], errors='coerce', origin='1904-01-01', unit='D')\n",
    "    # elegimos el que tenga más fechas razonables (no NaT y no < 1900)\n",
    "    valid_a = fpu.notna() & (fpu.dt.year >= 1900)\n",
    "    valid_b = fpu_alt.notna() & (fpu_alt.dt.year >= 1900)\n",
    "    out['FechaPrimerUso_dt_raw'] = np.where(valid_a | (~valid_b),\n",
    "                                            fpu, fpu_alt)\n",
    "    out['FechaPrimerUso_dt_raw'] = pd.to_datetime(out['FechaPrimerUso_dt_raw'], errors='coerce')\n",
    "    return out\n",
    "\n",
    "def report_date_anomalies(df):\n",
    "    print(\"\\n>>> Chequeos de integridad (fechas):\")\n",
    "    n = len(df)\n",
    "    c1 = (df['FechaPrimerUso_dt_raw'] < df['FechaVinculacionCliente_dt']).sum()\n",
    "    c2 = (df['FechaUltimoUso_dt_raw'] > df['FechaEvento_dt']).sum()\n",
    "    c3 = df['FechaPrimerUso_dt_raw'].isna().sum()\n",
    "    c4 = df['FechaUltimoUso_dt_raw'].isna().sum()\n",
    "    print(f\" - PrimerUso < Vinculacion: {c1} ({c1/n:.1%})\")\n",
    "    print(f\" - UltimoUso  > Evento    : {c2} ({c2/n:.1%})\")\n",
    "    print(f\" - PrimerUso NaT           : {c3} ({c3/n:.1%})\")\n",
    "    print(f\" - UltimoUso  NaT          : {c4} ({c4/n:.1%})\")\n",
    "    # Lugares/fechas extremas para inspección\n",
    "    if c1>0:\n",
    "        ej = df.loc[df['FechaPrimerUso_dt_raw'] < df['FechaVinculacionCliente_dt'],\n",
    "                    ['IdentificadorCliente','FechaPrimerUso_dt_raw','FechaVinculacionCliente_dt']].head(5)\n",
    "        print(\"\\n  Ejemplos PrimerUso<Vinculacion:\\n\", ej.to_string(index=False))\n",
    "    if c2>0:\n",
    "        ej2 = df.loc[df['FechaUltimoUso_dt_raw'] > df['FechaEvento_dt'],\n",
    "                     ['IdentificadorCliente','FechaUltimoUso_dt_raw','FechaEvento_dt']].head(5)\n",
    "        print(\"\\n  Ejemplos UltimoUso>Evento:\\n\", ej2.to_string(index=False))\n",
    "\n",
    "clientes = parse_dates_raw(clientes)\n",
    "report_date_anomalies(clientes)\n",
    "\n",
    "# ============================================================\n",
    "# 3) FEATURE ENGINEERING (con variantes de tratamiento de fechas)\n",
    "# ============================================================\n",
    "EXCLUDE_FROM_FEATURES = {\n",
    "    'IdentificadorCliente','DiasMora',\n",
    "    'FechaEvento','FechaVinculacionCliente','FechaUltimoUso','FechaPrimerUso',\n",
    "    'CodigoAlmacenEntregaTC','CodigoMunicipioEntregaTC'\n",
    "}\n",
    "CAT_CANDIDATES = ['CategoriaPrincipalCredito','UsoAppWeb','Genero',\n",
    "                  'TipoMunicipioEntregaTC','CanalMunicipioEntregaTC']\n",
    "\n",
    "def parse_and_features(df_raw: pd.DataFrame,\n",
    "                       primeruso_strategy: str = \"clip_a_vinc\",\n",
    "                       ultimo_uso_clip_evento: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    primeruso_strategy:\n",
    "      - 'clip_a_vinc': si PrimerUso< Vinc => usar Vinculacion (con flag)\n",
    "      - 'poner_na'   : si PrimerUso< Vinc => NaT (con flag)\n",
    "      - 'mantener'   : mantener valor (pero flag de anomalia)\n",
    "    ultimo_uso_clip_evento:\n",
    "      - True: si UltimoUso>Evento => clipear a Evento (con flag)\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # Fechas base (ya parseadas arriba)\n",
    "    df['FechaEvento_dt'] = df['FechaEvento_dt']\n",
    "    df['FechaVinculacionCliente_dt'] = df['FechaVinculacionCliente_dt']\n",
    "\n",
    "    # ---- Primer Uso\n",
    "    df['Flag_PrimerUsoAntesVinc'] = (df['FechaPrimerUso_dt_raw'] < df['FechaVinculacionCliente_dt']).astype(int)\n",
    "    if primeruso_strategy == \"clip_a_vinc\":\n",
    "        df['FechaPrimerUso_dt'] = df['FechaPrimerUso_dt_raw']\n",
    "        mask_bad = df['FechaPrimerUso_dt_raw'].isna() | (df['FechaPrimerUso_dt_raw'] < df['FechaVinculacionCliente_dt'])\n",
    "        df.loc[mask_bad, 'FechaPrimerUso_dt'] = df.loc[mask_bad, 'FechaVinculacionCliente_dt']\n",
    "    elif primeruso_strategy == \"poner_na\":\n",
    "        df['FechaPrimerUso_dt'] = df['FechaPrimerUso_dt_raw']\n",
    "        mask_bad = df['FechaPrimerUso_dt_raw'] < df['FechaVinculacionCliente_dt']\n",
    "        df.loc[mask_bad, 'FechaPrimerUso_dt'] = pd.NaT\n",
    "    elif primeruso_strategy == \"mantener\":\n",
    "        df['FechaPrimerUso_dt'] = df['FechaPrimerUso_dt_raw']\n",
    "    else:\n",
    "        raise ValueError(\"primeruso_strategy inválida\")\n",
    "\n",
    "    # ---- Último Uso\n",
    "    df['Flag_UltimoUsoPosteriorEvento'] = (df['FechaUltimoUso_dt_raw'] > df['FechaEvento_dt']).astype(int)\n",
    "    if ultimo_uso_clip_evento:\n",
    "        df['FechaUltimoUso_dt'] = df['FechaUltimoUso_dt_raw'].copy()\n",
    "        mask_bad2 = df['FechaUltimoUso_dt_raw'] > df['FechaEvento_dt']\n",
    "        df.loc[mask_bad2, 'FechaUltimoUso_dt'] = df.loc[mask_bad2, 'FechaEvento_dt']\n",
    "    else:\n",
    "        df['FechaUltimoUso_dt'] = df['FechaUltimoUso_dt_raw']\n",
    "\n",
    "    # ---- Diferencias temporales seguras\n",
    "    def safe_months(a, b):\n",
    "        d = (a - b).dt.days\n",
    "        d = d.where(d.notna(), 0); d = np.where(d < 0, 0, d)\n",
    "        return d / 30.0\n",
    "\n",
    "    def safe_days(a, b):\n",
    "        d = (a - b).dt.days\n",
    "        d = d.where(d.notna(), 0); d = np.where(d < 0, 0, d)\n",
    "        return d\n",
    "\n",
    "    df['MesesDesdeVinculacion'] = safe_months(df['FechaEvento_dt'], df['FechaVinculacionCliente_dt'])\n",
    "    df['MesesDesdePrimerUso']   = safe_months(df['FechaEvento_dt'], df['FechaPrimerUso_dt'])\n",
    "    df['DiasDesdeUltimoUso']    = safe_days(df['FechaEvento_dt'],  df['FechaUltimoUso_dt'])\n",
    "\n",
    "    # ---- Numéricas clave + flags\n",
    "    keep_nums = [\n",
    "        'UsabilidadCupo','DiasMaximosMoraCreditosGenerados','NumeroCreditosGPrevius',\n",
    "        'NumeroCreditosGCanalFPrevius','NumeroCreditosGEstadoActivosPrevius','NumeroCreditosGEstadoPagadosPrevius',\n",
    "        'NumeroCreditosGCanalVPrevius','NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius',\n",
    "        'NumeroCreditosLEstadoPagadosPrevius','TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroIntentosFallidos','CupoAprobado','ScoreCrediticio','Edad',\n",
    "        'MesesDesdeVinculacion','MesesDesdePrimerUso','DiasDesdeUltimoUso'\n",
    "    ]\n",
    "    for c in keep_nums:\n",
    "        if c in df.columns:\n",
    "            df[f'Flag_{c}_NaN'] = df[c].isna().astype(int)\n",
    "\n",
    "    # UsabilidadCupo\n",
    "    df['UsabilidadCupo'] = pd.to_numeric(df.get('UsabilidadCupo', np.nan), errors='coerce')\n",
    "    df['Flag_Usab_Outlier']= ((df['UsabilidadCupo'] < 0) | (df['UsabilidadCupo'] > 2)).astype(int)\n",
    "    df.loc[(df['UsabilidadCupo'] < 0) | (df['UsabilidadCupo'] > 2), 'UsabilidadCupo'] = np.nan\n",
    "    df['UsabilidadCupo'] = df['UsabilidadCupo'].fillna(df['UsabilidadCupo'].median())\n",
    "\n",
    "    # Score & Cupo\n",
    "    df['ScoreSinInfo'] = (df['ScoreCrediticio'].fillna(0) == 0).astype(int)\n",
    "    df['ScoreCrediticio'] = df['ScoreCrediticio'].fillna(df['ScoreCrediticio'][df['ScoreCrediticio']>0].median())\n",
    "    df.loc[df['ScoreCrediticio'] < 0, 'ScoreCrediticio'] = 0\n",
    "    df['log_CupoAprobado'] = np.log1p(df['CupoAprobado'].fillna(df['CupoAprobado'].median()))\n",
    "\n",
    "    # Categóricas limpias\n",
    "    for c in CAT_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna('Desconocido')\n",
    "    if 'Genero' in df.columns:\n",
    "        df['Genero'] = df['Genero'].replace({27:'Desconocido'})\n",
    "\n",
    "    if 'TipoMunicipioEntregaTC' in df.columns:\n",
    "        df['TipoMunicipioEntregaTC'] = df['TipoMunicipioEntregaTC'].replace({'PEQUEÃ‘O':'PEQUEÑO'}).fillna('Desconocido')\n",
    "\n",
    "    # Derivadas\n",
    "    df['Flag_PrimerUsoTemu'] = (df['NumeroCreditosGPrevius'].fillna(0) == 0).astype(int)\n",
    "    df['ratio_pagos_local_global'] = (\n",
    "        df['TotalPagosEfectuadosLocalmentePrevius'].fillna(0) /\n",
    "        (df['TotalPagosEfectuadosGlobalmentePrevius'].fillna(0) + 1.0)\n",
    "    ).clip(0,1)\n",
    "\n",
    "    df['creditos_activos_ratio'] = (\n",
    "        df['NumeroCreditosGEstadoActivosPrevius'].fillna(0) /\n",
    "        (df['NumeroCreditosGPrevius'].fillna(0) + 1.0)\n",
    "    ).clip(0,1)\n",
    "\n",
    "    # Buckets score\n",
    "    df['ScoreBucket'] = 'sin_info'\n",
    "    mask_pos = df['ScoreCrediticio'] > 0\n",
    "    if mask_pos.sum() > 0:\n",
    "        q1, q2 = df.loc[mask_pos, 'ScoreCrediticio'].quantile([0.33, 0.66]).values\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] <= q1), 'ScoreBucket'] = 'bajo'\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] >  q1) & (df['ScoreCrediticio'] <= q2), 'ScoreBucket'] = 'medio'\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] >  q2), 'ScoreBucket'] = 'alto'\n",
    "    df['ScoreBucket'] = pd.Categorical(df['ScoreBucket'], categories=['sin_info','bajo','medio','alto'], ordered=True)\n",
    "\n",
    "    # Winsorización (p99) + flags\n",
    "    def cap_with_flag(s, upper):\n",
    "        s = s.fillna(0)\n",
    "        flag = (s > upper).astype(int)\n",
    "        return np.where(s > upper, upper, s), flag\n",
    "\n",
    "    cap_cols = [\n",
    "        'DiasDesdeUltimoUso','MesesDesdeVinculacion',\n",
    "        'TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroCreditosGPrevius','NumeroCreditosGCanalFPrevius','NumeroCreditosGCanalVPrevius',\n",
    "        'NumeroCreditosGEstadoActivosPrevius','NumeroCreditosGEstadoPagadosPrevius',\n",
    "        'NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius','NumeroCreditosLEstadoPagadosPrevius'\n",
    "    ]\n",
    "    for c in cap_cols:\n",
    "        if c in df.columns:\n",
    "            p99 = df[c].quantile(0.99)\n",
    "            capped, flag = cap_with_flag(df[c], p99)\n",
    "            df[c] = capped\n",
    "            df[f'Flag_{c}_Capped'] = flag\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_feature_lists(df: pd.DataFrame):\n",
    "    drop = set(EXCLUDE_FROM_FEATURES) | {\n",
    "        'FechaEvento_dt','FechaVinculacionCliente_dt','FechaUltimoUso_dt_raw','FechaPrimerUso_dt_raw'\n",
    "    }\n",
    "    num_base = [\n",
    "        'UsabilidadCupo','MesesDesdeVinculacion','MesesDesdePrimerUso','DiasDesdeUltimoUso',\n",
    "        'NumeroCreditosGPrevius','NumeroCreditosGCanalFPrevius','NumeroCreditosGEstadoActivosPrevius',\n",
    "        'NumeroCreditosGEstadoPagadosPrevius','NumeroCreditosGCanalVPrevius',\n",
    "        'NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius','NumeroCreditosLEstadoPagadosPrevius',\n",
    "        'TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroIntentosFallidos','ScoreCrediticio','ScoreSinInfo','CupoAprobado','log_CupoAprobado','Edad',\n",
    "        'Flag_PrimerUsoAntesVinc','Flag_Usab_Outlier','ratio_pagos_local_global','creditos_activos_ratio',\n",
    "        'Flag_UltimoUsoPosteriorEvento'\n",
    "    ]\n",
    "    num_dyn = [c for c in df.columns if c.startswith('Flag_') and (c.endswith('_NaN') or c.endswith('_Capped'))]\n",
    "    num_final = [c for c in (num_base + num_dyn) if c in df.columns and c not in drop]\n",
    "    cat_final = [c for c in (CAT_CANDIDATES + ['Flag_PrimerUsoTemu','ScoreBucket']) if c in df.columns and c not in drop]\n",
    "    overlap = set(num_final) & set(cat_final)\n",
    "    if overlap:\n",
    "        print(\">>> Aviso: había columnas en num y cat. Se quitan de num:\", sorted(list(overlap)))\n",
    "        num_final = [c for c in num_final if c not in overlap]\n",
    "    num_final = list(dict.fromkeys(num_final))\n",
    "    cat_final = list(dict.fromkeys(cat_final))\n",
    "    return num_final, cat_final\n",
    "\n",
    "\n",
    "def prune_low_variance(X: pd.DataFrame):\n",
    "    low_var = [c for c in X.columns if X[c].nunique(dropna=False) <= 1]\n",
    "    if low_var:\n",
    "        print(\">>> Poda de columnas casi-constantes:\", low_var)\n",
    "        X = X.drop(columns=low_var, errors='ignore')\n",
    "    else:\n",
    "        print(\">>> Poda de columnas casi-constantes: ninguna\")\n",
    "    return X, low_var if low_var else []\n",
    "\n",
    "\n",
    "def build_model(cat_cols, num_cols):\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('imp', SimpleImputer(strategy='most_frequent')),\n",
    "                ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "            ]), cat_cols),\n",
    "            ('num', Pipeline(steps=[\n",
    "                ('imp', SimpleImputer(strategy='median'))\n",
    "            ]), num_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    clf = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.07, max_leaf_nodes=31, l2_regularization=1.0,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    pipe = Pipeline([('pre', pre), ('clf', clf)])\n",
    "    return pipe\n",
    "\n",
    "def evaluate_proba(y_true, y_proba, name=\"model\"):\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    ap  = average_precision_score(y_true, y_proba)\n",
    "    br  = brier_score_loss(y_true, y_proba)\n",
    "    p, r, thr = precision_recall_curve(y_true, y_proba)\n",
    "    f1 = 2*p*r/(p+r+1e-12)\n",
    "    j  = np.argmax(f1)\n",
    "    best_thr = thr[j-1] if j>0 and j-1 < len(thr) else 0.5\n",
    "    y_hat = (y_proba >= best_thr).astype(int)\n",
    "\n",
    "    print(f\"[{name}] ROC-AUC={auc:.3f} | PR-AUC={ap:.3f} | Brier={br:.3f}\")\n",
    "    print(f\"[{name}] Base rate y=1: {y_true.mean():.3f} | mean(p1)={np.mean(y_proba):.3f}\")\n",
    "    print(f\"[{name}] best-F1={f1[j]:.3f} @ thr={best_thr:.3f}\")\n",
    "    print(f\"[{name}] ConfMatrix @thr={best_thr:.3f}:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    print(f\"[{name}] Report @thr={best_thr:.3f}:\\n\", classification_report(y_true, y_hat, digits=3))\n",
    "    return dict(roc_auc=auc, pr_auc=ap, brier=br, thr=best_thr, p=p, r=r, thr_curve=thr)\n",
    "\n",
    "def sweep_thresholds(y_true, y_proba, thresholds):\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        yhat = (y_proba >= t).astype(int)\n",
    "        TN, FP, FN, TP = confusion_matrix(y_true, yhat).ravel()\n",
    "        prec1 = TP/(TP+FP) if TP+FP>0 else 0.0\n",
    "        rec1  = TP/(TP+FN) if TP+FN>0 else 0.0\n",
    "        rows.append({\"thr\": round(t,3),\n",
    "                     \"prec_1\": round(prec1,3), \"rec_1\": round(rec1,3),\n",
    "                     \"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN})\n",
    "    tab = pd.DataFrame(rows)\n",
    "    print(\"\\n[BARRIDO UMBRALES] (clase=1) primeras filas:\\n\", tab.head(10).to_string(index=False))\n",
    "    return tab\n",
    "\n",
    "def topk_gain_table(y_true, y_proba, top_k_list):\n",
    "    n = len(y_true)\n",
    "    order = np.argsort(-y_proba)\n",
    "    y_sorted = np.array(y_true)[order]\n",
    "    out = []\n",
    "    base_rate = y_true.mean()\n",
    "    cum_ones = np.cumsum(y_sorted)\n",
    "    for k in top_k_list:\n",
    "        m = max(1, int(n * k / 100.0))\n",
    "        tp_k = int(cum_ones[m-1])\n",
    "        rate_k = tp_k / m\n",
    "        lift_k = rate_k / base_rate if base_rate>0 else np.nan\n",
    "        out.append({\"top_%\": k, \"n_alertas\": m, \"morosos_detectados\": tp_k,\n",
    "                    \"tasa_moros_topk\": round(rate_k,3), \"lift_vs_base\": round(lift_k,2)})\n",
    "    tab = pd.DataFrame(out)\n",
    "    print(\"\\n[GANANCIA/LIFT por top-k%]\\n\", tab.to_string(index=False))\n",
    "    return tab\n",
    "\n",
    "def expected_value_at_threshold(y_true, y_proba, thr,\n",
    "                                c_fp=COST_FP, c_fn=COST_FN, b_tp=BENEFIT_TP, c_tn=COST_TN):\n",
    "    yhat = (y_proba >= thr).astype(int)\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, yhat).ravel()\n",
    "    EV = TP*b_tp - FP*c_fp - FN*c_fn - TN*c_tn\n",
    "    return EV, TP, FP, FN, TN\n",
    "\n",
    "def search_best_ev(y_true, y_proba, c_fp=COST_FP, c_fn=COST_FN, b_tp=BENEFIT_TP, c_tn=COST_TN):\n",
    "    grid_thr = np.unique(np.round(np.linspace(0.01, 0.99, 99), 3))\n",
    "    rows = []\n",
    "    for t in grid_thr:\n",
    "        ev, TP, FP, FN, TN = expected_value_at_threshold(y_true, y_proba, t, c_fp, c_fn, b_tp, c_tn)\n",
    "        rows.append({\"thr\": t, \"EV\": ev, \"TP\":TP,\"FP\":FP,\"FN\":FN,\"TN\":TN})\n",
    "    df = pd.DataFrame(rows).sort_values(\"EV\", ascending=False)\n",
    "    best = df.iloc[0].to_dict()\n",
    "    print(f\"\\n[EV] Mejor umbral por valor económico: thr={best['thr']:.3f} | EV={best['EV']:.1f} | TP={best['TP']} FP={best['FP']} FN={best['FN']} TN={best['TN']}\")\n",
    "    return df, best\n",
    "\n",
    "# ============================================================\n",
    "# 4) SPLIT TEMPORAL 64/16/20 (train_in / valid_in / holdout)\n",
    "# ============================================================\n",
    "cut1 = clientes['FechaEvento_dt'].quantile(0.64)  # 64%\n",
    "cut2 = clientes['FechaEvento_dt'].quantile(0.80)  # 64%+16% = 80%\n",
    "\n",
    "train_in_idx = clientes['FechaEvento_dt'] <= cut1\n",
    "valid_in_idx = (clientes['FechaEvento_dt'] > cut1) & (clientes['FechaEvento_dt'] <= cut2)\n",
    "holdout_idx  = clientes['FechaEvento_dt'] > cut2\n",
    "\n",
    "print(\"\\n>>> Splits temporales:\")\n",
    "print(f\"  train_in = {train_in_idx.sum()} filas (<= {pd.Timestamp(cut1).date()})\")\n",
    "print(f\"  valid_in = {valid_in_idx.sum()} filas (<= {pd.Timestamp(cut2).date()})\")\n",
    "print(f\"  holdout  = {holdout_idx.sum()} filas (>  {pd.Timestamp(cut2).date()})\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) EXPERIMENTOS (al estilo Chip Huyen) — ablation de fechas\n",
    "# ============================================================\n",
    "EXPERIMENTS = [\n",
    "    {\"name\": \"clip_a_vinc__clip_ultimo\", \"primeruso\":\"clip_a_vinc\", \"clip_ultimo\": True},\n",
    "    {\"name\": \"poner_na__clip_ultimo\",     \"primeruso\":\"poner_na\",    \"clip_ultimo\": True},\n",
    "    {\"name\": \"mantener__clip_ultimo\",     \"primeruso\":\"mantener\",    \"clip_ultimo\": True},\n",
    "]\n",
    "\n",
    "exp_results = []\n",
    "\n",
    "for exp in EXPERIMENTS:\n",
    "    print(\"\\n======================\")\n",
    "    print(f\">>> EXPERIMENTO: {exp['name']} (PrimerUso={exp['primeruso']}, UltimoUsoClipEvento={exp['clip_ultimo']})\")\n",
    "    df_feat = parse_and_features(clientes, primeruso_strategy=exp['primeruso'],\n",
    "                                 ultimo_uso_clip_evento=exp['clip_ultimo'])\n",
    "    num_final, cat_final = build_feature_lists(df_feat)\n",
    "\n",
    "    X = df_feat[num_final + cat_final].copy()\n",
    "    y = df_feat[TARGET].astype(int).copy()\n",
    "\n",
    "    # Poda\n",
    "    X, dropped_lowvar = prune_low_variance(X)\n",
    "\n",
    "    # Split\n",
    "    X_tr_in, y_tr_in = X.loc[train_in_idx], y.loc[train_in_idx]\n",
    "    X_va_in, y_va_in = X.loc[valid_in_idx], y.loc[valid_in_idx]\n",
    "    X_ho,    y_ho    = X.loc[holdout_idx],  y.loc[holdout_idx]\n",
    "\n",
    "    # Modelo y calibración SOLO en train_in + valid_in\n",
    "    pipe = build_model(cat_final, [c for c in X.columns if c not in cat_final])\n",
    "    sw = compute_sample_weight(\"balanced\", y_tr_in)\n",
    "    pipe.fit(X_tr_in, y_tr_in, clf__sample_weight=sw)\n",
    "\n",
    "    cal = CalibratedClassifierCV(pipe, method='isotonic', cv='prefit')\n",
    "    cal.fit(X_va_in, y_va_in)  # calibración estricta en VALID\n",
    "\n",
    "    # Métricas en valid_in (para seleccionar variante)\n",
    "    proba_va = cal.predict_proba(X_va_in)[:,1]\n",
    "    mv = evaluate_proba(y_va_in, proba_va, name=f\"VALID[{exp['name']}]\")\n",
    "    # Métricas en train_in (diagnóstico de sobreajuste)\n",
    "    proba_tr = cal.predict_proba(X_tr_in)[:,1]\n",
    "    mt = evaluate_proba(y_tr_in, proba_tr, name=f\"TRAIN[{exp['name']}]\")\n",
    "\n",
    "    exp_results.append({\n",
    "        \"exp\": exp[\"name\"], \"primeruso\":exp[\"primeruso\"], \"clip_ultimo\":exp[\"clip_ultimo\"],\n",
    "        \"valid_pr_auc\": mv[\"pr_auc\"], \"valid_roc_auc\": mv[\"roc_auc\"], \"valid_brier\": mv[\"brier\"],\n",
    "        \"train_pr_auc\": mt[\"pr_auc\"], \"train_roc_auc\": mt[\"roc_auc\"]\n",
    "    })\n",
    "\n",
    "# Comparativa estilo Chip Huyen\n",
    "exp_df = pd.DataFrame(exp_results).sort_values([\"valid_pr_auc\",\"valid_roc_auc\"], ascending=False)\n",
    "print(\"\\n====================== RESULTADOS DE EXPERIMENTOS (ordenado por PR-AUC VALID) ======================\")\n",
    "print(exp_df.to_string(index=False))\n",
    "\n",
    "best = exp_df.iloc[0].to_dict()\n",
    "print(f\"\\n>>> Variante ganadora por VALID: {best['exp']} (PR-AUC={best['valid_pr_auc']:.3f} | ROC-AUC={best['valid_roc_auc']:.3f})\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) ENTRENAR DEFINITIVO (train_in -> fit, valid_in -> calibrar) y EVALUAR en HOLDOUT\n",
    "# ============================================================\n",
    "# Reconstruimos con la mejor variante\n",
    "best_exp = [e for e in EXPERIMENTS if e[\"name\"] == best[\"exp\"]][0]\n",
    "df_best = parse_and_features(clientes, primeruso_strategy=best_exp['primeruso'],\n",
    "                             ultimo_uso_clip_evento=best_exp['clip_ultimo'])\n",
    "num_final, cat_final = build_feature_lists(df_best)\n",
    "Xall = df_best[num_final + cat_final].copy()\n",
    "yall = df_best[TARGET].astype(int).copy()\n",
    "Xall, _ = prune_low_variance(Xall)\n",
    "\n",
    "X_tr_in, y_tr_in = Xall.loc[train_in_idx], yall.loc[train_in_idx]\n",
    "X_va_in, y_va_in = Xall.loc[valid_in_idx], yall.loc[valid_in_idx]\n",
    "X_ho,    y_ho    = Xall.loc[holdout_idx],  yall.loc[holdout_idx]\n",
    "\n",
    "print(\"\\n>>> Entrenando ganador en train_in y calibrando en valid_in; evaluando en holdout (nunca visto).\")\n",
    "pipe = build_model(cat_final, [c for c in Xall.columns if c not in cat_final])\n",
    "sw = compute_sample_weight(\"balanced\", y_tr_in)\n",
    "pipe.fit(X_tr_in, y_tr_in, clf__sample_weight=sw)\n",
    "\n",
    "cal = CalibratedClassifierCV(pipe, method='isotonic', cv='prefit')\n",
    "cal.fit(X_va_in, y_va_in)\n",
    "\n",
    "# EVALUACIÓN FINAL\n",
    "proba_ho = cal.predict_proba(X_ho)[:,1]\n",
    "m = evaluate_proba(y_ho, proba_ho, name=\"HOLDOUT (final)\")\n",
    "\n",
    "# Reportes de negocio\n",
    "_ = topk_gain_table(y_ho, proba_ho, TOP_K_LIST)\n",
    "\n",
    "# Barrido y objetivo de precisión\n",
    "grid_thr = np.unique(np.round(np.linspace(0.01, 0.99, 99), 3))\n",
    "tab_thr = sweep_thresholds(y_ho, proba_ho, grid_thr)\n",
    "tab_thr.to_csv(ARTIF_DIR/\"holdout_threshold_sweep.csv\", index=False)\n",
    "\n",
    "p, r, thr = m[\"p\"], m[\"r\"], m[\"thr_curve\"]\n",
    "idx = np.where(p >= GOAL_PRECISION)[0]\n",
    "if len(idx) == 0:\n",
    "    print(f\"\\n>>> No hay punto con precision ≥ {GOAL_PRECISION:.2f} en holdout.\")\n",
    "else:\n",
    "    k = idx[np.argmax(r[idx])]\n",
    "    thr_goal = thr[k-1] if k>0 else 0.5\n",
    "    yhat = (proba_ho >= thr_goal).astype(int)\n",
    "    TN, FP, FN, TP = confusion_matrix(y_ho, yhat).ravel()\n",
    "    prec1, rec1 = (TP/(TP+FP) if TP+FP>0 else 0), (TP/(TP+FN) if TP+FN>0 else 0)\n",
    "    print(f\"\\n[Objetivo negocio] PRECISIÓN≥{GOAL_PRECISION:.2f} @thr={thr_goal:.3f}\")\n",
    "    print(f\"  TP={TP} FP={FP} FN={FN} TN={TN}\")\n",
    "    print(f\"  precision1={prec1:.3f} recall1={rec1:.3f}\")\n",
    "    print(f\"  %alertados={(TP+FP)/len(y_ho):.3f}   %morosos_detectados={TP/len(y_ho):.3f}\")\n",
    "\n",
    "# Umbral por Máx-F1 (referencia)\n",
    "j  = np.argmax(2*m[\"p\"]*m[\"r\"]/(m[\"p\"]+m[\"r\"]+1e-12))\n",
    "thr_star = m[\"thr_curve\"][j-1] if j>0 else 0.5\n",
    "print(f\"\\n[Referencia] Máx-F1 @thr≈{thr_star:.3f} | precision={m['p'][j]:.3f} | recall={m['r'][j]:.3f}\")\n",
    "\n",
    "# Calibración por deciles\n",
    "def decile_calibration(y_true, y_proba, name=\"holdout\"):\n",
    "    bins = pd.qcut(y_proba, q=10, duplicates='drop')\n",
    "    tab = pd.DataFrame({\"p\":y_proba, \"y\":y_true}).groupby(bins, observed=True).agg(\n",
    "        p_mean=('p','mean'),\n",
    "        y_rate=('y','mean'),\n",
    "        n=('p','size')\n",
    "    ).reset_index()\n",
    "    print(f\"\\n[{name}] Calibración por deciles (p_mean ~ y_rate):\\n\", tab.to_string(index=False))\n",
    "    tab.to_csv(ARTIF_DIR/f\"calibracion_deciles_{name}.csv\", index=False)\n",
    "    return tab\n",
    "_ = decile_calibration(y_ho, proba_ho, name=\"HOLDOUT_final\")\n",
    "\n",
    "# Umbral por máximo EV económico (si definiste costos/beneficios)\n",
    "_ , best_ev = search_best_ev(y_ho, proba_ho)\n",
    "print(\" Tip: ajusta COST_FP, COST_FN, BENEFIT_TP según tu negocio y re-ejecuta.\")\n",
    "\n",
    "# Importancias por permutación (en holdout, usando el pipe base)\n",
    "print(\"\\n>>> Importancias por permutación (holdout) — top 20\")\n",
    "pi = permutation_importance(pipe, X_ho, y_ho, n_repeats=5, random_state=RANDOM_STATE, scoring='average_precision')\n",
    "pi_df = pd.DataFrame({\"feature\": X_ho.columns, \"importance_mean\": pi.importances_mean, \"importance_std\": pi.importances_std})\n",
    "pi_df = pi_df.sort_values(\"importance_mean\", ascending=False)\n",
    "print(pi_df.head(20).to_string(index=False))\n",
    "pi_df.to_csv(ARTIF_DIR/\"permutation_importance_holdout.csv\", index=False)\n",
    "\n",
    "# (Opcional) SHAP\n",
    "try:\n",
    "    import shap\n",
    "    print(\"\\n>>> Calculando SHAP summary (muestra de holdout).\")\n",
    "    sample_idx = np.random.RandomState(RANDOM_STATE).choice(X_ho.index, size=min(5000, X_ho.shape[0]), replace=False)\n",
    "    X_sample = X_ho.loc[sample_idx]\n",
    "    preproc = pipe.named_steps['pre']\n",
    "    # Importante: ajustar con train_in (no valid/holdout) para no fugar\n",
    "    _ = preproc.fit(X_tr_in, y_tr_in)\n",
    "    X_trans = preproc.transform(X_sample)\n",
    "    model = pipe.named_steps['clf']\n",
    "    explainer = shap.Explainer(model.predict_proba, X_trans)\n",
    "    shap_values = explainer(X_trans)\n",
    "    sv_abs = np.abs(shap_values.values[...,1]).mean(axis=0)\n",
    "    shap_df = pd.DataFrame({\"feature_transformed_id\": np.arange(sv_abs.shape[0]), \"mean_abs_shap\": sv_abs})\n",
    "    shap_df.to_csv(ARTIF_DIR/\"shap_mean_abs_holdout.csv\", index=False)\n",
    "    print(\"SHAP exportado.\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP no disponible/omitido:\", e)\n",
    "\n",
    "# ============================================================\n",
    "# 7) GUARDAR ARTEFACTOS Y FUNCIÓN DE SCORING\n",
    "# ============================================================\n",
    "bundle = {\n",
    "    \"model_calibrado\": cal,\n",
    "    \"columns_after_prune\": list(Xall.columns),\n",
    "    \"num_final\": num_final,\n",
    "    \"cat_final\": cat_final,\n",
    "    \"cut1\": cut1, \"cut2\": cut2,\n",
    "    \"experiment_best\": best,\n",
    "    \"costs\": {\"COST_FP\":COST_FP,\"COST_FN\":COST_FN,\"BENEFIT_TP\":BENEFIT_TP,\"COST_TN\":COST_TN}\n",
    "}\n",
    "joblib.dump(bundle, ARTIF_DIR/\"modelo_calibrado.joblib\")\n",
    "pd.DataFrame({\"metric\":[\"ROC_AUC\",\"PR_AUC\",\"Brier\"],\n",
    "              \"value\":[m['roc_auc'], m['pr_auc'], m['brier']]}).to_csv(ARTIF_DIR/\"holdout_metrics.csv\", index=False)\n",
    "print(f\"\\n>>> Artefactos guardados en {ARTIF_DIR.resolve()}\")\n",
    "\n",
    "def score_future(df_nuevo_raw: pd.DataFrame, artif_path=ARTIF_DIR/\"modelo_calibrado.joblib\",\n",
    "                 primeruso_strategy=best_exp['primeruso'], ultimo_uso_clip_evento=best_exp['clip_ultimo'],\n",
    "                 export_csv=True, target_col=TARGET):\n",
    "    bundle = joblib.load(artif_path)\n",
    "    model  = bundle[\"model_calibrado\"]\n",
    "    cols_ok = bundle[\"columns_after_prune\"]\n",
    "    df_nuevo = parse_and_features(df_nuevo_raw, primeruso_strategy, ultimo_uso_clip_evento)\n",
    "    Xn = df_nuevo.reindex(columns=cols_ok, fill_value=np.nan)\n",
    "    proba1 = model.predict_proba(Xn)[:,1]\n",
    "    proba0 = 1 - proba1\n",
    "    print(\"\\n[SCORING] Resumen probabilidades nueva data:\")\n",
    "    print(f\"mean(p1)= {proba1.mean():.3f} | min/max p1= {proba1.min():.3f}/{proba1.max():.3f}\")\n",
    "    if target_col in df_nuevo.columns:\n",
    "        y_true = df_nuevo[target_col].astype(int)\n",
    "        _ = evaluate_proba(y_true, proba1, name=\"NUEVA_DATA\")\n",
    "    out = pd.DataFrame({\"p_no_perdida\": proba0, \"p_perdida\": proba1})\n",
    "    if export_csv:\n",
    "        out_path = ARTIF_DIR/\"scoring_df_nuevo.csv\"\n",
    "        out.to_csv(out_path, index=False)\n",
    "        print(f\"Scoring exportado a {out_path}\")\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 8) SNIPPET FASTAPI\n",
    "# ============================================================\n",
    "FASTAPI_SNIPPET = f\"\"\"\n",
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "app = FastAPI(title=\"API Riesgo Temu\", version=\"1.0\")\n",
    "\n",
    "BUNDLE_PATH = \"{str((ARTIF_DIR/'modelo_calibrado.joblib').resolve())}\"\n",
    "\n",
    "from __main__ import parse_and_features  # reutilizamos funciones\n",
    "\n",
    "bundle = joblib.load(BUNDLE_PATH)\n",
    "model = bundle[\"model_calibrado\"]\n",
    "cols_ok = bundle[\"columns_after_prune\"]\n",
    "best_exp = bundle.get(\"experiment_best\", None)\n",
    "\n",
    "@app.post(\"/score\")\n",
    "def score(payload: dict):\n",
    "    df = pd.DataFrame([payload])\n",
    "    primeruso = \"clip_a_vinc\" if not best_exp else best_exp.get(\"primeruso\",\"clip_a_vinc\")\n",
    "    clip_ult  = True if not best_exp else bool(best_exp.get(\"clip_ultimo\", True))\n",
    "    df = parse_and_features(df, primeruso_strategy=primeruso, ultimo_uso_clip_evento=clip_ult)\n",
    "    Xn = df.reindex(columns=cols_ok, fill_value={{}})\n",
    "    proba1 = model.predict_proba(Xn)[:,1]\n",
    "    return {{\"p_perdida\": float(proba1[0])}}\n",
    "\"\"\"\n",
    "with open(ARTIF_DIR/\"app_fastapi_snippet.py\", \"w\") as f:\n",
    "    f.write(FASTAPI_SNIPPET)\n",
    "print(f\">>> Snippet FastAPI guardado en {ARTIF_DIR/'app_fastapi_snippet.py'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b8624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[train_in (≤ cut1)] n=94041\n",
      "  min=2022-05-01 | max=2023-05-13\n",
      "\n",
      "[valid_in (cut1 < · ≤ cut2)] n=23510\n",
      "  min=2023-05-13 | max=2023-07-29\n",
      "\n",
      "[holdout  (> cut2)] n=29388\n",
      "  min=2023-07-29 | max=2023-10-31\n",
      "\n",
      ">>> Verificación de cortes:\n",
      "  cut1: 2023-05-13 | cut2: 2023-07-29\n",
      "  max(train) ≤ cut1 ?  True\n",
      "  min(valid)  > cut1 ?  True\n",
      "  max(valid) ≤ cut2 ?  True\n",
      "  min(hold)   > cut2 ?  True\n",
      "\n",
      ">>> Cobertura partición (excluye NaT):\n",
      "  asignadas=146939 de 146939 con fecha válida\n",
      "\n",
      ">>> Muestras cercanas a cut1/cut2:\n",
      " IdentificadorCliente          FechaEvento_dt\n",
      "                91798 2023-05-11 21:53:02.980\n",
      "                46661 2023-05-11 21:54:59.957\n",
      "               125977 2023-05-11 21:55:24.190\n",
      "               119272 2023-05-11 21:56:03.770\n",
      "                31315 2023-05-11 21:57:28.290\n",
      "                51686 2023-05-11 22:01:47.593\n",
      "               128791 2023-05-11 22:02:56.230\n",
      "                25380 2023-05-11 22:08:10.203\n",
      " IdentificadorCliente          FechaEvento_dt\n",
      "               100418 2023-07-27 19:56:11.397\n",
      "                35949 2023-07-27 19:57:15.343\n",
      "                60791 2023-07-27 19:58:59.590\n",
      "               144029 2023-07-27 20:00:32.087\n",
      "               135008 2023-07-27 20:03:16.273\n",
      "                45772 2023-07-27 20:08:23.307\n",
      "               120182 2023-07-27 20:12:00.460\n",
      "               129621 2023-07-27 20:12:32.993\n"
     ]
    }
   ],
   "source": [
    "def resumen_split(df, mask, nombre):\n",
    "    sub = df.loc[mask, 'FechaEvento_dt']\n",
    "    print(f\"\\n[{nombre}] n={mask.sum()}\")\n",
    "    if sub.notna().any():\n",
    "        print(f\"  min={sub.min().date()} | max={sub.max().date()}\")\n",
    "    else:\n",
    "        print(\"  (todas las fechas son NaT)\")\n",
    "\n",
    "# 1) Rango por split\n",
    "resumen_split(clientes, train_in_idx, \"train_in (≤ cut1)\")\n",
    "resumen_split(clientes, valid_in_idx, \"valid_in (cut1 < · ≤ cut2)\")\n",
    "resumen_split(clientes, holdout_idx,  \"holdout  (> cut2)\")\n",
    "\n",
    "# 2) Chequeos de integridad (orden temporal y no solape)\n",
    "assert set(clientes.index[train_in_idx]).isdisjoint(clientes.index[valid_in_idx])\n",
    "assert set(clientes.index[train_in_idx]).isdisjoint(clientes.index[holdout_idx])\n",
    "assert set(clientes.index[valid_in_idx]).isdisjoint(clientes.index[holdout_idx])\n",
    "\n",
    "max_train = clientes.loc[train_in_idx, 'FechaEvento_dt'].max()\n",
    "min_valid = clientes.loc[valid_in_idx, 'FechaEvento_dt'].min()\n",
    "max_valid = clientes.loc[valid_in_idx, 'FechaEvento_dt'].max()\n",
    "min_hold  = clientes.loc[holdout_idx,  'FechaEvento_dt'].min()\n",
    "\n",
    "print(\"\\n>>> Verificación de cortes:\")\n",
    "print(\"  cut1:\", pd.Timestamp(cut1).date(), \"| cut2:\", pd.Timestamp(cut2).date())\n",
    "print(\"  max(train) ≤ cut1 ? \", max_train <= cut1)\n",
    "print(\"  min(valid)  > cut1 ? \", min_valid  >  cut1)\n",
    "print(\"  max(valid) ≤ cut2 ? \", max_valid <= cut2)\n",
    "print(\"  min(hold)   > cut2 ? \", min_hold   >  cut2)\n",
    "\n",
    "# 3) ¿Cubre todo el dataset (excepto NaT)?\n",
    "mask_any = train_in_idx | valid_in_idx | holdout_idx\n",
    "n_total = clientes['FechaEvento_dt'].notna().sum()\n",
    "print(\"\\n>>> Cobertura partición (excluye NaT):\")\n",
    "print(f\"  asignadas={mask_any.sum()} de {n_total} con fecha válida\")\n",
    "assert mask_any.sum() == n_total, \"Hay filas con fecha válida que no cayeron en ningún split.\"\n",
    "\n",
    "# 4) (Opcional) Muestra 3 fechas cercanas al borde para tranquilidad\n",
    "print(\"\\n>>> Muestras cercanas a cut1/cut2:\")\n",
    "print(clientes[['IdentificadorCliente','FechaEvento_dt']].loc[\n",
    "    clientes['FechaEvento_dt'].between(cut1 - pd.Timedelta(days=2), cut1 + pd.Timedelta(days=2))\n",
    "].sort_values('FechaEvento_dt').head(8).to_string(index=False))\n",
    "\n",
    "print(clientes[['IdentificadorCliente','FechaEvento_dt']].loc[\n",
    "    clientes['FechaEvento_dt'].between(cut2 - pd.Timedelta(days=2), cut2 + pd.Timedelta(days=2))\n",
    "].sort_values('FechaEvento_dt').head(8).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f16003b",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b694bf",
   "metadata": {},
   "source": [
    "# Reporte Automatizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfaa9339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================== RESULTADOS DE EXPERIMENTOS (ordenado por PR-AUC VALID) ======================\n",
      "                     exp   primeruso  clip_ultimo  valid_pr_auc  valid_roc_auc  valid_brier  train_pr_auc  train_roc_auc\n",
      "   mantener__clip_ultimo    mantener         True      0.756547       0.870612     0.117141      0.745563       0.884077\n",
      "clip_a_vinc__clip_ultimo clip_a_vinc         True      0.755922       0.870066     0.117397      0.747341       0.883777\n",
      "   poner_na__clip_ultimo    poner_na         True      0.755392       0.870729     0.117097      0.744647       0.883514\n"
     ]
    }
   ],
   "source": [
    "exp_df = pd.DataFrame(exp_results).sort_values([\"valid_pr_auc\",\"valid_roc_auc\"], ascending=False)\n",
    "print(\"\\n====================== RESULTADOS DE EXPERIMENTOS (ordenado por PR-AUC VALID) ======================\")\n",
    "print(exp_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe8806a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resumen de experimentos\n",
    "exp_df.to_csv(ARTIF_DIR/\"experiments_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4da74e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GANANCIA/LIFT por top-k%]\n",
      "  top_%  n_alertas  morosos_detectados  tasa_moros_topk  lift_vs_base\n",
      "     1        293                 211            0.720          3.47\n",
      "     2        587                 430            0.733          3.53\n",
      "     5       1469                1099            0.748          3.61\n",
      "    10       2938                1932            0.658          3.17\n",
      "    20       5877                3090            0.526          2.53\n"
     ]
    }
   ],
   "source": [
    "topk_df = topk_gain_table(y_ho, proba_ho, TOP_K_LIST)\n",
    "topk_df.to_csv(ARTIF_DIR/\"holdout_topk_gain.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9dc599c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EV] Mejor umbral por valor económico: thr=0.140 | EV=8976.0 | TP=5578.0 FP=10741.0 FN=519.0 TN=12550.0\n"
     ]
    }
   ],
   "source": [
    "ev_grid, best_ev = search_best_ev(y_ho, proba_ho)\n",
    "ev_grid.to_csv(ARTIF_DIR/\"holdout_ev_grid.csv\", index=False)\n",
    "pd.DataFrame([best_ev]).to_csv(ARTIF_DIR/\"holdout_best_ev.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dadee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markdown\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Downloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: markdown\n",
      "Successfully installed markdown-3.9\n"
     ]
    }
   ],
   "source": [
    "# dentro de tu venv, si usas uno\n",
    "!pip install markdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f932d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Informe Markdown generado en: artifacts_modelo/informe_modelo.md\n",
      "[OK] Informe HTML generado en: artifacts_modelo/informe_modelo.html\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GENERADOR DE INFORME AUTOMÁTICO (Markdown + opcional HTML)\n",
    "# ============================================================\n",
    "import os, io, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Usa el mismo ARTIF_DIR que definiste en el script principal\n",
    "ARTIF_DIR = Path(\"./artifacts_modelo\")\n",
    "REPORT_MD  = ARTIF_DIR/\"informe_modelo.md\"\n",
    "REPORT_HTML= ARTIF_DIR/\"informe_modelo.html\"  # opcional (si tienes 'markdown' instalado)\n",
    "\n",
    "# (Opcional) pega aquí tu texto “notas de correlaciones” para que salga en el informe:\n",
    "NOTAS_CORRELACIONES = \"\"\"\n",
    "## 📌 Correlaciones y hallazgos rápidos\n",
    "\n",
    "- **DiasMora** correlación alta con *PerdidaCartera* (~0.83). Es variable de **estado actual** → no usar como feature predictiva (evitar fuga).\n",
    "- **Historial de créditos previos**: activos y pagados muestran señal (p. ej., `NumeroCreditosGEstadoActivosPrevius`, `NumeroCreditosLEstadoActivosPrevius`, `NumeroCreditosLPrevius`…).\n",
    "- **Antigüedades** (meses desde vinculación / primer uso) aportan señal más estable que las fechas crudas.\n",
    "- **Demografía y cupo** (`Edad`, `CupoAprobado`, `ScoreCrediticio`): correlación negativa moderada → más edad/cupo/score, menos pérdida.\n",
    "- **Canal/Tipo municipio**: patrón “Virtual > Físico” en pérdida (≈28% vs 16–20%).\n",
    "- **Género**: leve diferencia; mantener como categórica.\n",
    "\"\"\"\n",
    "\n",
    "def _exists(p): \n",
    "    try: return Path(p).exists()\n",
    "    except: return False\n",
    "\n",
    "def _read_csv_safe(path):\n",
    "    try:\n",
    "        if _exists(path):\n",
    "            return pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] No pude leer {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "def _fmt_pct(x, digits=1):\n",
    "    try: return f\"{100*float(x):.{digits}f}%\"\n",
    "    except: return \"NA\"\n",
    "\n",
    "def _table_md(df, max_rows=20):\n",
    "    if df is None or len(df)==0:\n",
    "        return \"_(sin datos)_\"\n",
    "    df2 = df.copy()\n",
    "    if len(df2) > max_rows:\n",
    "        df2 = df2.head(max_rows)\n",
    "    try:\n",
    "        return df2.to_markdown(index=False)\n",
    "    except Exception:\n",
    "        # por si no está tabulate, devolvemos CSV como fallback\n",
    "        return \"```\\n\" + df2.to_csv(index=False) + \"```\"\n",
    "\n",
    "def _load_bundle():\n",
    "    bundle_path = ARTIF_DIR/\"modelo_calibrado.joblib\"\n",
    "    if not _exists(bundle_path):\n",
    "        print(f\"[WARN] No existe {bundle_path}\")\n",
    "        return None\n",
    "    return joblib.load(bundle_path)\n",
    "\n",
    "def _infer_dates_from_bundle(bundle):\n",
    "    # spliteo temporal (cut1/cut2) y experimento ganador almacenados en bundle\n",
    "    cuts = {}\n",
    "    try:\n",
    "        cuts[\"cut1\"] = pd.to_datetime(bundle.get(\"cut1\", None))\n",
    "        cuts[\"cut2\"] = pd.to_datetime(bundle.get(\"cut2\", None))\n",
    "    except Exception:\n",
    "        pass\n",
    "    best_exp = bundle.get(\"experiment_best\", {})\n",
    "    return cuts, best_exp\n",
    "\n",
    "def _load_key_artifacts():\n",
    "    return {\n",
    "        \"metrics\": _read_csv_safe(ARTIF_DIR/\"holdout_metrics.csv\"),\n",
    "        \"thr_sweep\": _read_csv_safe(ARTIF_DIR/\"holdout_threshold_sweep.csv\"),\n",
    "        \"topk\": _read_csv_safe(ARTIF_DIR/\"holdout_topk_gain.csv\"),\n",
    "        \"perm_imp\": _read_csv_safe(ARTIF_DIR/\"permutation_importance_holdout.csv\"),\n",
    "        \"deciles\": _read_csv_safe(ARTIF_DIR/\"calibracion_deciles_HOLDOUT_final.csv\"),\n",
    "        \"exp\": _read_csv_safe(ARTIF_DIR/\"experiments_summary.csv\"),\n",
    "        \"ev_grid\": _read_csv_safe(ARTIF_DIR/\"holdout_ev_grid.csv\"),\n",
    "        \"ev_best\": _read_csv_safe(ARTIF_DIR/\"holdout_best_ev.csv\"),\n",
    "    }\n",
    "\n",
    "def generar_informe_markdown(extra_notas=NOTAS_CORRELACIONES, export_html=True):\n",
    "    ARTIF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    bundle = _load_bundle()\n",
    "    cuts, best_exp = _infer_dates_from_bundle(bundle) if bundle else ({}, {})\n",
    "    artifacts = _load_key_artifacts()\n",
    "\n",
    "    # Cabecera\n",
    "    md = io.StringIO()\n",
    "    md.write(\"# Informe Automático — Modelo de Riesgo Temu\\n\")\n",
    "    md.write(\"_Validación temporal estricta (sin fuga)._\\n\\n\")\n",
    "\n",
    "    # Resumen corto (tarjeta ejecutiva)\n",
    "    md.write(\"## 🧾 Resumen ejecutivo\\n\")\n",
    "    if artifacts[\"metrics\"] is not None:\n",
    "        met = artifacts[\"metrics\"].set_index(\"metric\")[\"value\"].to_dict()\n",
    "        md.write(f\"- **ROC-AUC (holdout):** {met.get('ROC_AUC', np.nan):.3f}\\n\")\n",
    "        md.write(f\"- **PR-AUC (holdout):** {met.get('PR_AUC', np.nan):.3f}\\n\")\n",
    "        md.write(f\"- **Brier (holdout):** {met.get('Brier', np.nan):.3f}\\n\")\n",
    "    if best_exp:\n",
    "        md.write(f\"- **Variante ganadora:** `{best_exp.get('exp','NA')}` | \"\n",
    "                 f\"PR-AUC(VALID)={best_exp.get('valid_pr_auc', np.nan):.3f} | \"\n",
    "                 f\"ROC-AUC(VALID)={best_exp.get('valid_roc_auc', np.nan):.3f}\\n\")\n",
    "    if cuts:\n",
    "        c1 = cuts.get(\"cut1\", None)\n",
    "        c2 = cuts.get(\"cut2\", None)\n",
    "        if c1 is not None and c2 is not None:\n",
    "            md.write(f\"- **Cortes temporales:** train≤**{str(pd.Timestamp(c1).date())}**, \"\n",
    "                     f\"valid≤**{str(pd.Timestamp(c2).date())}**, holdout>**{str(pd.Timestamp(c2).date())}**\\n\")\n",
    "    md.write(\"\\n\")\n",
    "\n",
    "    # Experimentos\n",
    "    md.write(\"## 🔬 Comparativa de experimentos (VALID)\\n\")\n",
    "    md.write(_table_md(artifacts[\"exp\"]))\n",
    "    md.write(\"\\n\\n\")\n",
    "\n",
    "    # Métricas holdout (detalle)\n",
    "    md.write(\"## 📈 Métricas en HOLDOUT (final)\\n\")\n",
    "    md.write(_table_md(artifacts[\"metrics\"]))\n",
    "    md.write(\"\\n\\n\")\n",
    "\n",
    "    # Barrido de umbrales (top 15)\n",
    "    md.write(\"## 🎯 Barrido de umbrales (clase=1)\\n\")\n",
    "    md.write(_table_md(artifacts[\"thr_sweep\"], max_rows=15))\n",
    "    md.write(\"\\n\\n\")\n",
    "\n",
    "    # Top-k / Lift\n",
    "    md.write(\"## 🚀 Ganancia/Lift por Top-k%\\n\")\n",
    "    md.write(_table_md(artifacts[\"topk\"]))\n",
    "    md.write(\"\\n\\n\")\n",
    "\n",
    "    # EV económico\n",
    "    md.write(\"## 💰 Valor esperado por umbral\\n\")\n",
    "    if artifacts[\"ev_best\"] is not None and len(artifacts[\"ev_best\"])>0:\n",
    "        row = artifacts[\"ev_best\"].iloc[0].to_dict()\n",
    "        md.write(f\"- **Mejor EV:** thr={row.get('thr','NA')} | EV={row.get('EV','NA')} \"\n",
    "                 f\"| TP={row.get('TP','NA')} FP={row.get('FP','NA')} FN={row.get('FN','NA')} TN={row.get('TN','NA')}\\n\\n\")\n",
    "    md.write(_table_md(artifacts[\"ev_grid\"], max_rows=20))\n",
    "    md.write(\"\\n\\n\")\n",
    "\n",
    "    # Calibración por deciles\n",
    "    md.write(\"## ⚖️ Calibración por deciles\\n\")\n",
    "    md.write(_table_md(artifacts[\"deciles\"]))\n",
    "    md.write(\"\\n\\n\")\n",
    "\n",
    "    # Importancias por permutación (Top-20)\n",
    "    md.write(\"## 🔍 Importancias por permutación (Top-20)\\n\")\n",
    "    imp = artifacts[\"perm_imp\"]\n",
    "    if imp is not None and \"importance_mean\" in imp.columns:\n",
    "        imp = imp.sort_values(\"importance_mean\", ascending=False)\n",
    "    md.write(_table_md(imp, max_rows=20))\n",
    "    md.write(\"\\n\\n\")\n",
    "\n",
    "    # Notas (tu texto de correlaciones / hallazgos)\n",
    "    if extra_notas and extra_notas.strip():\n",
    "        md.write(\"## 📝 Notas/lecturas rápidas\\n\")\n",
    "        md.write(extra_notas.strip() + \"\\n\\n\")\n",
    "\n",
    "    # Costeo (desde bundle)\n",
    "    if bundle and \"costs\" in bundle:\n",
    "        md.write(\"## ⚙️ Parámetros de costo/beneficio usados\\n\")\n",
    "        md.write(\"```json\\n\" + json.dumps(bundle[\"costs\"], indent=2) + \"\\n```\\n\\n\")\n",
    "\n",
    "    # Guardar MD\n",
    "    with open(REPORT_MD, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(md.getvalue())\n",
    "    print(f\"[OK] Informe Markdown generado en: {REPORT_MD}\")\n",
    "\n",
    "    # (Opcional) Exportar HTML si tienes instalado 'markdown' (pip install markdown)\n",
    "    if export_html:\n",
    "        try:\n",
    "            import markdown as _md\n",
    "            html = _md.markdown(md.getvalue(), extensions=[\"tables\"])\n",
    "            with open(REPORT_HTML, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"<!doctype html><meta charset='utf-8'><style>table{{border-collapse:collapse}}td,th{{border:1px solid #ddd;padding:6px}}</style>{html}\")\n",
    "            print(f\"[OK] Informe HTML generado en: {REPORT_HTML}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] No se generó HTML (instala 'markdown' si lo quieres): {e}\")\n",
    "\n",
    "# Si quieres poder ejecutarlo aparte:\n",
    "if __name__ == \"__main__\":\n",
    "    generar_informe_markdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc243f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a11be188",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57d722a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b499626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temuapi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
