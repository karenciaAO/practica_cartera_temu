{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c536c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/karenaraque/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: pyxlsb in /Users/karenaraque/anaconda3/lib/python3.11/site-packages (1.0.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/karenaraque/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/karenaraque/anaconda3/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/karenaraque/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/karenaraque/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/karenaraque/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyxlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3fd548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99bad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DataFramePrueba', 'diccionario']\n",
      "Shape: (146939, 30)\n",
      "\n",
      "Tipos de datos:\n",
      "IdentificadorCliente                        int64\n",
      "FechaEvento                                object\n",
      "UsabilidadCupo                             object\n",
      "CategoriaPrincipalCredito                  object\n",
      "DiasMaximosMoraCreditosGenerados          float64\n",
      "NumeroCreditosGPrevius                    float64\n",
      "NumeroCreditosGCanalFPrevius              float64\n",
      "NumeroCreditosGEstadoActivosPrevius       float64\n",
      "NumeroCreditosGEstadoPagadosPrevius       float64\n",
      "NumeroCreditosGCanalVPrevius              float64\n",
      "NumeroCreditosLPrevius                    float64\n",
      "NumeroCreditosLEstadoActivosPrevius       float64\n",
      "NumeroCreditosLEstadoPagadosPrevius       float64\n",
      "FechaVinculacionCliente                   float64\n",
      "FechaPrimerUso                            float64\n",
      "FechaUltimoUso                            float64\n",
      "TotalPagosEfectuadosGlobalmentePrevius    float64\n",
      "TotalPagosEfectuadosLocalmentePrevius     float64\n",
      "CodigoAlmacenEntregaTC                     object\n",
      "CodigoMunicipioEntregaTC                  float64\n",
      "TipoMunicipioEntregaTC                     object\n",
      "CanalMunicipioEntregaTC                    object\n",
      "NumeroIntentosFallidos                    float64\n",
      "CupoAprobado                              float64\n",
      "UsoAppWeb                                  object\n",
      "ScoreCrediticio                           float64\n",
      "Genero                                     object\n",
      "Edad                                      float64\n",
      "DiasMora                                    int64\n",
      "PerdidaCartera                              int64\n",
      "dtype: object\n",
      "\n",
      "IdentificadorCliente — 146939 valores únicos\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "\n",
      "FechaEvento — 146939 valores únicos\n",
      "['2022-09-19T13:25:31.867Z' '2023-08-23T11:33:46.417Z'\n",
      " '2022-10-01T14:59:48.920Z' '2022-09-22T21:25:09.187Z'\n",
      " '2023-03-19T17:48:52.310Z' '2022-11-10T22:14:06.450Z'\n",
      " '2022-12-04T21:33:38.130Z' '2022-06-23T09:44:55.610Z'\n",
      " '2022-10-01T13:32:38.053Z' '2023-09-26T13:59:00.617Z'\n",
      " '2023-07-12T12:09:24.737Z' '2022-05-31T09:22:36.477Z'\n",
      " '2022-05-11T15:22:39.510Z' '2023-08-31T18:23:56.993Z'\n",
      " '2022-10-01T22:10:17.433Z']\n",
      "\n",
      "UsabilidadCupo — 107705 valores únicos\n",
      "['0.1184320077740548479' '0.0771590000000000000' '0.1855666666666666667'\n",
      " 'null' '0.4752148710773535878' '0.1561200000000000000'\n",
      " '0.5927940000000000000' '0.8576124728323314114' '0.2435427485988745499'\n",
      " '0.9671000000000000000' '0.1248160000000000000' '0.3081000000000000000'\n",
      " '0.1855000000000000000' '0.0732485183968226177' '0.1845000000000000000']\n",
      "\n",
      "CategoriaPrincipalCredito — 32 valores únicos\n",
      "[nan 'hogar-y-muebles' 'computacion' 'belleza-y-cuidado-personal'\n",
      " 'electronica,-audio-y-video' 'celulares-y-telefonos'\n",
      " 'instrumentos-musicales-2' 'electrodomesticos' 'animales-y-mascotas'\n",
      " 'pines-virtuales' 'ropa-y-accesorios' 'herramientas-y-construccion'\n",
      " 'salud-y-equipamiento-medico' 'accesorios-para-vehiculos'\n",
      " 'alimentos-y-bebidas']\n",
      "\n",
      "DiasMaximosMoraCreditosGenerados — 1767 valores únicos\n",
      "[  0.  nan 129.  49.  31. 141. 122.  78.  66. 266. 140.  68. 101. 113.\n",
      "  72.]\n",
      "\n",
      "NumeroCreditosGPrevius — 208 valores únicos\n",
      "[ 9.  5. 33.  3.  4. 18.  2. 12. 14.  7. 30. 29. 40. 16.  1.]\n",
      "\n",
      "NumeroCreditosGCanalFPrevius — 207 valores únicos\n",
      "[ 9.  4. 33.  3. 18.  2. 11. 14.  7. 30. 29. 16.  1. 17. 27.]\n",
      "\n",
      "NumeroCreditosGEstadoActivosPrevius — 27 valores únicos\n",
      "[ 0.  1.  7.  3.  5.  9. nan  6.  2.  4. 11. 15.  8. 12. 13.]\n",
      "\n",
      "NumeroCreditosGEstadoPagadosPrevius — 207 valores únicos\n",
      "[ 9.  4. 33.  3. 18.  2. 12. 14.  7. 30. 29. 39. 16.  1. 11.]\n",
      "\n",
      "NumeroCreditosGCanalVPrevius — 43 valores únicos\n",
      "[ 0.  1.  7.  4.  2.  6.  3.  5. nan 13. 10.  8.  9. 35. 24.]\n",
      "\n",
      "NumeroCreditosLPrevius — 32 valores únicos\n",
      "[ 0.  1.  3.  2. nan  5.  8.  4.  7.  6. 16. 10. 11. 12. 18.]\n",
      "\n",
      "NumeroCreditosLEstadoActivosPrevius — 10 valores únicos\n",
      "[ 0.  1. nan  2.  6.  3.  5.  4. 13.  9.]\n",
      "\n",
      "NumeroCreditosLEstadoPagadosPrevius — 25 valores únicos\n",
      "[ 0.  1.  2. nan  5.  3.  4.  6. 14.  7.  9.  8. 11. 12. 19.]\n",
      "\n",
      "FechaVinculacionCliente — 3117 valores únicos\n",
      "[43719. 45157. 43545. 44688. 44496. 40189. 44792. 40382. 44171. 45193.\n",
      " 44670. 40177. 40301. 40145. 40508.]\n",
      "\n",
      "FechaPrimerUso — 4895 valores únicos\n",
      "[39065. 39650. 39541. 40019. 44496. 40196. 39075. 40382. 39612. 39806.\n",
      " 39552. 40177. 39617. 39796. 39983.]\n",
      "\n",
      "FechaUltimoUso — 2202 valores únicos\n",
      "[44765. 45158. 44830. 44716. 44807. 44864. 39146. 44692. 44780. 39924.\n",
      " 45111. 44658. 44585. 45128. 44490.]\n",
      "\n",
      "TotalPagosEfectuadosGlobalmentePrevius — 512 valores únicos\n",
      "[ 31.  22. 134.   9.  10.  32.  58.  57.   4.  19.  91. 104. 136.  67.\n",
      "   3.]\n",
      "\n",
      "TotalPagosEfectuadosLocalmentePrevius — 58 valores únicos\n",
      "[ 0.  1.  3.  7.  4. 12.  2. 14. nan  6. 26.  5.  9.  8. 11.]\n",
      "\n",
      "CodigoAlmacenEntregaTC — 83156 valores únicos\n",
      "[316 400688 3325 340786 87061 202 11609 750 164177 596199 12560 1948 1430\n",
      " 1757 1048]\n",
      "\n",
      "CodigoMunicipioEntregaTC — 438 valores únicos\n",
      "[  1.   2.  88.   4.  -1.   3.  63.  nan 130.  67.  49. 136.  46. 134.\n",
      " 135.]\n",
      "\n",
      "TipoMunicipioEntregaTC — 8 valores únicos\n",
      "['PRINCIPAL' 'INTERMEDIO' 'PEQUEÃ‘O' 'GRANDE' 'VIRTUAL' 'POBLADO' nan\n",
      " 'RURAL']\n",
      "\n",
      "CanalMunicipioEntregaTC — 3 valores únicos\n",
      "['Fisico' 'Virtual' nan]\n",
      "\n",
      "NumeroIntentosFallidos — 44 valores únicos\n",
      "[ 0.  1.  5.  3.  4.  2.  8. nan 10. 14.  6. 27. 18.  7. 30.]\n",
      "\n",
      "CupoAprobado — 115 valores únicos\n",
      "[3.0e+10 2.0e+10 5.0e+09 1.0e+10 3.0e+09 1.5e+10 4.0e+09 1.2e+10 8.0e+09\n",
      " 2.0e+09 1.6e+10 6.0e+09 1.3e+10     nan 7.0e+09]\n",
      "\n",
      "UsoAppWeb — 3 valores únicos\n",
      "[nan 'App' 'Web']\n",
      "\n",
      "ScoreCrediticio — 890 valores únicos\n",
      "[865. 726.   0. 837. 487. 616. 686. 689. 896. 598. 430. 718. 611. 849.\n",
      " 458.]\n",
      "\n",
      "Genero — 5 valores únicos\n",
      "['Femenino' 'Masculino' 'Desconocido' nan 27]\n",
      "\n",
      "Edad — 77 valores únicos\n",
      "[37. 38. 35. 34. 55. nan 33. 36. 45. 49. 71. 40. 47. 39. 57.]\n",
      "\n",
      "DiasMora — 1712 valores únicos\n",
      "[   0 1660  889 1520 1673 1373  622  847  740  161 1202 1327   90  842\n",
      " 1512]\n",
      "\n",
      "PerdidaCartera — 2 valores únicos\n",
      "[0 1]\n",
      "Duplicados: 0\n",
      "\n",
      "Porcentaje de nulos (top 10):\n",
      "TotalPagosEfectuadosLocalmentePrevius     26.6\n",
      "TotalPagosEfectuadosGlobalmentePrevius    26.6\n",
      "DiasMaximosMoraCreditosGenerados          25.8\n",
      "UsoAppWeb                                 24.5\n",
      "FechaPrimerUso                            24.4\n",
      "FechaUltimoUso                            24.4\n",
      "CategoriaPrincipalCredito                 22.3\n",
      "NumeroCreditosGCanalFPrevius              21.7\n",
      "NumeroCreditosLEstadoActivosPrevius       21.7\n",
      "NumeroCreditosGEstadoActivosPrevius       21.7\n",
      "NumeroCreditosGPrevius                    21.7\n",
      "NumeroCreditosLEstadoPagadosPrevius       21.7\n",
      "NumeroCreditosLPrevius                    21.7\n",
      "NumeroCreditosGCanalVPrevius              21.7\n",
      "NumeroCreditosGEstadoPagadosPrevius       21.7\n",
      "CodigoMunicipioEntregaTC                   1.2\n",
      "TipoMunicipioEntregaTC                     1.2\n",
      "FechaVinculacionCliente                    1.2\n",
      "Edad                                       1.1\n",
      "CupoAprobado                               0.4\n",
      "NumeroIntentosFallidos                     0.1\n",
      "ScoreCrediticio                            0.1\n",
      "CanalMunicipioEntregaTC                    0.0\n",
      "Genero                                     0.0\n",
      "DiasMora                                   0.0\n",
      "IdentificadorCliente                       0.0\n",
      "CodigoAlmacenEntregaTC                     0.0\n",
      "FechaEvento                                0.0\n",
      "UsabilidadCupo                             0.0\n",
      "PerdidaCartera                             0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/karenaraque/Desktop/practica_cartera_temu/data/DataFramePrueba 2025_08.xlsb\"\n",
    "# Lista de hojas para ver cuáles tiene\n",
    "excel_hojas = pd.ExcelFile(file_path, engine=\"pyxlsb\")\n",
    "print(excel_hojas.sheet_names)\n",
    "clientes = pd.read_excel(file_path, sheet_name=\"DataFramePrueba\", engine=\"pyxlsb\")\n",
    "clientes.head(10)\n",
    "diccionario= pd.read_excel(file_path, sheet_name=\"diccionario\", engine=\"pyxlsb\")\n",
    "diccionario.head(30)\n",
    "print(\"Shape:\", clientes.shape)\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(clientes.dtypes)\n",
    "# Muestra hasta 15 valores únicos por columna\n",
    "for col in clientes.columns:\n",
    "    uniques = clientes[col].unique()\n",
    "    nuniques = len(uniques)\n",
    "    print(f\"\\n{col} — {nuniques} valores únicos\")\n",
    "    print(uniques[:15])  # muestra los primeros 15 valores distintos\n",
    "print(\"Duplicados:\", clientes.duplicated().sum())\n",
    "missing = clientes.isnull().mean().sort_values(ascending=False)\n",
    "print(\"\\nPorcentaje de nulos (top 10):\")\n",
    "print((missing*100).round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd08f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c06f9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Cargando Excel .xlsb ...\n",
      "Hojas encontradas: ['DataFramePrueba', 'diccionario']\n",
      "\n",
      ">>> SHAPE crudo: (146939, 30)\n",
      ">>> Columnas (primeras 30): ['IdentificadorCliente', 'FechaEvento', 'UsabilidadCupo', 'CategoriaPrincipalCredito', 'DiasMaximosMoraCreditosGenerados', 'NumeroCreditosGPrevius', 'NumeroCreditosGCanalFPrevius', 'NumeroCreditosGEstadoActivosPrevius', 'NumeroCreditosGEstadoPagadosPrevius', 'NumeroCreditosGCanalVPrevius', 'NumeroCreditosLPrevius', 'NumeroCreditosLEstadoActivosPrevius', 'NumeroCreditosLEstadoPagadosPrevius', 'FechaVinculacionCliente', 'FechaPrimerUso', 'FechaUltimoUso', 'TotalPagosEfectuadosGlobalmentePrevius', 'TotalPagosEfectuadosLocalmentePrevius', 'CodigoAlmacenEntregaTC', 'CodigoMunicipioEntregaTC', 'TipoMunicipioEntregaTC', 'CanalMunicipioEntregaTC', 'NumeroIntentosFallidos', 'CupoAprobado', 'UsoAppWeb', 'ScoreCrediticio', 'Genero', 'Edad', 'DiasMora', 'PerdidaCartera']\n",
      "\n",
      ">>> Tipos de datos (top 20):\n",
      " IdentificadorCliente                        int64\n",
      "FechaEvento                                object\n",
      "UsabilidadCupo                             object\n",
      "CategoriaPrincipalCredito                  object\n",
      "DiasMaximosMoraCreditosGenerados          float64\n",
      "NumeroCreditosGPrevius                    float64\n",
      "NumeroCreditosGCanalFPrevius              float64\n",
      "NumeroCreditosGEstadoActivosPrevius       float64\n",
      "NumeroCreditosGEstadoPagadosPrevius       float64\n",
      "NumeroCreditosGCanalVPrevius              float64\n",
      "NumeroCreditosLPrevius                    float64\n",
      "NumeroCreditosLEstadoActivosPrevius       float64\n",
      "NumeroCreditosLEstadoPagadosPrevius       float64\n",
      "FechaVinculacionCliente                   float64\n",
      "FechaPrimerUso                            float64\n",
      "FechaUltimoUso                            float64\n",
      "TotalPagosEfectuadosGlobalmentePrevius    float64\n",
      "TotalPagosEfectuadosLocalmentePrevius     float64\n",
      "CodigoAlmacenEntregaTC                     object\n",
      "CodigoMunicipioEntregaTC                  float64\n",
      "dtype: object\n",
      "\n",
      ">>> Duplicados exactos: 0\n",
      "\n",
      ">>> % de nulos (top 20):\n",
      " TotalPagosEfectuadosLocalmentePrevius     26.6\n",
      "TotalPagosEfectuadosGlobalmentePrevius    26.6\n",
      "DiasMaximosMoraCreditosGenerados          25.8\n",
      "UsoAppWeb                                 24.5\n",
      "FechaPrimerUso                            24.4\n",
      "FechaUltimoUso                            24.4\n",
      "CategoriaPrincipalCredito                 22.3\n",
      "NumeroCreditosGCanalFPrevius              21.7\n",
      "NumeroCreditosLEstadoActivosPrevius       21.7\n",
      "NumeroCreditosGEstadoActivosPrevius       21.7\n",
      "NumeroCreditosGPrevius                    21.7\n",
      "NumeroCreditosLEstadoPagadosPrevius       21.7\n",
      "NumeroCreditosLPrevius                    21.7\n",
      "NumeroCreditosGCanalVPrevius              21.7\n",
      "NumeroCreditosGEstadoPagadosPrevius       21.7\n",
      "CodigoMunicipioEntregaTC                   1.2\n",
      "TipoMunicipioEntregaTC                     1.2\n",
      "FechaVinculacionCliente                    1.2\n",
      "Edad                                       1.1\n",
      "CupoAprobado                               0.4\n",
      "dtype: float64\n",
      " - IdentificadorCliente                → únicos=146939\n",
      " - FechaEvento                         → únicos=146939\n",
      " - UsabilidadCupo                      → únicos=107705\n",
      " - CategoriaPrincipalCredito           → únicos=32\n",
      " - DiasMaximosMoraCreditosGenerados    → únicos=1767\n",
      " - NumeroCreditosGPrevius              → únicos=208\n",
      " - NumeroCreditosGCanalFPrevius        → únicos=207\n",
      " - NumeroCreditosGEstadoActivosPrevius → únicos=27\n",
      " - NumeroCreditosGEstadoPagadosPrevius → únicos=207\n",
      " - NumeroCreditosGCanalVPrevius        → únicos=43\n",
      " - NumeroCreditosLPrevius              → únicos=32\n",
      " - NumeroCreditosLEstadoActivosPrevius → únicos=10\n",
      " - NumeroCreditosLEstadoPagadosPrevius → únicos=25\n",
      " - FechaVinculacionCliente             → únicos=3117\n",
      " - FechaPrimerUso                      → únicos=4895\n",
      "\n",
      ">>> Construyendo features (sin fuga) ...\n",
      "\n",
      "===================== EDA PREVIA =====================\n",
      "Shape de X: (146939, 63) | y rate (1)= 0.226\n",
      "Rango de fechas (FechaEvento_dt): 2022-05-01 → 2023-10-31\n",
      "\n",
      "[Balance de clases]\n",
      "                  count    pct\n",
      "PerdidaCartera               \n",
      "no_perdida      113803  0.774\n",
      "perdida          33136  0.226\n",
      "\n",
      "[Correlación numéricas vs target] (top 20)\n",
      " NumeroCreditosGEstadoActivosPrevius                0.400\n",
      "creditos_activos_ratio                             0.396\n",
      "NumeroCreditosLEstadoActivosPrevius                0.216\n",
      "NumeroCreditosLPrevius                             0.201\n",
      "ratio_pagos_local_global                           0.182\n",
      "NumeroCreditosLEstadoPagadosPrevius                0.163\n",
      "Flag_NumeroCreditosGEstadoActivosPrevius_Capped    0.151\n",
      "Flag_PrimerUsoTemu                                 0.145\n",
      "Flag_TotalPagosEfectuadosLocalmentePrevius_NaN     0.137\n",
      "Flag_TotalPagosEfectuadosGlobalmentePrevius_NaN    0.137\n",
      "TotalPagosEfectuadosLocalmentePrevius              0.125\n",
      "Flag_DiasMaximosMoraCreditosGenerados_NaN          0.116\n",
      "UsabilidadCupo                                     0.114\n",
      "Flag_NumeroCreditosLEstadoActivosPrevius_Capped    0.101\n",
      "Flag_NumeroCreditosLEstadoPagadosPrevius_NaN       0.101\n",
      "Flag_NumeroCreditosGPrevius_NaN                    0.101\n",
      "Flag_NumeroCreditosLEstadoActivosPrevius_NaN       0.101\n",
      "Flag_NumeroCreditosLPrevius_NaN                    0.101\n",
      "Flag_NumeroCreditosGCanalVPrevius_NaN              0.101\n",
      "Flag_NumeroCreditosGEstadoPagadosPrevius_NaN       0.101\n",
      "\n",
      "[Correlación numéricas vs target] (bottom 20)\n",
      " Flag_TotalPagosEfectuadosGlobalmentePrevius_Capped   -0.025\n",
      "Flag_NumeroCreditosGEstadoPagadosPrevius_Capped      -0.025\n",
      "Flag_MesesDesdeVinculacion_Capped                    -0.026\n",
      "Flag_PrimerUsoAntesVinc                              -0.031\n",
      "MesesDesdePrimerUso                                  -0.046\n",
      "DiasDesdeUltimoUso                                   -0.048\n",
      "ScoreSinInfo                                         -0.056\n",
      "Edad                                                 -0.068\n",
      "CupoAprobado                                         -0.084\n",
      "NumeroCreditosGPrevius                               -0.087\n",
      "NumeroCreditosGCanalFPrevius                         -0.087\n",
      "TotalPagosEfectuadosGlobalmentePrevius               -0.097\n",
      "MesesDesdeVinculacion                                -0.101\n",
      "log_CupoAprobado                                     -0.102\n",
      "NumeroCreditosGEstadoPagadosPrevius                  -0.111\n",
      "Flag_Score_Negativo                                     NaN\n",
      "Flag_UsabilidadCupo_NaN                                 NaN\n",
      "Flag_MesesDesdeVinculacion_NaN                          NaN\n",
      "Flag_MesesDesdePrimerUso_NaN                            NaN\n",
      "Flag_DiasDesdeUltimoUso_NaN                             NaN\n",
      "\n",
      "[Distribución CategoriaPrincipalCredito → proporción de pérdida]\n",
      "PerdidaCartera               no_perdida  perdida\n",
      "CategoriaPrincipalCredito                       \n",
      "alimentos-y-bebidas               0.455    0.545\n",
      "pines-virtuales                   0.675    0.325\n",
      "bebes12255                        0.689    0.311\n",
      "ropa-y-accesorios                 0.695    0.305\n",
      "camaras-y-accesorios              0.709    0.291\n",
      "otras-categorias                  0.723    0.277\n",
      "juegos-y-juguetes                 0.732    0.268\n",
      "relojes-y-joyas                   0.735    0.265\n",
      "electronica,-audio-y-video        0.739    0.261\n",
      "accesorios-para-vehiculos         0.746    0.254\n",
      "celulares-y-telefonos             0.747    0.253\n",
      "cuidado-personal                  0.748    0.252\n",
      "arte,-papeleria-y-merceria        0.749    0.251\n",
      "electrodomesticos                 0.750    0.250\n",
      "computacion                       0.758    0.242\n",
      "consolas-y-videojuegos            0.763    0.237\n",
      "herramientas-y-construccion       0.764    0.236\n",
      "OtrosRare                         0.769    0.231\n",
      "deportes-y-fitness                0.785    0.215\n",
      "belleza-y-cuidado-personal        0.787    0.213\n",
      "animales-y-mascotas               0.787    0.213\n",
      "hogar-y-muebles                   0.789    0.211\n",
      "salud-y-equipamiento-medico       0.805    0.195\n",
      "Desconocido                       0.840    0.160\n",
      "\n",
      "[Distribución UsoAppWeb → proporción de pérdida]\n",
      "PerdidaCartera  no_perdida  perdida\n",
      "UsoAppWeb                          \n",
      "App                  0.763    0.237\n",
      "Web                  0.775    0.225\n",
      "Desconocido          0.807    0.193\n",
      "\n",
      "[Distribución Genero → proporción de pérdida]\n",
      "PerdidaCartera  no_perdida  perdida\n",
      "Genero                             \n",
      "Masculino            0.753    0.247\n",
      "Desconocido          0.756    0.244\n",
      "Femenino             0.801    0.199\n",
      "\n",
      "[Distribución TipoMunicipioEntregaTC → proporción de pérdida]\n",
      "PerdidaCartera          no_perdida  perdida\n",
      "TipoMunicipioEntregaTC                     \n",
      "VIRTUAL                      0.722    0.278\n",
      "INTERMEDIO                   0.768    0.232\n",
      "PRINCIPAL                    0.796    0.204\n",
      "GRANDE                       0.816    0.184\n",
      "RURAL                        0.825    0.175\n",
      "PEQUEÑO                      0.837    0.163\n",
      "POBLADO                      0.839    0.161\n",
      "Desconocido                  0.940    0.060\n",
      "\n",
      "[Distribución CanalMunicipioEntregaTC → proporción de pérdida]\n",
      "PerdidaCartera           no_perdida  perdida\n",
      "CanalMunicipioEntregaTC                     \n",
      "Desconocido                   0.312    0.688\n",
      "Virtual                       0.722    0.278\n",
      "Fisico                        0.805    0.195\n",
      "\n",
      "[Distribución Flag_PrimerUsoTemu → proporción de pérdida]\n",
      "PerdidaCartera      no_perdida  perdida\n",
      "Flag_PrimerUsoTemu                     \n",
      "1                        0.668    0.332\n",
      "0                        0.809    0.191\n",
      "\n",
      "[Distribución ScoreBucket → proporción de pérdida]\n",
      "PerdidaCartera  no_perdida  perdida\n",
      "ScoreBucket                        \n",
      "bajo                 0.686    0.314\n",
      "medio                0.757    0.243\n",
      "sin_info             0.828    0.172\n",
      "alto                 0.847    0.153\n",
      ">>> Poda de columnas casi-constantes: ['Flag_Score_Negativo', 'Flag_UsabilidadCupo_NaN', 'Flag_MesesDesdeVinculacion_NaN', 'Flag_MesesDesdePrimerUso_NaN', 'Flag_DiasDesdeUltimoUso_NaN']\n",
      "\n",
      "=== Resumen columnas finales ===\n",
      "Numéricas: 51 | Categóricas: 7\n",
      "X shape: (146939, 58) | y rate (1): 0.226\n",
      "\n",
      ">>> Holdout temporal (futuro): train=(117551, 58), holdout=(29388, 58), corte=2023-07-29\n",
      ">>> Checks anti-fuga...\n",
      "OK sin fuga.\n",
      "[HOLDOUT-HGB+Cal] ROC-AUC=0.838 | PR-AUC=0.557 | Brier=0.120\n",
      "[HOLDOUT-HGB+Cal] Base rate y=1: 0.207 | mean(p1)=0.207\n",
      "[HOLDOUT-HGB+Cal] best-F1=0.572 @ thr=0.280\n",
      "[HOLDOUT-HGB+Cal] ConfMatrix @thr=0.280:\n",
      " [[18417  4874]\n",
      " [ 1707  4390]]\n",
      "[HOLDOUT-HGB+Cal] Report @thr=0.280:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.915     0.791     0.848     23291\n",
      "           1      0.474     0.720     0.572      6097\n",
      "\n",
      "    accuracy                          0.776     29388\n",
      "   macro avg      0.695     0.755     0.710     29388\n",
      "weighted avg      0.824     0.776     0.791     29388\n",
      "\n",
      "\n",
      "[HOLDOUT] Barrido de umbrales (métricas por clase y confusión):\n",
      " thr  prec_1  rec_1  prec_0  rec_0   TP    FP   FN    TN\n",
      "0.01   0.225  0.998   0.994  0.101 6082 20931   15  2360\n",
      "0.02   0.246  0.991   0.989  0.205 6044 18521   53  4770\n",
      "0.03   0.248  0.991   0.989  0.212 6040 18343   57  4948\n",
      "0.04   0.275  0.975   0.980  0.326 5944 15693  153  7598\n",
      "0.05   0.293  0.963   0.976  0.391 5873 14188  224  9103\n",
      "0.06   0.295  0.961   0.975  0.398 5862 14012  235  9279\n",
      "0.07   0.331  0.932   0.966  0.507 5684 11473  413 11818\n",
      "0.08   0.333  0.931   0.966  0.511 5678 11396  419 11895\n",
      "0.09   0.333  0.930   0.966  0.513 5673 11340  424 11951\n",
      "0.10   0.349  0.913   0.960  0.555 5565 10359  532 12932\n",
      "0.11   0.349  0.913   0.960  0.555 5565 10359  532 12932\n",
      "0.12   0.365  0.895   0.955  0.592 5454  9499  643 13792\n",
      "0.13   0.369  0.889   0.954  0.602 5423  9277  674 14014\n",
      "0.14   0.386  0.867   0.948  0.639 5285  8400  812 14891\n",
      "0.15   0.387  0.865   0.948  0.642 5275  8343  822 14948\n",
      "0.16   0.397  0.852   0.945  0.661 5194  7885  903 15406\n",
      "0.17   0.419  0.820   0.937  0.702 5000  6934 1097 16357\n",
      "0.18   0.419  0.820   0.937  0.702 5000  6934 1097 16357\n",
      "0.19   0.419  0.820   0.937  0.702 5000  6934 1097 16357\n",
      "0.20   0.419  0.819   0.937  0.703 4995  6913 1102 16378\n",
      "0.21   0.444  0.778   0.928  0.744 4743  5951 1354 17340\n",
      "0.22   0.444  0.778   0.928  0.744 4743  5951 1354 17340\n",
      "0.23   0.444  0.778   0.928  0.744 4743  5951 1354 17340\n",
      "0.24   0.457  0.754   0.922  0.765 4595  5464 1502 17827\n",
      "0.25   0.461  0.746   0.921  0.771 4549  5324 1548 17967\n",
      "0.26   0.463  0.742   0.920  0.775 4524  5251 1573 18040\n",
      "0.27   0.474  0.720   0.915  0.791 4390  4874 1707 18417\n",
      "0.28   0.487  0.693   0.910  0.809 4223  4444 1874 18847\n",
      "0.29   0.492  0.683   0.908  0.816 4162  4294 1935 18997\n",
      "0.30   0.520  0.632   0.898  0.848 3851  3549 2246 19742\n",
      "0.31   0.527  0.621   0.896  0.854 3788  3403 2309 19888\n",
      "0.32   0.527  0.621   0.896  0.854 3788  3403 2309 19888\n",
      "0.33   0.527  0.621   0.896  0.854 3788  3403 2309 19888\n",
      "0.34   0.527  0.621   0.896  0.854 3788  3403 2309 19888\n",
      "0.35   0.533  0.607   0.893  0.861 3701  3237 2396 20054\n",
      "0.36   0.538  0.596   0.891  0.866 3636  3117 2461 20174\n",
      "0.37   0.539  0.596   0.891  0.866 3632  3110 2465 20181\n",
      "0.38   0.539  0.596   0.891  0.866 3632  3110 2465 20181\n",
      "0.39   0.548  0.571   0.886  0.877 3481  2870 2616 20421\n",
      "0.40   0.548  0.571   0.886  0.877 3481  2870 2616 20421\n",
      "0.41   0.548  0.571   0.886  0.877 3479  2867 2618 20424\n",
      "0.42   0.549  0.569   0.886  0.878 3467  2850 2630 20441\n",
      "0.43   0.549  0.569   0.886  0.878 3467  2850 2630 20441\n",
      "0.44   0.549  0.569   0.886  0.878 3467  2850 2630 20441\n",
      "0.45   0.660  0.330   0.845  0.956 2009  1034 4088 22257\n",
      "0.46   0.702  0.290   0.839  0.968 1767   749 4330 22542\n",
      "0.47   0.702  0.290   0.839  0.968 1767   749 4330 22542\n",
      "0.48   0.702  0.290   0.839  0.968 1767   749 4330 22542\n",
      "0.49   0.702  0.290   0.839  0.968 1767   749 4330 22542\n",
      "0.50   0.702  0.290   0.839  0.968 1767   749 4330 22542\n",
      "0.51   0.702  0.290   0.839  0.968 1767   749 4330 22542\n",
      "0.52   0.702  0.290   0.839  0.968 1767   749 4330 22542\n",
      "0.53   0.702  0.290   0.839  0.968 1767   749 4330 22542\n",
      "0.54   0.702  0.290   0.839  0.968 1767   749 4330 22542\n",
      "0.55   0.707  0.284   0.838  0.969 1730   718 4367 22573\n",
      "0.56   0.707  0.284   0.838  0.969 1730   718 4367 22573\n",
      "0.57   0.707  0.284   0.838  0.969 1730   718 4367 22573\n",
      "0.58   0.707  0.284   0.838  0.969 1730   718 4367 22573\n",
      "0.59   0.707  0.284   0.838  0.969 1730   718 4367 22573\n",
      "0.60   0.721  0.256   0.833  0.974 1561   604 4536 22687\n",
      "0.61   0.721  0.256   0.833  0.974 1561   604 4536 22687\n",
      "0.62   0.721  0.256   0.833  0.974 1561   604 4536 22687\n",
      "0.63   0.736  0.227   0.829  0.979 1385   498 4712 22793\n",
      "0.64   0.736  0.227   0.829  0.979 1385   498 4712 22793\n",
      "0.65   0.736  0.227   0.829  0.979 1385   498 4712 22793\n",
      "0.66   0.736  0.227   0.829  0.979 1385   498 4712 22793\n",
      "0.67   0.736  0.227   0.829  0.979 1383   497 4714 22794\n",
      "0.68   0.736  0.227   0.829  0.979 1383   497 4714 22794\n",
      "0.69   0.757  0.166   0.819  0.986 1014   326 5083 22965\n",
      "0.70   0.757  0.166   0.819  0.986 1014   326 5083 22965\n",
      "0.71   0.757  0.166   0.819  0.986 1014   326 5083 22965\n",
      "0.72   0.757  0.165   0.819  0.986 1009   324 5088 22967\n",
      "0.73   0.763  0.144   0.815  0.988  880   274 5217 23017\n",
      "0.74   0.766  0.128   0.813  0.990  781   239 5316 23052\n",
      "0.75   0.766  0.128   0.813  0.990  781   239 5316 23052\n",
      "0.76   0.810  0.023   0.796  0.999  141    33 5956 23258\n",
      "0.77   0.810  0.023   0.796  0.999  141    33 5956 23258\n",
      "0.78   0.810  0.023   0.796  0.999  141    33 5956 23258\n",
      "0.79   0.810  0.023   0.796  0.999  141    33 5956 23258\n",
      "0.80   0.810  0.023   0.796  0.999  141    33 5956 23258\n",
      "0.81   0.848  0.005   0.793  1.000   28     5 6069 23286\n",
      "0.82   0.848  0.005   0.793  1.000   28     5 6069 23286\n",
      "0.83   0.848  0.005   0.793  1.000   28     5 6069 23286\n",
      "0.84   0.848  0.005   0.793  1.000   28     5 6069 23286\n",
      "0.85   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.86   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.87   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.88   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.89   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.90   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.91   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.92   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.93   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.94   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.95   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.96   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.97   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.98   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "0.99   0.000  0.000   0.793  1.000    0     0 6097 23291\n",
      "\n",
      "[Objetivo] PRECISIÓN≥0.80 @thr=0.757\n",
      "  TP=781 FP=239 FN=5316 TN=23052\n",
      "  precision1=0.766 recall1=0.128\n",
      "  %alertados=0.035   %morosos_detectados=0.027\n",
      "\n",
      "[Referencia] Máx-F1 @thr≈0.280 | precision=0.487 | recall=0.693\n",
      "\n",
      "[HOLDOUT] Ganancia/Lift por top-k% (ordenado por riesgo):\n",
      " top_%  n_alertas  morosos_detectados  tasa_moros_topk  lift_vs_base\n",
      "     1        293                 233            0.795          3.83\n",
      "     2        587                 447            0.761          3.67\n",
      "     5       1469                1099            0.748          3.61\n",
      "    10       2938                1960            0.667          3.22\n",
      "    20       5877                3309            0.563          2.71\n",
      "\n",
      ">>> Importancias por permutación (holdout) — top 20\n",
      "                               feature  importance_mean  importance_std\n",
      "   NumeroCreditosGEstadoActivosPrevius         0.151389        0.002233\n",
      "   NumeroCreditosLEstadoActivosPrevius         0.076833        0.003490\n",
      "   NumeroCreditosGEstadoPagadosPrevius         0.060294        0.001762\n",
      "                        UsabilidadCupo         0.031497        0.001093\n",
      "   NumeroCreditosLEstadoPagadosPrevius         0.030140        0.001657\n",
      "                       ScoreCrediticio         0.021852        0.000932\n",
      "                creditos_activos_ratio         0.021245        0.000799\n",
      "                    DiasDesdeUltimoUso         0.010775        0.000622\n",
      "TotalPagosEfectuadosGlobalmentePrevius         0.007841        0.000516\n",
      " TotalPagosEfectuadosLocalmentePrevius         0.007466        0.000344\n",
      "                             UsoAppWeb         0.006982        0.000145\n",
      "                 MesesDesdeVinculacion         0.005836        0.000370\n",
      "                           ScoreBucket         0.004945        0.000526\n",
      "                          CupoAprobado         0.004198        0.000678\n",
      "                TipoMunicipioEntregaTC         0.003840        0.000182\n",
      "             CategoriaPrincipalCredito         0.002244        0.000529\n",
      "                                  Edad         0.002085        0.000589\n",
      "          NumeroCreditosGCanalVPrevius         0.001176        0.000197\n",
      "                                Genero         0.001026        0.000129\n",
      "              ratio_pagos_local_global         0.000804        0.000109\n",
      "SHAP no disponible/omitido: No module named 'shap'\n",
      "\n",
      "[HOLDOUT] Calibración por deciles (p_mean vs y_rate):\n",
      "           index   p_mean   y_rate    n\n",
      "(-0.001, 0.0126] 0.007775 0.007775 3087\n",
      "(0.0126, 0.0331] 0.023610 0.023610 3219\n",
      "(0.0331, 0.0472] 0.041046 0.041046 3021\n",
      "(0.0472, 0.0655] 0.065083 0.065083 2904\n",
      "  (0.0655, 0.13] 0.107157 0.107157 2557\n",
      "   (0.13, 0.206] 0.159775 0.159775 3023\n",
      "   (0.206, 0.28] 0.241924 0.241924 2910\n",
      "   (0.28, 0.445] 0.393670 0.393670 5624\n",
      "  (0.445, 0.459] 0.458763 0.458763  194\n",
      "  (0.459, 0.848] 0.673921 0.673921 2849\n",
      "\n",
      ">>> Artefactos guardados en /Users/karenaraque/Desktop/practica_cartera_temu/notebooks_prueba/artifacts_modelo\n",
      "\n",
      "[SCORING] Resumen probabilidades:\n",
      "mean(p1)= 0.206 | min/max p1= 0.000/0.848\n",
      "[DF_NUEVO] ROC-AUC=0.838 | PR-AUC=0.555 | Brier=0.120\n",
      "[DF_NUEVO] Base rate y=1: 0.207 | mean(p1)=0.206\n",
      "[DF_NUEVO] best-F1=0.573 @ thr=0.292\n",
      "[DF_NUEVO] ConfMatrix @thr=0.292:\n",
      " [[18995  4296]\n",
      " [ 1934  4163]]\n",
      "[DF_NUEVO] Report @thr=0.292:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.908     0.816     0.859     23291\n",
      "           1      0.492     0.683     0.572      6097\n",
      "\n",
      "    accuracy                          0.788     29388\n",
      "   macro avg      0.700     0.749     0.716     29388\n",
      "weighted avg      0.821     0.788     0.800     29388\n",
      "\n",
      "Scoring exportado a artifacts_modelo/scoring_df_nuevo.csv\n",
      ">>> Snippet FastAPI guardado en artifacts_modelo/app_fastapi_snippet.py\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PRUEBA TÉCNICA — CIENTÍFICO DE DATOS (SISTECREDITO)\n",
    "# MODELO DE RIESGO TEMU — VALIDACIÓN TEMPORAL ESTRICTA (SIN FUGA)\n",
    "# ============================================================\n",
    "# Requiere: pandas, numpy, scikit-learn>=1.1, joblib, pyxlsb\n",
    "# Opcional (explicabilidad): shap\n",
    "# ------------------------------------------------------------\n",
    "# Entradas esperadas (hoja Excel .xlsb):\n",
    "#   - 'DataFramePrueba' con columnas:\n",
    "#       FechaEvento, FechaVinculacionCliente, FechaUltimoUso, FechaPrimerUso,\n",
    "#       (muchas num/cat), target: PerdidaCartera (0/1)\n",
    "#   - 'Diccionario' (descripciones de variables)\n",
    "# Notas:\n",
    "#   * 'DiasMora' es estado actual -> ¡NO usarla como predictor! (se excluye explícitamente)\n",
    "#   * 'NumeroCreditosGPrevius' = 0 -> primer uso fue en Temu (feature clave)\n",
    "#   * Sin fuga: cortes temporales, holdout final = 20% más reciente\n",
    "# ============================================================\n",
    "\n",
    "import warnings, os, sys, json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    classification_report, confusion_matrix,\n",
    "    precision_recall_curve, roc_curve\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import joblib\n",
    "\n",
    "# ============================================================\n",
    "# 0) CONFIGURACIÓN GENERAL\n",
    "# ============================================================\n",
    "# >>>> EDITA esta ruta a tu archivo .xlsb <<<<\n",
    "FILE_PATH = \"/Users/karenaraque/Desktop/practica_cartera_temu/data/DataFramePrueba 2025_08.xlsb\"\n",
    "\n",
    "SHEET_DATA = \"DataFramePrueba\"\n",
    "SHEET_DICT = \"diccionario\"\n",
    "\n",
    "TARGET = \"PerdidaCartera\"           # 0/1 (1 = moroso)\n",
    "HOLDOUT_FRAC = 0.20                  # último 20% en el tiempo\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "GOAL_PRECISION = 0.80                # objetivo de precisión en morosos\n",
    "TOP_K_LIST = [1, 2, 5, 10, 20]       # % top riesgo para curvas de ganancia\n",
    "\n",
    "ARTIF_DIR = Path(\"./artifacts_modelo\"); ARTIF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "# ============================================================\n",
    "# 1) INGESTA — .xlsb con dos hojas\n",
    "# ============================================================\n",
    "print(\">>> Cargando Excel .xlsb ...\")\n",
    "try:\n",
    "    excel_hojas = pd.ExcelFile(FILE_PATH, engine=\"pyxlsb\")\n",
    "    print(\"Hojas encontradas:\", excel_hojas.sheet_names)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"No pude abrir el archivo .xlsb en {FILE_PATH}: {e}\")\n",
    "\n",
    "clientes = pd.read_excel(FILE_PATH, sheet_name=SHEET_DATA, engine=\"pyxlsb\")\n",
    "diccionario = pd.read_excel(FILE_PATH, sheet_name=SHEET_DICT, engine=\"pyxlsb\")\n",
    "\n",
    "print(\"\\n>>> SHAPE crudo:\", clientes.shape)\n",
    "print(\">>> Columnas (primeras 30):\", list(clientes.columns)[:30])\n",
    "\n",
    "# Tipos / únicos / nulos (resumen compacto)\n",
    "print(\"\\n>>> Tipos de datos (top 20):\\n\", clientes.dtypes.head(20))\n",
    "print(\"\\n>>> Duplicados exactos:\", clientes.duplicated().sum())\n",
    "missing = clientes.isnull().mean().sort_values(ascending=False)\n",
    "print(\"\\n>>> % de nulos (top 20):\\n\", (missing.head(20)*100).round(1))\n",
    "\n",
    "# Vista rápida de cardinalidades (máx 15 muestras por col)\n",
    "for col in list(clientes.columns)[:15]:\n",
    "    u = clientes[col].nunique(dropna=False)\n",
    "    print(f\" - {col:35s} → únicos={u}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) FEATURE ENGINEERING — SIN FUGA\n",
    "# ============================================================\n",
    "EXCLUDE_FROM_FEATURES = {\n",
    "    # IDs / códigos / columnas de fecha originales / variables que inducen fuga\n",
    "    'IdentificadorCliente','DiasMora',                      # <- EXCLUIR DIASMORA (estado actual)\n",
    "    'FechaEvento','FechaVinculacionCliente','FechaUltimoUso','FechaPrimerUso',\n",
    "    'CodigoAlmacenEntregaTC','CodigoMunicipioEntregaTC'\n",
    "}\n",
    "\n",
    "CAT_CANDIDATES = ['CategoriaPrincipalCredito','UsoAppWeb','Genero',\n",
    "                  'TipoMunicipioEntregaTC','CanalMunicipioEntregaTC']\n",
    "\n",
    "def parse_and_features(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # --- Fechas (UTC-safe) ---\n",
    "    df['FechaEvento_dt'] = (pd.to_datetime(df['FechaEvento'], errors='coerce', utc=True).dt.tz_convert(None))\n",
    "    df['FechaVinculacionCliente_dt'] = pd.to_datetime(df['FechaVinculacionCliente'], errors='coerce', origin='1899-12-30', unit='D')\n",
    "    df['FechaUltimoUso_dt']          = pd.to_datetime(df['FechaUltimoUso'], errors='coerce', origin='1899-12-30', unit='D')\n",
    "    df['FechaPrimerUso_dt']          = pd.to_datetime(df['FechaPrimerUso'], errors='coerce', origin='1904-01-01', unit='D')\n",
    "\n",
    "    # Corrección conservadora del PrimerUso (sin mirar al futuro)\n",
    "    df['Flag_PrimerUsoAntesVinc'] = (df['FechaPrimerUso_dt'] < df['FechaVinculacionCliente_dt']).astype(int)\n",
    "    mask_bad = df['FechaPrimerUso_dt'].isna() | (df['FechaPrimerUso_dt'] < df['FechaVinculacionCliente_dt'])\n",
    "    df['FechaPrimerUso_corr'] = df['FechaPrimerUso_dt']\n",
    "    df.loc[mask_bad, 'FechaPrimerUso_corr'] = df.loc[mask_bad, 'FechaVinculacionCliente_dt']\n",
    "\n",
    "    # Diferencias seguras\n",
    "    def safe_months(a, b):\n",
    "        d = (a - b).dt.days\n",
    "        d = d.where(d.notna(), 0); d = np.where(d < 0, 0, d)\n",
    "        return d / 30.0\n",
    "\n",
    "    def safe_days(a, b):\n",
    "        d = (a - b).dt.days\n",
    "        d = d.where(d.notna(), 0); d = np.where(d < 0, 0, d)\n",
    "        return d\n",
    "\n",
    "    df['MesesDesdeVinculacion'] = safe_months(df['FechaEvento_dt'], df['FechaVinculacionCliente_dt'])\n",
    "    df['MesesDesdePrimerUso']   = safe_months(df['FechaEvento_dt'], df['FechaPrimerUso_corr'])\n",
    "    df['DiasDesdeUltimoUso']    = safe_days(df['FechaEvento_dt'],  df['FechaUltimoUso_dt'])\n",
    "\n",
    "    # --- Numéricas clave (imputaciones conservadoras + flags de nulos/outliers) ---\n",
    "    keep_nums = [\n",
    "        'UsabilidadCupo','DiasMaximosMoraCreditosGenerados','NumeroCreditosGPrevius',\n",
    "        'NumeroCreditosGCanalFPrevius','NumeroCreditosGEstadoActivosPrevius','NumeroCreditosGEstadoPagadosPrevius',\n",
    "        'NumeroCreditosGCanalVPrevius','NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius',\n",
    "        'NumeroCreditosLEstadoPagadosPrevius','TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroIntentosFallidos','CupoAprobado','ScoreCrediticio','Edad',\n",
    "        'MesesDesdeVinculacion','MesesDesdePrimerUso','DiasDesdeUltimoUso'\n",
    "    ]\n",
    "    for c in keep_nums:\n",
    "        if c in df.columns:\n",
    "            df[f'Flag_{c}_NaN'] = df[c].isna().astype(int)\n",
    "\n",
    "    # UsabilidadCupo\n",
    "    df['UsabilidadCupo'] = pd.to_numeric(df.get('UsabilidadCupo', np.nan), errors='coerce')\n",
    "    df['Flag_Usab_Outlier']= ((df['UsabilidadCupo'] < 0) | (df['UsabilidadCupo'] > 2)).astype(int)\n",
    "    df.loc[(df['UsabilidadCupo'] < 0) | (df['UsabilidadCupo'] > 2), 'UsabilidadCupo'] = np.nan\n",
    "    df['UsabilidadCupo'] = df['UsabilidadCupo'].fillna(df['UsabilidadCupo'].median())\n",
    "\n",
    "    # Score & Cupo\n",
    "    df['ScoreSinInfo'] = (df['ScoreCrediticio'].fillna(0) == 0).astype(int)\n",
    "    df['ScoreCrediticio'] = df['ScoreCrediticio'].fillna(df['ScoreCrediticio'][df['ScoreCrediticio']>0].median())\n",
    "    df.loc[df['ScoreCrediticio'] < 0, 'ScoreCrediticio'] = 0\n",
    "    df['log_CupoAprobado'] = np.log1p(df['CupoAprobado'].fillna(df['CupoAprobado'].median()))\n",
    "\n",
    "    # Categóricas limpias\n",
    "    for c in CAT_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna('Desconocido')\n",
    "    if 'Genero' in df.columns:\n",
    "        df['Genero'] = df['Genero'].replace({27:'Desconocido'})\n",
    "\n",
    "    if 'TipoMunicipioEntregaTC' in df.columns:\n",
    "        df['TipoMunicipioEntregaTC'] = df['TipoMunicipioEntregaTC'].replace({'PEQUEÃ‘O':'PEQUEÑO'}).fillna('Desconocido')\n",
    "\n",
    "    # Derivadas de comportamiento\n",
    "    df['Flag_PrimerUsoTemu'] = (df['NumeroCreditosGPrevius'].fillna(0) == 0).astype(int)\n",
    "    df['ratio_pagos_local_global'] = (\n",
    "        df['TotalPagosEfectuadosLocalmentePrevius'].fillna(0) /\n",
    "        (df['TotalPagosEfectuadosGlobalmentePrevius'].fillna(0) + 1.0)\n",
    "    ).clip(0,1)\n",
    "\n",
    "    df['creditos_activos_ratio'] = (\n",
    "        df['NumeroCreditosGEstadoActivosPrevius'].fillna(0) /\n",
    "        (df['NumeroCreditosGPrevius'].fillna(0) + 1.0)\n",
    "    ).clip(0,1)\n",
    "\n",
    "    df['Flag_Score_Negativo'] = (df['ScoreCrediticio'] < 0).astype(int)\n",
    "\n",
    "    # Buckets de score (cuantiles solo con positivos)\n",
    "    df['ScoreBucket'] = 'sin_info'\n",
    "    mask_pos = df['ScoreCrediticio'] > 0\n",
    "    if mask_pos.sum() > 0:\n",
    "        q1, q2 = df.loc[mask_pos, 'ScoreCrediticio'].quantile([0.33, 0.66]).values\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] <= q1), 'ScoreBucket'] = 'bajo'\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] >  q1) & (df['ScoreCrediticio'] <= q2), 'ScoreBucket'] = 'medio'\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] >  q2), 'ScoreBucket'] = 'alto'\n",
    "    df['ScoreBucket'] = pd.Categorical(df['ScoreBucket'], categories=['sin_info','bajo','medio','alto'], ordered=True)\n",
    "\n",
    "    # Winsorización (p99) + flags\n",
    "    def cap_with_flag(s, upper):\n",
    "        s = s.fillna(0)\n",
    "        flag = (s > upper).astype(int)\n",
    "        return np.where(s > upper, upper, s), flag\n",
    "\n",
    "    cap_cols = [\n",
    "        'DiasDesdeUltimoUso','MesesDesdeVinculacion',\n",
    "        'TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroCreditosGPrevius','NumeroCreditosGCanalFPrevius','NumeroCreditosGCanalVPrevius',\n",
    "        'NumeroCreditosGEstadoActivosPrevius','NumeroCreditosGEstadoPagadosPrevius',\n",
    "        'NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius','NumeroCreditosLEstadoPagadosPrevius'\n",
    "    ]\n",
    "    for c in cap_cols:\n",
    "        if c in df.columns:\n",
    "            p99 = df[c].quantile(0.99)\n",
    "            capped, flag = cap_with_flag(df[c], p99)\n",
    "            df[c] = capped\n",
    "            df[f'Flag_{c}_Capped'] = flag\n",
    "\n",
    "    # Agrupar categorías raras\n",
    "    if 'CategoriaPrincipalCredito' in df.columns:\n",
    "        vc = df['CategoriaPrincipalCredito'].astype(str).value_counts(dropna=False)\n",
    "        cutoff = max(1, int(df.shape[0]*0.001))\n",
    "        rare = vc[vc < cutoff].index\n",
    "        df['CategoriaPrincipalCredito'] = df['CategoriaPrincipalCredito'].astype(str)\n",
    "        df.loc[df['CategoriaPrincipalCredito'].isin(rare), 'CategoriaPrincipalCredito'] = 'OtrosRare'\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_feature_lists(df: pd.DataFrame):\n",
    "    drop = set(EXCLUDE_FROM_FEATURES) | {\n",
    "        'FechaEvento_dt','FechaVinculacionCliente_dt','FechaUltimoUso_dt','FechaPrimerUso_dt','FechaPrimerUso_corr'\n",
    "    }\n",
    "    num_base = [\n",
    "        'UsabilidadCupo','MesesDesdeVinculacion','MesesDesdePrimerUso','DiasDesdeUltimoUso',\n",
    "        'NumeroCreditosGPrevius','NumeroCreditosGCanalFPrevius','NumeroCreditosGEstadoActivosPrevius',\n",
    "        'NumeroCreditosGEstadoPagadosPrevius','NumeroCreditosGCanalVPrevius',\n",
    "        'NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius','NumeroCreditosLEstadoPagadosPrevius',\n",
    "        'TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroIntentosFallidos','ScoreCrediticio','ScoreSinInfo','CupoAprobado','log_CupoAprobado','Edad',\n",
    "        'Flag_PrimerUsoAntesVinc','Flag_Usab_Outlier','ratio_pagos_local_global','creditos_activos_ratio',\n",
    "        'Flag_Score_Negativo'\n",
    "    ]\n",
    "    num_dyn = [c for c in df.columns if c.startswith('Flag_') and (c.endswith('_NaN') or c.endswith('_Capped'))]\n",
    "    num_final = [c for c in (num_base + num_dyn) if c in df.columns and c not in drop]\n",
    "\n",
    "    cat_final = [c for c in (CAT_CANDIDATES + ['Flag_PrimerUsoTemu','ScoreBucket']) if c in df.columns and c not in drop]\n",
    "\n",
    "    # Evitar solape\n",
    "    overlap = set(num_final) & set(cat_final)\n",
    "    if overlap:\n",
    "        print(\">>> Aviso: había columnas en num y cat. Se quitan de num:\", sorted(list(overlap)))\n",
    "        num_final = [c for c in num_final if c not in overlap]\n",
    "\n",
    "    # Únicas y ordenadas\n",
    "    num_final = list(dict.fromkeys(num_final))\n",
    "    cat_final = list(dict.fromkeys(cat_final))\n",
    "    return num_final, cat_final\n",
    "\n",
    "\n",
    "def prune_low_variance(X: pd.DataFrame):\n",
    "    low_var = [c for c in X.columns if X[c].nunique(dropna=False) <= 1]\n",
    "    if low_var:\n",
    "        print(\">>> Poda de columnas casi-constantes:\", low_var)\n",
    "        X = X.drop(columns=low_var, errors='ignore')\n",
    "    else:\n",
    "        print(\">>> Poda de columnas casi-constantes: ninguna\")\n",
    "    return X, low_var if low_var else []\n",
    "\n",
    "\n",
    "def build_model(cat_cols, num_cols):\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('imp', SimpleImputer(strategy='most_frequent')),\n",
    "                ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "            ]), cat_cols),\n",
    "            ('num', Pipeline(steps=[\n",
    "                ('imp', SimpleImputer(strategy='median'))\n",
    "            ]), num_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    clf = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.07, max_leaf_nodes=31, l2_regularization=1.0,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    pipe = Pipeline([('pre', pre), ('clf', clf)])\n",
    "    return pipe\n",
    "\n",
    "def evaluate_proba(y_true, y_proba, name=\"model\"):\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    ap  = average_precision_score(y_true, y_proba)\n",
    "    br  = brier_score_loss(y_true, y_proba)\n",
    "    p, r, thr = precision_recall_curve(y_true, y_proba)\n",
    "    f1 = 2*p*r/(p+r+1e-12)\n",
    "    j  = np.argmax(f1)\n",
    "    best_thr = thr[j-1] if j>0 and j-1 < len(thr) else 0.5\n",
    "    y_hat = (y_proba >= best_thr).astype(int)\n",
    "\n",
    "    print(f\"[{name}] ROC-AUC={auc:.3f} | PR-AUC={ap:.3f} | Brier={br:.3f}\")\n",
    "    print(f\"[{name}] Base rate y=1: {y_true.mean():.3f} | mean(p1)={np.mean(y_proba):.3f}\")\n",
    "    print(f\"[{name}] best-F1={f1[j]:.3f} @ thr={best_thr:.3f}\")\n",
    "    print(f\"[{name}] ConfMatrix @thr={best_thr:.3f}:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    print(f\"[{name}] Report @thr={best_thr:.3f}:\\n\", classification_report(y_true, y_hat, digits=3))\n",
    "    return dict(roc_auc=auc, pr_auc=ap, brier=br, thr=best_thr, p=p, r=r, thr_curve=thr)\n",
    "\n",
    "def sweep_thresholds(y_true, y_proba, thresholds):\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        yhat = (y_proba >= t).astype(int)\n",
    "        TN, FP, FN, TP = confusion_matrix(y_true, yhat).ravel()\n",
    "        prec1 = TP/(TP+FP) if TP+FP>0 else 0.0\n",
    "        rec1  = TP/(TP+FN) if TP+FN>0 else 0.0\n",
    "        prec0 = TN/(TN+FN) if TN+FN>0 else 0.0\n",
    "        rec0  = TN/(TN+FP) if TN+FP>0 else 0.0\n",
    "        rows.append({\n",
    "            \"thr\": round(t,3),\n",
    "            \"prec_1\": round(prec1,3), \"rec_1\": round(rec1,3),\n",
    "            \"prec_0\": round(prec0,3), \"rec_0\": round(rec0,3),\n",
    "            \"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN\n",
    "        })\n",
    "    tab = pd.DataFrame(rows)\n",
    "    print(\"\\n[HOLDOUT] Barrido de umbrales (métricas por clase y confusión):\")\n",
    "    print(tab.to_string(index=False))\n",
    "    return tab\n",
    "\n",
    "def topk_gain_table(y_true, y_proba, top_k_list):\n",
    "    \"\"\"Curva de ganancia / lift: ¿qué capturo si alerto al top-k% más riesgoso?\"\"\"\n",
    "    n = len(y_true)\n",
    "    order = np.argsort(-y_proba)\n",
    "    y_sorted = np.array(y_true)[order]\n",
    "    out = []\n",
    "    base_rate = y_true.mean()\n",
    "    cum_ones = np.cumsum(y_sorted)\n",
    "    for k in top_k_list:\n",
    "        m = max(1, int(n * k / 100.0))\n",
    "        tp_k = int(cum_ones[m-1])\n",
    "        rate_k = tp_k / m\n",
    "        lift_k = rate_k / base_rate if base_rate>0 else np.nan\n",
    "        out.append({\"top_%\": k, \"n_alertas\": m, \"morosos_detectados\": tp_k,\n",
    "                    \"tasa_moros_topk\": round(rate_k,3), \"lift_vs_base\": round(lift_k,2)})\n",
    "    tab = pd.DataFrame(out)\n",
    "    print(\"\\n[HOLDOUT] Ganancia/Lift por top-k% (ordenado por riesgo):\")\n",
    "    print(tab.to_string(index=False))\n",
    "    return tab\n",
    "\n",
    "# ============================================================\n",
    "# 3) PIPELINE EDA + MODELADO\n",
    "# ============================================================\n",
    "print(\"\\n>>> Construyendo features (sin fuga) ...\")\n",
    "clientes = parse_and_features(clientes)\n",
    "assert TARGET in clientes.columns, f\"No encuentro columna target '{TARGET}'\"\n",
    "\n",
    "num_final, cat_final = build_feature_lists(clientes)\n",
    "X = clientes[num_final + cat_final].copy()\n",
    "y = clientes[TARGET].astype(int).copy()\n",
    "\n",
    "print(\"\\n===================== EDA PREVIA =====================\")\n",
    "print(f\"Shape de X: {X.shape} | y rate (1)= {y.mean():.3f}\")\n",
    "print(\"Rango de fechas (FechaEvento_dt):\",\n",
    "      str(clientes['FechaEvento_dt'].min().date()), \"→\", str(clientes['FechaEvento_dt'].max().date()))\n",
    "vc = y.value_counts().rename({0:'no_perdida',1:'perdida'})\n",
    "print(\"\\n[Balance de clases]\\n\", pd.concat([vc, (vc/vc.sum()).round(3).rename('pct')], axis=1))\n",
    "\n",
    "# Correlaciones (numéricas) — diagnóstico rápido\n",
    "num_cols_for_corr = [c for c in X.columns if is_numeric_dtype(X[c])]\n",
    "if num_cols_for_corr:\n",
    "    corrs = X[num_cols_for_corr].corrwith(y).sort_values(ascending=False)\n",
    "    print(\"\\n[Correlación numéricas vs target] (top 20)\\n\", corrs.head(20).round(3).to_string())\n",
    "    print(\"\\n[Correlación numéricas vs target] (bottom 20)\\n\", corrs.tail(20).round(3).to_string())\n",
    "\n",
    "# Crosstabs (categóricas de baja/média cardinalidad)\n",
    "for c in [c for c in cat_final if X[c].nunique(dropna=False) <= 25]:\n",
    "    tab = pd.crosstab(X[c], y, normalize='index').rename(columns={0:'no_perdida',1:'perdida'}).round(3)\n",
    "    print(f\"\\n[Distribución {c} → proporción de pérdida]\")\n",
    "    print(tab.sort_values('perdida', ascending=False).to_string())\n",
    "\n",
    "# Podas útiles\n",
    "X, dropped_lowvar = prune_low_variance(X)\n",
    "print(\"\\n=== Resumen columnas finales ===\")\n",
    "print(f\"Numéricas: {len([c for c in X.columns if c in num_final])} | Categóricas: {len([c for c in X.columns if c in cat_final])}\")\n",
    "print(\"X shape:\", X.shape, \"| y rate (1):\", y.mean().round(3))\n",
    "\n",
    "# ============================================================\n",
    "# 4) SPLIT TEMPORAL — HOLDOUT “FUTURO”\n",
    "# ============================================================\n",
    "cutoff = clientes['FechaEvento_dt'].quantile(1-HOLDOUT_FRAC)\n",
    "train_idx = clientes['FechaEvento_dt'] <= cutoff\n",
    "hold_idx  = clientes['FechaEvento_dt'] >  cutoff\n",
    "X_tr, X_ho = X.loc[train_idx], X.loc[hold_idx]\n",
    "y_tr, y_ho = y.loc[train_idx], y.loc[hold_idx]\n",
    "print(f\"\\n>>> Holdout temporal (futuro): train={X_tr.shape}, holdout={X_ho.shape}, corte={pd.Timestamp(cutoff).date()}\")\n",
    "\n",
    "# Checks anti-fuga\n",
    "print(\">>> Checks anti-fuga...\")\n",
    "assert set(X_tr.index).isdisjoint(set(X_ho.index)), \"Solapamiento entre train y holdout\"\n",
    "assert clientes.loc[X_tr.index,'FechaEvento_dt'].max() <= cutoff, \"Train tiene fechas > cutoff\"\n",
    "assert clientes.loc[X_ho.index,'FechaEvento_dt'].min() >  cutoff, \"Holdout tiene fechas <= cutoff\"\n",
    "print(\"OK sin fuga.\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) ENTRENAR + CALIBRAR + EVALUAR (HOLDOUT)\n",
    "# ============================================================\n",
    "pipe = build_model(cat_final, [c for c in X.columns if c not in cat_final])\n",
    "sw = compute_sample_weight(\"balanced\", y_tr)   # compensar desbalance sin oversampling temporal\n",
    "pipe.fit(X_tr, y_tr, clf__sample_weight=sw)\n",
    "\n",
    "# Calibración (isotónica) usando sólo holdout (no fuga porque el score base viene de train)\n",
    "cal = CalibratedClassifierCV(pipe, method='isotonic', cv='prefit')\n",
    "cal.fit(X_ho, y_ho)\n",
    "\n",
    "proba_ho = cal.predict_proba(X_ho)[:,1]\n",
    "m = evaluate_proba(y_ho, proba_ho, name=\"HOLDOUT-HGB+Cal\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) UMBRALES DE NEGOCIO — PRECISIÓN OBJETIVO vs RECALL\n",
    "# ============================================================\n",
    "# Barrido rápido de umbrales:\n",
    "grid_thr = np.unique(np.round(np.linspace(0.01, 0.99, 99), 3))\n",
    "tab_thr = sweep_thresholds(y_ho, proba_ho, grid_thr)\n",
    "tab_thr.to_csv(ARTIF_DIR/\"holdout_threshold_sweep.csv\", index=False)\n",
    "\n",
    "# Objetivo: precisión >= GOAL_PRECISION, con el MAYOR recall posible\n",
    "p, r, thr = m[\"p\"], m[\"r\"], m[\"thr_curve\"]\n",
    "idx = np.where(p >= GOAL_PRECISION)[0]\n",
    "if len(idx) == 0:\n",
    "    print(f\"\\n>>> No existe punto con precisión ≥ {GOAL_PRECISION:.2f}. \"\n",
    "          f\"Con variables actuales, alcanzar p≥{GOAL_PRECISION:.2f} reduce demasiado el recall.\")\n",
    "    thr_goal = None\n",
    "else:\n",
    "    k = idx[np.argmax(r[idx])]\n",
    "    thr_goal = thr[k-1] if k>0 else 0.5\n",
    "    yhat = (proba_ho >= thr_goal).astype(int)\n",
    "    TN, FP, FN, TP = confusion_matrix(y_ho, yhat).ravel()\n",
    "    prec1, rec1 = (TP/(TP+FP) if TP+FP>0 else 0), (TP/(TP+FN) if TP+FN>0 else 0)\n",
    "    print(f\"\\n[Objetivo] PRECISIÓN≥{GOAL_PRECISION:.2f} @thr={thr_goal:.3f}\")\n",
    "    print(f\"  TP={TP} FP={FP} FN={FN} TN={TN}\")\n",
    "    print(f\"  precision1={prec1:.3f} recall1={rec1:.3f}\")\n",
    "    print(f\"  %alertados={(TP+FP)/len(y_ho):.3f}   %morosos_detectados={TP/len(y_ho):.3f}\")\n",
    "\n",
    "# Punto de referencia: Máx-F1\n",
    "j  = np.argmax(2*m[\"p\"]*m[\"r\"]/(m[\"p\"]+m[\"r\"]+1e-12))\n",
    "thr_star = thr[j-1] if j>0 else 0.5\n",
    "print(f\"\\n[Referencia] Máx-F1 @thr≈{thr_star:.3f} | precision={m['p'][j]:.3f} | recall={m['r'][j]:.3f}\")\n",
    "\n",
    "# Curva de ganancia / lift por top-k%\n",
    "_ = topk_gain_table(y_ho, proba_ho, TOP_K_LIST)\n",
    "\n",
    "# ============================================================\n",
    "# 7) IMPORTANCIA DE VARIABLES — PERMUTATION IMPORTANCE (HOLDOUT)\n",
    "# ============================================================\n",
    "# Se usa sobre el pipeline calibrado: para PI usamos el pipe base (sin la capa Calibrated)\n",
    "print(\"\\n>>> Importancias por permutación (holdout) — top 20\")\n",
    "pi = permutation_importance(pipe, X_ho, y_ho, n_repeats=5, random_state=RANDOM_STATE, scoring='average_precision')\n",
    "pi_df = pd.DataFrame({\"feature\": X_ho.columns, \"importance_mean\": pi.importances_mean, \"importance_std\": pi.importances_std})\n",
    "pi_df = pi_df.sort_values(\"importance_mean\", ascending=False)\n",
    "print(pi_df.head(20).to_string(index=False))\n",
    "pi_df.to_csv(ARTIF_DIR/\"permutation_importance_holdout.csv\", index=False)\n",
    "\n",
    "# (Opcional) SHAP si está instalado — útil para slide de explicabilidad\n",
    "try:\n",
    "    import shap\n",
    "    print(\"\\n>>> Calculando SHAP summary (muestra de holdout). Esto es opcional.\")\n",
    "    # Tomamos una muestra para que sea rápido:\n",
    "    sample_idx = np.random.RandomState(RANDOM_STATE).choice(X_ho.index, size=min(5000, X_ho.shape[0]), replace=False)\n",
    "    X_sample = X_ho.loc[sample_idx]\n",
    "    # Extraemos la matriz post-preprocesamiento:\n",
    "    preproc = pipe.named_steps['pre']\n",
    "    X_trans = preproc.fit(X_tr, y_tr).transform(X_sample)  # fit con train (no fuga)\n",
    "    model = pipe.named_steps['clf']\n",
    "    explainer = shap.Explainer(model.predict_proba, X_trans)\n",
    "    shap_values = explainer(X_trans)\n",
    "    # Guardamos datos tabulares con valores medios absolutos por feature transformada\n",
    "    sv_abs = np.abs(shap_values.values[...,1]).mean(axis=0)\n",
    "    shap_df = pd.DataFrame({\"feature_transformed_id\": np.arange(sv_abs.shape[0]), \"mean_abs_shap\": sv_abs})\n",
    "    shap_df.to_csv(ARTIF_DIR/\"shap_mean_abs_holdout.csv\", index=False)\n",
    "    print(\"SHAP (mean|abs|) por feature transformada exportado.\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP no disponible/omitido:\", e)\n",
    "\n",
    "# ============================================================\n",
    "# 8) CALIBRACIÓN POR DECILES + CURVA PR\n",
    "# ============================================================\n",
    "def decile_calibration(y_true, y_proba, name=\"holdout\"):\n",
    "    bins = pd.qcut(y_proba, q=10, duplicates='drop')\n",
    "    tab = pd.DataFrame({\"p\":y_proba, \"y\":y_true}).groupby(bins, observed=True).agg(\n",
    "        p_mean=('p','mean'),\n",
    "        y_rate=('y','mean'),\n",
    "        n=('p','size')\n",
    "    ).reset_index()\n",
    "    print(f\"\\n[{name}] Calibración por deciles (p_mean vs y_rate):\")\n",
    "    print(tab.to_string(index=False))\n",
    "    tab.to_csv(ARTIF_DIR/f\"calibracion_deciles_{name}.csv\", index=False)\n",
    "    return tab\n",
    "_ = decile_calibration(y_ho, proba_ho, name=\"HOLDOUT\")\n",
    "\n",
    "# Guardar artefactos\n",
    "bundle = {\n",
    "    \"model_calibrado\": cal,\n",
    "    \"columns_after_prune\": list(X.columns),\n",
    "    \"num_final\": num_final,\n",
    "    \"cat_final\": cat_final,\n",
    "    \"cutoff\": cutoff,\n",
    "    \"goal_precision\": GOAL_PRECISION\n",
    "}\n",
    "joblib.dump(bundle, ARTIF_DIR/\"modelo_calibrado.joblib\")\n",
    "pd.DataFrame({\"metric\":[\"ROC_AUC\",\"PR_AUC\",\"Brier\"],\n",
    "              \"value\":[m['roc_auc'], m['pr_auc'], m['brier']]}).to_csv(ARTIF_DIR/\"holdout_metrics.csv\", index=False)\n",
    "print(f\"\\n>>> Artefactos guardados en {ARTIF_DIR.resolve()}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9) FUNCIÓN DE SCORING PARA DATA NUEVA (SIN VER EL FUTURO)\n",
    "# ============================================================\n",
    "def score_future(df_nuevo_raw: pd.DataFrame, artif_path=ARTIF_DIR/\"modelo_calibrado.joblib\", target_col=TARGET,\n",
    "                 export_csv=True):\n",
    "    bundle = joblib.load(artif_path)\n",
    "    model  = bundle[\"model_calibrado\"]\n",
    "    cols_ok = bundle[\"columns_after_prune\"]\n",
    "    df_nuevo = parse_and_features(df_nuevo_raw)\n",
    "    Xn = df_nuevo[[c for c in cols_ok if c in df_nuevo.columns]].copy()\n",
    "    # alinear columnas esperadas\n",
    "    Xn = Xn.reindex(columns=cols_ok, fill_value=np.nan)\n",
    "    proba1 = model.predict_proba(Xn)[:,1]\n",
    "    proba0 = 1 - proba1\n",
    "\n",
    "    print(\"\\n[SCORING] Resumen probabilidades:\")\n",
    "    print(f\"mean(p1)= {proba1.mean():.3f} | min/max p1= {proba1.min():.3f}/{proba1.max():.3f}\")\n",
    "\n",
    "    if target_col in df_nuevo.columns:\n",
    "        y_true = df_nuevo[target_col].astype(int)\n",
    "        _ = evaluate_proba(y_true, proba1, name=\"DF_NUEVO\")\n",
    "\n",
    "    out = pd.DataFrame({\"p_no_perdida\": proba0, \"p_perdida\": proba1})\n",
    "    if export_csv:\n",
    "        out_path = ARTIF_DIR/\"scoring_df_nuevo.csv\"\n",
    "        out.to_csv(out_path, index=False)\n",
    "        print(f\"Scoring exportado a {out_path}\")\n",
    "    return out\n",
    "\n",
    "# -------- QUICK TEST (comentado): usar el 20% más reciente como “futuro” real\n",
    "df_nuevo = clientes.loc[clientes['FechaEvento_dt'] > cutoff].copy()\n",
    "_ = score_future(df_nuevo)\n",
    "\n",
    "# ============================================================\n",
    "# 10) (OPCIONAL) ESQUELETO FASTAPI para despliegue\n",
    "#    Guardar como app.py y ejecutar: uvicorn app:app --reload\n",
    "# ============================================================\n",
    "FASTAPI_SNIPPET = f\"\"\"\n",
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "app = FastAPI(title=\"API Riesgo Temu\", version=\"1.0\")\n",
    "\n",
    "BUNDLE_PATH = \"{str((ARTIF_DIR/'modelo_calibrado.joblib').resolve())}\"\n",
    "\n",
    "from __main__ import parse_and_features  # reutilizamos funciones de arriba\n",
    "\n",
    "bundle = joblib.load(BUNDLE_PATH)\n",
    "model = bundle[\"model_calibrado\"]\n",
    "cols_ok = bundle[\"columns_after_prune\"]\n",
    "\n",
    "@app.post(\"/score\")\n",
    "def score(payload: dict):\n",
    "    df = pd.DataFrame([payload])\n",
    "    df = parse_and_features(df)\n",
    "    Xn = df.reindex(columns=cols_ok, fill_value={{}})\n",
    "    proba1 = model.predict_proba(Xn)[:,1]\n",
    "    return {{\"p_perdida\": float(proba1[0])}}\n",
    "\"\"\"\n",
    "with open(ARTIF_DIR/\"app_fastapi_snippet.py\", \"w\") as f:\n",
    "    f.write(FASTAPI_SNIPPET)\n",
    "print(f\">>> Snippet FastAPI guardado en {ARTIF_DIR/'app_fastapi_snippet.py'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "751edaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> SHAPE crudo: (146939, 30)\n",
      ">>> Columnas (primeras 20): ['IdentificadorCliente', 'FechaEvento', 'UsabilidadCupo', 'CategoriaPrincipalCredito', 'DiasMaximosMoraCreditosGenerados', 'NumeroCreditosGPrevius', 'NumeroCreditosGCanalFPrevius', 'NumeroCreditosGEstadoActivosPrevius', 'NumeroCreditosGEstadoPagadosPrevius', 'NumeroCreditosGCanalVPrevius', 'NumeroCreditosLPrevius', 'NumeroCreditosLEstadoActivosPrevius', 'NumeroCreditosLEstadoPagadosPrevius', 'FechaVinculacionCliente', 'FechaPrimerUso', 'FechaUltimoUso', 'TotalPagosEfectuadosGlobalmentePrevius', 'TotalPagosEfectuadosLocalmentePrevius', 'CodigoAlmacenEntregaTC', 'CodigoMunicipioEntregaTC']\n",
      "\n",
      ">>> Construyendo features (sin fuga) ...\n",
      "\n",
      "===================== EDA PREVIA (SIN FUGA) =====================\n",
      "Shape de X: (146939, 65) | y rate (1)= 0.226\n",
      "Rango de fechas (FechaEvento_dt): 2022-05-01 → 2023-10-31\n",
      "\n",
      "[Balance de clases]\n",
      "                 count    pct\n",
      "PerdidaCartera               \n",
      "no_perdida      113803  0.774\n",
      "perdida          33136  0.226\n",
      "\n",
      "[Resumen por columna] (primeras 30 filas)\n",
      "                                  columna    dtype  %nulos  n_unicos                                                      top5_valores\n",
      "                              ScoreBucket category     0.0         4 {'alto': 41836, 'medio': 40654, 'bajo': 40652, 'sin_info': 23797}\n",
      "                             CupoAprobado  float64     0.4       115                                                                {}\n",
      "                       DiasDesdeUltimoUso  float64     0.0      1096                                                                {}\n",
      "                                     Edad  float64     0.0        76                                                                {}\n",
      "                      MesesDesdePrimerUso  float64     0.0      2901                                                                {}\n",
      "                    MesesDesdeVinculacion  float64     0.0      2366                                                                {}\n",
      "             NumeroCreditosGCanalFPrevius  float64     0.0        76                                                                {}\n",
      "             NumeroCreditosGCanalVPrevius  float64     0.0         6                                                                {}\n",
      "      NumeroCreditosGEstadoActivosPrevius  float64     0.0         6                                                                {}\n",
      "      NumeroCreditosGEstadoPagadosPrevius  float64     0.0        76                                                                {}\n",
      "                   NumeroCreditosGPrevius  float64     0.0        77                                                                {}\n",
      "      NumeroCreditosLEstadoActivosPrevius  float64     0.0         2                                                                {}\n",
      "      NumeroCreditosLEstadoPagadosPrevius  float64     0.0         4                                                                {}\n",
      "                   NumeroCreditosLPrevius  float64     0.0         5                                                                {}\n",
      "                   NumeroIntentosFallidos  float64     0.0        43                                                                {}\n",
      "                          ScoreCrediticio  float64     0.0       883                                                                {}\n",
      "   TotalPagosEfectuadosGlobalmentePrevius  float64     0.0       220                                                                {}\n",
      "    TotalPagosEfectuadosLocalmentePrevius  float64     0.0        12                                                                {}\n",
      "                           UsabilidadCupo  float64     0.0    101998                                                                {}\n",
      "                   creditos_activos_ratio  float64     0.0       554                                                                {}\n",
      "                         log_CupoAprobado  float64     0.0       114                                                                {}\n",
      "                 ratio_pagos_local_global  float64     0.0      1628                                                                {}\n",
      "                    Flag_CupoAprobado_NaN    int64     0.0         2                                                                {}\n",
      "           Flag_DiasDesdeUltimoUso_Capped    int64     0.0         2                                                                {}\n",
      "              Flag_DiasDesdeUltimoUso_NaN    int64     0.0         1                                                                {}\n",
      "Flag_DiasMaximosMoraCreditosGenerados_NaN    int64     0.0         2                                                                {}\n",
      "                            Flag_Edad_NaN    int64     0.0         2                                                                {}\n",
      "                            Flag_Edad_Out    int64     0.0         2                                                                {}\n",
      "             Flag_MesesDesdePrimerUso_NaN    int64     0.0         1                                                                {}\n",
      "        Flag_MesesDesdeVinculacion_Capped    int64     0.0         2                                                                {}\n",
      "\n",
      "[Correlación numéricas vs target] (top 20)\n",
      "NumeroCreditosGEstadoActivosPrevius                0.400\n",
      "creditos_activos_ratio                             0.396\n",
      "NumeroCreditosLEstadoActivosPrevius                0.216\n",
      "NumeroCreditosLPrevius                             0.201\n",
      "ratio_pagos_local_global                           0.182\n",
      "NumeroCreditosLEstadoPagadosPrevius                0.164\n",
      "Flag_NumeroCreditosGEstadoActivosPrevius_Capped    0.151\n",
      "Flag_PrimerUsoTemu                                 0.145\n",
      "Flag_TotalPagosEfectuadosLocalmentePrevius_NaN     0.137\n",
      "Flag_TotalPagosEfectuadosGlobalmentePrevius_NaN    0.137\n",
      "TotalPagosEfectuadosLocalmentePrevius              0.125\n",
      "Flag_DiasMaximosMoraCreditosGenerados_NaN          0.116\n",
      "UsabilidadCupo                                     0.114\n",
      "Flag_NumeroCreditosLEstadoActivosPrevius_Capped    0.101\n",
      "Flag_NumeroCreditosGPrevius_NaN                    0.101\n",
      "Flag_NumeroCreditosLEstadoPagadosPrevius_NaN       0.101\n",
      "Flag_NumeroCreditosLEstadoActivosPrevius_NaN       0.101\n",
      "Flag_NumeroCreditosGEstadoPagadosPrevius_NaN       0.101\n",
      "Flag_NumeroCreditosGEstadoActivosPrevius_NaN       0.101\n",
      "Flag_NumeroCreditosGCanalVPrevius_NaN              0.101\n",
      "\n",
      "[Correlación numéricas vs target] (bottom 20)\n",
      "Flag_NumeroCreditosGPrevius_Capped                   -0.026\n",
      "Flag_NumeroCreditosGCanalFPrevius_Capped             -0.027\n",
      "Flag_TotalPagosEfectuadosGlobalmentePrevius_Capped   -0.028\n",
      "Flag_NumeroCreditosGEstadoPagadosPrevius_Capped      -0.029\n",
      "Flag_PrimerUsoAntesVinc                              -0.031\n",
      "MesesDesdePrimerUso                                  -0.046\n",
      "DiasDesdeUltimoUso                                   -0.048\n",
      "ScoreSinInfo                                         -0.056\n",
      "Edad                                                 -0.069\n",
      "CupoAprobado                                         -0.084\n",
      "NumeroCreditosGPrevius                               -0.087\n",
      "NumeroCreditosGCanalFPrevius                         -0.088\n",
      "TotalPagosEfectuadosGlobalmentePrevius               -0.098\n",
      "MesesDesdeVinculacion                                -0.101\n",
      "log_CupoAprobado                                     -0.102\n",
      "NumeroCreditosGEstadoPagadosPrevius                  -0.112\n",
      "Flag_UltimoUsoPosterior                                 NaN\n",
      "Flag_MesesDesdeVinculacion_NaN                          NaN\n",
      "Flag_MesesDesdePrimerUso_NaN                            NaN\n",
      "Flag_DiasDesdeUltimoUso_NaN                             NaN\n",
      "\n",
      "[Distribución Genero → proporción de pérdida por categoría]\n",
      "PerdidaCartera  no_perdida  perdida\n",
      "Genero                             \n",
      "Masculino            0.753    0.247\n",
      "Desconocido          0.756    0.244\n",
      "Femenino             0.801    0.199\n",
      "\n",
      "[Distribución TipoMunicipioEntregaTC → proporción de pérdida por categoría]\n",
      "PerdidaCartera          no_perdida  perdida\n",
      "TipoMunicipioEntregaTC                     \n",
      "VIRTUAL                      0.722    0.278\n",
      "INTERMEDIO                   0.768    0.232\n",
      "PRINCIPAL                    0.796    0.204\n",
      "GRANDE                       0.816    0.184\n",
      "RURAL                        0.825    0.175\n",
      "PEQUEÑO                      0.837    0.163\n",
      "POBLADO                      0.839    0.161\n",
      "Desconocido                  0.940    0.060\n",
      "\n",
      "[Distribución CanalMunicipioEntregaTC → proporción de pérdida por categoría]\n",
      "PerdidaCartera           no_perdida  perdida\n",
      "CanalMunicipioEntregaTC                     \n",
      "Desconocido                   0.312    0.688\n",
      "Virtual                       0.722    0.278\n",
      "Fisico                        0.805    0.195\n",
      "\n",
      "[Distribución UsoAppWeb → proporción de pérdida por categoría]\n",
      "PerdidaCartera  no_perdida  perdida\n",
      "UsoAppWeb                          \n",
      "App                  0.763    0.237\n",
      "Web                  0.775    0.225\n",
      "Desconocido          0.807    0.193\n",
      "\n",
      "[Distribución CategoriaPrincipalCredito → proporción de pérdida por categoría]\n",
      "PerdidaCartera               no_perdida  perdida\n",
      "CategoriaPrincipalCredito                       \n",
      "alimentos-y-bebidas               0.455    0.545\n",
      "pines-virtuales                   0.675    0.325\n",
      "bebes12255                        0.689    0.311\n",
      "ropa-y-accesorios                 0.695    0.305\n",
      "camaras-y-accesorios              0.709    0.291\n",
      "otras-categorias                  0.723    0.277\n",
      "juegos-y-juguetes                 0.732    0.268\n",
      "relojes-y-joyas                   0.735    0.265\n",
      "electronica,-audio-y-video        0.739    0.261\n",
      "accesorios-para-vehiculos         0.746    0.254\n",
      "celulares-y-telefonos             0.747    0.253\n",
      "cuidado-personal                  0.748    0.252\n",
      "arte,-papeleria-y-merceria        0.749    0.251\n",
      "electrodomesticos                 0.750    0.250\n",
      "computacion                       0.758    0.242\n",
      "consolas-y-videojuegos            0.763    0.237\n",
      "herramientas-y-construccion       0.764    0.236\n",
      "OtrosRare                         0.769    0.231\n",
      "deportes-y-fitness                0.785    0.215\n",
      "belleza-y-cuidado-personal        0.787    0.213\n",
      "animales-y-mascotas               0.787    0.213\n",
      "hogar-y-muebles                   0.789    0.211\n",
      "salud-y-equipamiento-medico       0.805    0.195\n",
      "Desconocido                       0.840    0.160\n",
      "\n",
      "[Distribución ScoreBucket → proporción de pérdida por categoría]\n",
      "PerdidaCartera  no_perdida  perdida\n",
      "ScoreBucket                        \n",
      "bajo                 0.686    0.314\n",
      "medio                0.757    0.243\n",
      "sin_info             0.828    0.172\n",
      "alto                 0.847    0.153\n",
      "\n",
      "================= FIN EDA PREVIA (SIGUE MODELADO) =================\n",
      ">>> Sin columnas duplicadas en X.\n",
      ">>> Poda de columnas casi-constantes: ['Flag_UltimoUsoPosterior', 'Flag_MesesDesdeVinculacion_NaN', 'Flag_MesesDesdePrimerUso_NaN', 'Flag_DiasDesdeUltimoUso_NaN']\n",
      "\n",
      "=== Resumen columnas finales (previas al modelado) ===\n",
      "Numéricas: 54 | Categóricas: 7\n",
      "X shape: (146939, 61) | y rate (1): 0.226\n",
      "FechaEvento_dt rango: 2022-05-01 → 2023-10-31\n",
      "\n",
      ">>> Walk-forward CV (sin fuga):\n",
      "  - Split 1: train=(88163, 61), valid=(58776, 61), cutoff=2023-04-26\n",
      "[Logit+Cal (WF1)] ROC-AUC=0.558 | PR-AUC=0.284 | Brier=0.181\n",
      "[Logit+Cal (WF1)] Base rate y=1: 0.239 | mean(p1)=0.239 | mean(p0)=0.761\n",
      "[Logit+Cal (WF1)] best-F1=0.388 @ thr=0.191\n",
      "[Logit+Cal (WF1)] ConfMatrix @thr=0.191:\n",
      " [[ 2085 42651]\n",
      " [  403 13637]]\n",
      "[Logit+Cal (WF1)] Report @thr=0.191:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.838     0.047     0.088     44736\n",
      "           1      0.242     0.971     0.388     14040\n",
      "\n",
      "    accuracy                          0.267     58776\n",
      "   macro avg      0.540     0.509     0.238     58776\n",
      "weighted avg      0.696     0.267     0.160     58776\n",
      "\n",
      "[HGB+Cal (WF1)] ROC-AUC=0.849 | PR-AUC=0.648 | Brier=0.124\n",
      "[HGB+Cal (WF1)] Base rate y=1: 0.239 | mean(p1)=0.239 | mean(p0)=0.761\n",
      "[HGB+Cal (WF1)] best-F1=0.619 @ thr=0.291\n",
      "[HGB+Cal (WF1)] ConfMatrix @thr=0.291:\n",
      " [[36894  7842]\n",
      " [ 4230  9810]]\n",
      "[HGB+Cal (WF1)] Report @thr=0.291:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.897     0.825     0.859     44736\n",
      "           1      0.556     0.699     0.619     14040\n",
      "\n",
      "    accuracy                          0.795     58776\n",
      "   macro avg      0.726     0.762     0.739     58776\n",
      "weighted avg      0.816     0.795     0.802     58776\n",
      "\n",
      "  - Split 2: train=(102857, 61), valid=(44082, 61), cutoff=2023-06-11\n",
      "[Logit+Cal (WF2)] ROC-AUC=0.569 | PR-AUC=0.285 | Brier=0.176\n",
      "[Logit+Cal (WF2)] Base rate y=1: 0.230 | mean(p1)=0.230 | mean(p0)=0.770\n",
      "[Logit+Cal (WF2)] best-F1=0.376 @ thr=0.175\n",
      "[Logit+Cal (WF2)] ConfMatrix @thr=0.175:\n",
      " [[ 1564 32387]\n",
      " [  297  9834]]\n",
      "[Logit+Cal (WF2)] Report @thr=0.175:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.840     0.046     0.087     33951\n",
      "           1      0.233     0.971     0.376     10131\n",
      "\n",
      "    accuracy                          0.259     44082\n",
      "   macro avg      0.537     0.508     0.232     44082\n",
      "weighted avg      0.701     0.259     0.154     44082\n",
      "\n",
      "[HGB+Cal (WF2)] ROC-AUC=0.844 | PR-AUC=0.615 | Brier=0.124\n",
      "[HGB+Cal (WF2)] Base rate y=1: 0.230 | mean(p1)=0.230 | mean(p0)=0.770\n",
      "[HGB+Cal (WF2)] best-F1=0.604 @ thr=0.294\n",
      "[HGB+Cal (WF2)] ConfMatrix @thr=0.294:\n",
      " [[27730  6221]\n",
      " [ 3062  7069]]\n",
      "[HGB+Cal (WF2)] Report @thr=0.294:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.901     0.817     0.857     33951\n",
      "           1      0.532     0.698     0.604     10131\n",
      "\n",
      "    accuracy                          0.789     44082\n",
      "   macro avg      0.716     0.757     0.730     44082\n",
      "weighted avg      0.816     0.789     0.798     44082\n",
      "\n",
      "  - Split 3: train=(117551, 61), valid=(29388, 61), cutoff=2023-07-29\n",
      "[Logit+Cal (WF3)] ROC-AUC=0.589 | PR-AUC=0.275 | Brier=0.163\n",
      "[Logit+Cal (WF3)] Base rate y=1: 0.207 | mean(p1)=0.207 | mean(p0)=0.793\n",
      "[Logit+Cal (WF3)] best-F1=0.358 @ thr=0.227\n",
      "[Logit+Cal (WF3)] ConfMatrix @thr=0.227:\n",
      " [[13086 10205]\n",
      " [ 2545  3552]]\n",
      "[Logit+Cal (WF3)] Report @thr=0.227:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.837     0.562     0.672     23291\n",
      "           1      0.258     0.583     0.358      6097\n",
      "\n",
      "    accuracy                          0.566     29388\n",
      "   macro avg      0.548     0.572     0.515     29388\n",
      "weighted avg      0.717     0.566     0.607     29388\n",
      "\n",
      "[HGB+Cal (WF3)] ROC-AUC=0.839 | PR-AUC=0.557 | Brier=0.120\n",
      "[HGB+Cal (WF3)] Base rate y=1: 0.207 | mean(p1)=0.207 | mean(p0)=0.793\n",
      "[HGB+Cal (WF3)] best-F1=0.575 @ thr=0.265\n",
      "[HGB+Cal (WF3)] ConfMatrix @thr=0.265:\n",
      " [[18643  4648]\n",
      " [ 1786  4311]]\n",
      "[HGB+Cal (WF3)] Report @thr=0.265:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.913     0.800     0.853     23291\n",
      "           1      0.481     0.707     0.573      6097\n",
      "\n",
      "    accuracy                          0.781     29388\n",
      "   macro avg      0.697     0.754     0.713     29388\n",
      "weighted avg      0.823     0.781     0.795     29388\n",
      "\n",
      "\n",
      "Logit — Walk-forward CV:\n",
      "                         cutoff   roc_auc    pr_auc     brier       thr\n",
      "0 2023-04-26 00:30:52.808999936  0.558101  0.283581  0.180803  0.190750\n",
      "1 2023-06-11 16:14:11.783000064  0.569474  0.284715  0.175720  0.175010\n",
      "2 2023-07-29 19:52:43.941999872  0.588571  0.275172  0.162625  0.227073\n",
      "Logit — medias: ROC-AUC=0.572 | PR-AUC=0.281 | Brier=0.173\n",
      "\n",
      "HGB — Walk-forward CV:\n",
      "                         cutoff   roc_auc    pr_auc     brier       thr\n",
      "0 2023-04-26 00:30:52.808999936  0.848629  0.647578  0.123772  0.290625\n",
      "1 2023-06-11 16:14:11.783000064  0.843766  0.614984  0.124089  0.294118\n",
      "2 2023-07-29 19:52:43.941999872  0.838577  0.557450  0.120149  0.264672\n",
      "HGB — medias:   ROC-AUC=0.844 | PR-AUC=0.607 | Brier=0.123\n",
      "\n",
      ">>> Modelo elegido por PR-AUC medio: HGB\n",
      "\n",
      ">>> Holdout temporal: train=(117551, 61), holdout=(29388, 61), corte=2023-07-29\n",
      ">>> Checks anti-fuga:\n",
      "OK sin fuga\n",
      "\n",
      "[HOLDOUT-HGB] ROC-AUC=0.839 | PR-AUC=0.557 | Brier=0.120\n",
      "[HOLDOUT-HGB] Base rate y=1: 0.207 | mean(p1)=0.207 | mean(p0)=0.793\n",
      "[HOLDOUT-HGB] best-F1=0.575 @ thr=0.265\n",
      "[HOLDOUT-HGB] ConfMatrix @thr=0.265:\n",
      " [[18643  4648]\n",
      " [ 1786  4311]]\n",
      "[HOLDOUT-HGB] Report @thr=0.265:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.913     0.800     0.853     23291\n",
      "           1      0.481     0.707     0.573      6097\n",
      "\n",
      "    accuracy                          0.781     29388\n",
      "   macro avg      0.697     0.754     0.713     29388\n",
      "weighted avg      0.823     0.781     0.795     29388\n",
      "\n",
      "\n",
      "[HOLDOUT-HGB] Puntos de operación sugeridos:\n",
      " - maxF1     : thr=0.265 | precision=0.504 | recall=0.669 | F1=0.575\n",
      " - prec>=0.6 : thr=0.448 | precision=0.666 | recall=0.327 | F1=0.439\n",
      " - rec>=0.7  : thr=0.250 | precision=0.481 | recall=0.707 | F1=0.573\n",
      "\n",
      "[Barrido de umbrales clase 1]\n",
      " thr  prec1  rec1   TP   FP   FN    TN\n",
      " 0.2  0.426 0.804 4905 6604 1192 16687\n",
      " 0.3  0.513 0.654 3988 3780 2109 19511\n",
      " 0.4  0.546 0.590 3600 2994 2497 20297\n",
      " 0.5  0.693 0.300 1831  811 4266 22480\n",
      "\n",
      "[Objetivo] PRECISIÓN≥0.80 en morosos @thr=0.763\n",
      "  TP=266 FP=78 FN=5831 TN=23213\n",
      "  precision1=0.773 recall1=0.044 (morosos)\n",
      "  %alertados=0.012 %morosos_detectados=0.009\n",
      "\n",
      ">>> Modelo calibrado guardado en artifacts_modelo/modelo_calibrado.joblib\n",
      "\n",
      "[HOLDOUT-HGB] Calibración por deciles (p_mean vs y_rate):\n",
      "           index   p_mean   y_rate    n\n",
      "(-0.001, 0.0157] 0.010896 0.010896 4497\n",
      "(0.0157, 0.0255] 0.025153 0.025153 1471\n",
      "(0.0255, 0.0436] 0.039778 0.039778 3067\n",
      "(0.0436, 0.0693] 0.064372 0.064372 2905\n",
      " (0.0693, 0.132] 0.103796 0.103796 2977\n",
      "  (0.132, 0.197] 0.164754 0.164754 2962\n",
      "  (0.197, 0.265] 0.241006 0.241006 3419\n",
      "  (0.265, 0.448] 0.409572 0.409572 5098\n",
      "  (0.448, 0.452] 0.452174 0.452174  115\n",
      "  (0.452, 0.846] 0.674661 0.674661 2877\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MODELO DE RIESGO — VALIDACIÓN TEMPORAL ESTRICTA (SIN FUGA)\n",
    "# ============================================================\n",
    "# Requiere: pandas, numpy, scikit-learn, joblib\n",
    "# Supone que 'clientes' (crudo) está disponible en memoria.\n",
    "#   - columnas de fecha: FechaEvento, FechaVinculacionCliente, FechaUltimoUso, FechaPrimerUso\n",
    "#   - target: PerdidaCartera (0/1)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    classification_report, confusion_matrix, precision_recall_curve\n",
    ")\n",
    "\n",
    "import joblib\n",
    "\n",
    "# -----------------------------\n",
    "# 0) CONFIG\n",
    "# -----------------------------\n",
    "TARGET = \"PerdidaCartera\"\n",
    "RANDOM_STATE = 42\n",
    "ARTIF_DIR = Path(\"./artifacts_modelo\"); ARTIF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "# ============================================================\n",
    "# 1) INGESTA (usa tu carga real). Aseguramos target y forma.\n",
    "# ============================================================\n",
    "# EJEMPLO (descomenta si lo necesitas):\n",
    "# clientes = pd.read_parquet(\"data/clientes.parquet\")\n",
    "\n",
    "assert TARGET in clientes.columns, f\"No encuentro columna target '{TARGET}' en 'clientes'\"\n",
    "print(\">>> SHAPE crudo:\", clientes.shape)\n",
    "print(\">>> Columnas (primeras 20):\", list(clientes.columns)[:20])\n",
    "\n",
    "# ============================================================\n",
    "# 2) FEATURE ENGINEERING — SIN FUGA\n",
    "# ============================================================\n",
    "def parse_and_features(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # --- Fechas\n",
    "    df['FechaEvento_dt'] = (\n",
    "        pd.to_datetime(df['FechaEvento'], errors='coerce', utc=True)\n",
    "          .dt.tz_convert(None)\n",
    "    )\n",
    "    df['FechaVinculacionCliente_dt'] = pd.to_datetime(\n",
    "        df['FechaVinculacionCliente'], errors='coerce',\n",
    "        origin='1899-12-30', unit='D'\n",
    "    )\n",
    "    df['FechaUltimoUso_dt'] = pd.to_datetime(\n",
    "        df['FechaUltimoUso'], errors='coerce',\n",
    "        origin='1899-12-30', unit='D'\n",
    "    )\n",
    "    df['FechaPrimerUso_dt'] = pd.to_datetime(\n",
    "        df['FechaPrimerUso'], errors='coerce',\n",
    "        origin='1904-01-01', unit='D'\n",
    "    )\n",
    "\n",
    "    # Corrección conservadora PrimerUso\n",
    "    df['Flag_PrimerUsoAntesVinc'] = (df['FechaPrimerUso_dt'] < df['FechaVinculacionCliente_dt']).astype(int)\n",
    "    mask_bad = df['FechaPrimerUso_dt'].isna() | (df['FechaPrimerUso_dt'] < df['FechaVinculacionCliente_dt'])\n",
    "    df.loc[mask_bad,  'FechaPrimerUso_corr'] = df.loc[mask_bad,  'FechaVinculacionCliente_dt']\n",
    "    df.loc[~mask_bad, 'FechaPrimerUso_corr'] = df.loc[~mask_bad, 'FechaPrimerUso_dt']\n",
    "\n",
    "    # Diferencias seguras\n",
    "    def safe_months(a, b):\n",
    "        d = (a - b).dt.days\n",
    "        d = d.where(d.notna(), 0); d = np.where(d < 0, 0, d)\n",
    "        return d / 30.0\n",
    "\n",
    "    def safe_days(a, b):\n",
    "        d = (a - b).dt.days\n",
    "        d = d.where(d.notna(), 0); d = np.where(d < 0, 0, d)\n",
    "        return d\n",
    "\n",
    "    df['MesesDesdeVinculacion'] = safe_months(df['FechaEvento_dt'], df['FechaVinculacionCliente_dt'])\n",
    "    df['MesesDesdePrimerUso']   = safe_months(df['FechaEvento_dt'], df['FechaPrimerUso_corr'])\n",
    "    df['DiasDesdeUltimoUso']    = safe_days(df['FechaEvento_dt'],  df['FechaUltimoUso_dt'])\n",
    "\n",
    "    # --- Usabilidad\n",
    "    df['UsabilidadCupo'] = pd.to_numeric(df['UsabilidadCupo'], errors='coerce')\n",
    "    df['Flag_Usab_NaN']    = df['UsabilidadCupo'].isna().astype(int)\n",
    "    df['Flag_Usab_Outlier']= ((df['UsabilidadCupo'] < 0) | (df['UsabilidadCupo'] > 2)).astype(int)\n",
    "    df.loc[(df['UsabilidadCupo'] < 0) | (df['UsabilidadCupo'] > 2), 'UsabilidadCupo'] = np.nan\n",
    "    df['UsabilidadCupo']   = df['UsabilidadCupo'].fillna(df['UsabilidadCupo'].median())\n",
    "\n",
    "    # --- Flags de fechas\n",
    "    df['Flag_UltimoUsoPosterior'] = (df['FechaUltimoUso_dt'] > df['FechaEvento_dt']).fillna(False).astype(int)\n",
    "\n",
    "    # --- Numéricas con nulos —> flags + imputación conservadora\n",
    "    num_cols_candidates = [\n",
    "        'DiasMaximosMoraCreditosGenerados',\n",
    "        'NumeroCreditosGPrevius','NumeroCreditosGCanalFPrevius','NumeroCreditosGCanalVPrevius',\n",
    "        'NumeroCreditosGEstadoActivosPrevius','NumeroCreditosGEstadoPagadosPrevius',\n",
    "        'NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius','NumeroCreditosLEstadoPagadosPrevius',\n",
    "        'TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroIntentosFallidos','CupoAprobado','ScoreCrediticio','Edad',\n",
    "        'MesesDesdeVinculacion','MesesDesdePrimerUso','DiasDesdeUltimoUso'\n",
    "    ]\n",
    "    for c in num_cols_candidates:\n",
    "        if c in df.columns:\n",
    "            df[f'Flag_{c}_NaN'] = df[c].isna().astype(int)\n",
    "            if c.startswith('NumeroCreditos') or c.startswith('TotalPagos'):\n",
    "                df[c] = df[c].fillna(0)\n",
    "            elif c in ['DiasMaximosMoraCreditosGenerados','Edad','NumeroIntentosFallidos']:\n",
    "                df[c] = df[c].fillna(0)\n",
    "\n",
    "    # --- Score & Cupo\n",
    "    df['ScoreSinInfo'] = (df['ScoreCrediticio'] == 0).astype(int)\n",
    "    if df['ScoreCrediticio'].isna().any():\n",
    "        med_pos = df.loc[df['ScoreCrediticio']>0, 'ScoreCrediticio'].median()\n",
    "        df['ScoreCrediticio'] = df['ScoreCrediticio'].fillna(med_pos)\n",
    "\n",
    "    df['log_CupoAprobado'] = np.log1p(df['CupoAprobado'])\n",
    "    df['log_CupoAprobado'] = df['log_CupoAprobado'].fillna(df['log_CupoAprobado'].median())\n",
    "\n",
    "    # --- Categóricas limpias\n",
    "    for c in ['CategoriaPrincipalCredito','UsoAppWeb','Genero','TipoMunicipioEntregaTC','CanalMunicipioEntregaTC']:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna('Desconocido')\n",
    "    df['Genero'] = df['Genero'].replace({27:'Desconocido'})\n",
    "    df['TipoMunicipioEntregaTC'] = df['TipoMunicipioEntregaTC'].replace({'PEQUEÃ‘O':'PEQUEÑO'}).fillna('Desconocido')\n",
    "\n",
    "    # --- Rango de Edad\n",
    "    df['Flag_Edad_Out'] = (~df['Edad'].between(18, 100, inclusive='both')).fillna(False).astype(int)\n",
    "    df.loc[df['Flag_Edad_Out']==1, 'Edad'] = np.nan\n",
    "    df['Edad'] = df['Edad'].fillna(df['Edad'].median())\n",
    "\n",
    "    # --- Features derivadas clave\n",
    "    if 'Flag_PrimerUsoTemu' not in df.columns and 'NumeroCreditosGPrevius' in df.columns:\n",
    "        df['Flag_PrimerUsoTemu'] = (df['NumeroCreditosGPrevius'] == 0).astype(int)\n",
    "\n",
    "    df['ratio_pagos_local_global'] = (\n",
    "        df['TotalPagosEfectuadosLocalmentePrevius'].fillna(0) /\n",
    "        (df['TotalPagosEfectuadosGlobalmentePrevius'].fillna(0) + 1.0)\n",
    "    )\n",
    "    df['creditos_activos_ratio'] = (\n",
    "        df['NumeroCreditosGEstadoActivosPrevius'].fillna(0) /\n",
    "        (df['NumeroCreditosGPrevius'].fillna(0) + 1.0)\n",
    "    )\n",
    "    for c in ['ratio_pagos_local_global','creditos_activos_ratio']:\n",
    "        df[c] = df[c].replace([np.inf,-np.inf], np.nan).fillna(0).clip(0,1)\n",
    "\n",
    "    df['Flag_CanalVirtual'] = (\n",
    "        (df['CanalMunicipioEntregaTC'].astype(str).str.lower()=='virtual') |\n",
    "        (df['TipoMunicipioEntregaTC'].astype(str).str.upper()=='VIRTUAL')\n",
    "    ).astype(int)\n",
    "\n",
    "    # --- Score negativo -> 0, más bucket\n",
    "    df['Flag_Score_Negativo'] = (df['ScoreCrediticio'] < 0).astype(int)\n",
    "    df.loc[df['ScoreCrediticio'] < 0, 'ScoreCrediticio'] = 0\n",
    "\n",
    "    df['ScoreBucket'] = 'sin_info'\n",
    "    mask_pos = df['ScoreCrediticio'] > 0\n",
    "    if mask_pos.sum() > 0:\n",
    "        b1, b2 = df.loc[mask_pos, 'ScoreCrediticio'].quantile([0.33, 0.66]).values\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] <= b1), 'ScoreBucket'] = 'bajo'\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] >  b1) & (df['ScoreCrediticio'] <= b2), 'ScoreBucket'] = 'medio'\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] >  b2), 'ScoreBucket'] = 'alto'\n",
    "    df['ScoreBucket'] = pd.Categorical(df['ScoreBucket'], categories=['sin_info','bajo','medio','alto'], ordered=True)\n",
    "\n",
    "    # --- Winsorización p99 + flags (colas largas)\n",
    "    def cap_with_flag(s, upper):\n",
    "        flag = (s > upper).astype(int)\n",
    "        return np.where(s > upper, upper, s), flag\n",
    "\n",
    "    cap_cols = [\n",
    "        'DiasDesdeUltimoUso','MesesDesdeVinculacion',\n",
    "        'TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroCreditosGPrevius','NumeroCreditosGCanalFPrevius','NumeroCreditosGCanalVPrevius',\n",
    "        'NumeroCreditosGEstadoActivosPrevius','NumeroCreditosGEstadoPagadosPrevius',\n",
    "        'NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius','NumeroCreditosLEstadoPagadosPrevius'\n",
    "    ]\n",
    "    for c in cap_cols:\n",
    "        if c in df.columns:\n",
    "            p99 = df[c].quantile(0.99)\n",
    "            capped, flag = cap_with_flag(df[c].fillna(0), p99)\n",
    "            df[c] = capped\n",
    "            df[f'Flag_{c}_Capped'] = flag\n",
    "\n",
    "    # Agrupar categorías raras de CategoriaPrincipalCredito (<0.1%)\n",
    "    if 'CategoriaPrincipalCredito' in df.columns:\n",
    "        vc = df['CategoriaPrincipalCredito'].astype(str).value_counts(dropna=False)\n",
    "        cutoff = df.shape[0] * 0.001\n",
    "        rare_levels = vc[vc < cutoff].index\n",
    "        df['CategoriaPrincipalCredito'] = df['CategoriaPrincipalCredito'].astype(str)\n",
    "        df.loc[df['CategoriaPrincipalCredito'].isin(rare_levels), 'CategoriaPrincipalCredito'] = 'OtrosRare'\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_feature_lists(df: pd.DataFrame):\n",
    "    drop_from_features = [\n",
    "        'IdentificadorCliente','FechaEvento','FechaVinculacionCliente','FechaPrimerUso','FechaUltimoUso',\n",
    "        'FechaEvento_dt','FechaVinculacionCliente_dt','FechaPrimerUso_dt','FechaUltimoUso_dt','FechaPrimerUso_corr',\n",
    "        'CodigoAlmacenEntregaTC','CodigoAlmacenEntregaTC_str','AlmacenTop20',\n",
    "        'CodigoMunicipioEntregaTC','MunicipioCat','MunicipioTop20','MesCompra',\n",
    "        'DiasMora'\n",
    "    ]\n",
    "    num_base = [\n",
    "        'UsabilidadCupo','MesesDesdeVinculacion','MesesDesdePrimerUso','DiasDesdeUltimoUso',\n",
    "        'NumeroCreditosGPrevius','NumeroCreditosGCanalFPrevius','NumeroCreditosGCanalVPrevius',\n",
    "        'NumeroCreditosGEstadoActivosPrevius','NumeroCreditosGEstadoPagadosPrevius',\n",
    "        'NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius','NumeroCreditosLEstadoPagadosPrevius',\n",
    "        'TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroIntentosFallidos','ScoreCrediticio','ScoreSinInfo','CupoAprobado','log_CupoAprobado','Edad',\n",
    "        'Flag_Usab_NaN','Flag_Usab_Outlier','Flag_PrimerUsoAntesVinc','Flag_UltimoUsoPosterior','Flag_Edad_Out',\n",
    "        'ratio_pagos_local_global','creditos_activos_ratio','Flag_Score_Negativo'\n",
    "    ]\n",
    "    num_dyn = [c for c in df.columns if c.startswith('Flag_') and (c.endswith('_NaN') or c.endswith('_Capped'))]\n",
    "    num_final = [c for c in (num_base + num_dyn) if c in df.columns and c not in drop_from_features]\n",
    "\n",
    "    cat_final = [\n",
    "        'Genero','TipoMunicipioEntregaTC','CanalMunicipioEntregaTC','UsoAppWeb','CategoriaPrincipalCredito',\n",
    "        'Flag_PrimerUsoTemu','ScoreBucket'\n",
    "    ]\n",
    "    cat_final = [c for c in cat_final if c in df.columns and c not in drop_from_features]\n",
    "\n",
    "    # Evitar solape entre num y cat + forzar unicidad y orden estable\n",
    "    overlap = set(num_final) & set(cat_final)\n",
    "    if overlap:\n",
    "        print(f\">>> Aviso: {len(overlap)} columnas estaban en num y cat. Se quitan de num: {sorted(overlap)}\")\n",
    "        num_final = [c for c in num_final if c not in overlap]\n",
    "\n",
    "    num_final = list(dict.fromkeys(num_final))\n",
    "    cat_final = list(dict.fromkeys(cat_final))\n",
    "    return num_final, cat_final\n",
    "\n",
    "\n",
    "def prune_low_variance(X: pd.DataFrame):\n",
    "    low_var = [c for c in X.columns if X[c].nunique(dropna=False) <= 1]\n",
    "    if low_var:\n",
    "        print(\">>> Poda de columnas casi-constantes:\", low_var)\n",
    "        return X.drop(columns=low_var, errors='ignore'), low_var\n",
    "    else:\n",
    "        print(\">>> Poda de columnas casi-constantes: ninguna\")\n",
    "        return X, []\n",
    "\n",
    "\n",
    "def build_pipelines(cat_cols, num_cols):\n",
    "    pre_logit = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('imp', SimpleImputer(strategy='most_frequent')),\n",
    "                ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "            ]), cat_cols),\n",
    "            ('num', Pipeline(steps=[\n",
    "                ('imp', SimpleImputer(strategy='median'))\n",
    "            ]), num_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    logit = Pipeline(steps=[\n",
    "        ('pre', pre_logit),\n",
    "        ('clf', LogisticRegression(\n",
    "            solver='saga', penalty='l2', class_weight='balanced',\n",
    "            max_iter=800, n_jobs=-1, random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    pre_hgb = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), cat_cols),\n",
    "            ('num', 'passthrough', num_cols),\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    hgb = Pipeline(steps=[\n",
    "        ('pre', pre_hgb),\n",
    "        ('clf', HistGradientBoostingClassifier(\n",
    "            learning_rate=0.07, max_leaf_nodes=31, l2_regularization=1.0,\n",
    "            random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ])\n",
    "    return logit, hgb\n",
    "\n",
    "\n",
    "def evaluate_proba(y_true, y_proba, name=\"model\"):\n",
    "    # Métricas de probas + resumen de ambos lados\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    ap  = average_precision_score(y_true, y_proba)\n",
    "    br  = brier_score_loss(y_true, y_proba)\n",
    "    print(f\"[{name}] ROC-AUC={auc:.3f} | PR-AUC={ap:.3f} | Brier={br:.3f}\")\n",
    "    print(f\"[{name}] Base rate y=1: {y_true.mean():.3f} | mean(p1)={np.mean(y_proba):.3f} | mean(p0)={np.mean(1-y_proba):.3f}\")\n",
    "    p, r, thr = precision_recall_curve(y_true, y_proba)\n",
    "    f1 = 2*p*r/(p+r+1e-12)\n",
    "    j  = np.argmax(f1)\n",
    "    best_thr = thr[j-1] if j>0 and j-1 < len(thr) else 0.5\n",
    "    y_hat = (y_proba >= best_thr).astype(int)\n",
    "    print(f\"[{name}] best-F1={f1[j]:.3f} @ thr={best_thr:.3f}\")\n",
    "    print(f\"[{name}] ConfMatrix @thr={best_thr:.3f}:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    print(f\"[{name}] Report @thr={best_thr:.3f}:\\n\", classification_report(y_true, y_hat, digits=3))\n",
    "    return dict(roc_auc=auc, pr_auc=ap, brier=br, thr=best_thr)\n",
    "\n",
    "\n",
    "def operating_points(y_true, y_proba, name=\"model\"):\n",
    "    p, r, thr = precision_recall_curve(y_true, y_proba)\n",
    "    f1 = 2*p*r/(p+r+1e-12)\n",
    "    pts = {}\n",
    "    # máx F1\n",
    "    j = np.argmax(f1); pts['maxF1'] = (thr[j-1] if j>0 else 0.5, p[j], r[j], f1[j])\n",
    "    # precisión >= 0.6\n",
    "    idx = np.where(p>=0.6)[0]\n",
    "    if len(idx)>0:\n",
    "        k = idx[np.argmax(r[idx])]\n",
    "        pts['prec>=0.6'] = (thr[k-1] if k>0 else 0.5, p[k], r[k], f1[k])\n",
    "    # recall >= 0.7\n",
    "    idx = np.where(r>=0.7)[0]\n",
    "    if len(idx)>0:\n",
    "        k = idx[np.argmax(p[idx])]\n",
    "        pts['rec>=0.7'] = (thr[k-1] if k>0 else 0.5, p[k], r[k], f1[k])\n",
    "\n",
    "    print(f\"\\n[{name}] Puntos de operación sugeridos:\")\n",
    "    for kk,(t,pp,rr,ff) in pts.items():\n",
    "        print(f\" - {kk:10s}: thr={t:.3f} | precision={pp:.3f} | recall={rr:.3f} | F1={ff:.3f}\")\n",
    "    return pts\n",
    "\n",
    "\n",
    "def walk_forward_cv(df, X, y, cat_cols, num_cols, cut_fracs=(0.6,0.7,0.8)):\n",
    "    \"\"\"Valida SIN FUGA con 3 cortes temporales (ajustable).\"\"\"\n",
    "    cutoffs = df['FechaEvento_dt'].quantile(list(cut_fracs)).values\n",
    "    rows_l, rows_h = [], []\n",
    "    logit, hgb = build_pipelines(cat_cols, num_cols)\n",
    "\n",
    "    print(\"\\n>>> Walk-forward CV (sin fuga):\")\n",
    "    for i, c in enumerate(cutoffs, 1):\n",
    "        tr_idx = df['FechaEvento_dt'] <= c\n",
    "        va_idx = df['FechaEvento_dt'] >  c\n",
    "        X_tr, X_va = X.loc[tr_idx], X.loc[va_idx]\n",
    "        y_tr, y_va = y.loc[tr_idx], y.loc[va_idx]\n",
    "        print(f\"  - Split {i}: train={X_tr.shape}, valid={X_va.shape}, cutoff={pd.Timestamp(c).date()}\")\n",
    "\n",
    "        # Logit + calibración\n",
    "        logit.fit(X_tr, y_tr)\n",
    "        cal_logit = CalibratedClassifierCV(logit, method='sigmoid', cv='prefit')\n",
    "        cal_logit.fit(X_va, y_va)\n",
    "        proba_l = cal_logit.predict_proba(X_va)[:,1]\n",
    "        m_l = evaluate_proba(y_va, proba_l, name=f\"Logit+Cal (WF{i})\")\n",
    "        rows_l.append({\"cutoff\": c, **m_l})\n",
    "\n",
    "        # HGB + sample_weight + calibración\n",
    "        sw = compute_sample_weight(\"balanced\", y_tr)\n",
    "        hgb.fit(X_tr, y_tr, clf__sample_weight=sw)\n",
    "        cal_hgb = CalibratedClassifierCV(hgb, method='isotonic', cv='prefit')\n",
    "        cal_hgb.fit(X_va, y_va)\n",
    "        proba_h = cal_hgb.predict_proba(X_va)[:,1]\n",
    "        m_h = evaluate_proba(y_va, proba_h, name=f\"HGB+Cal (WF{i})\")\n",
    "        rows_h.append({\"cutoff\": c, **m_h})\n",
    "\n",
    "    return pd.DataFrame(rows_l), pd.DataFrame(rows_h)\n",
    "\n",
    "# ============================================================\n",
    "# 3) CONSTRUIR DATASET LIMPIO + LISTAS DE FEATURES + EDA\n",
    "# ============================================================\n",
    "print(\"\\n>>> Construyendo features (sin fuga) ...\")\n",
    "clientes = parse_and_features(clientes)\n",
    "num_final, cat_final = build_feature_lists(clientes)\n",
    "\n",
    "X = clientes[num_final + cat_final].copy()\n",
    "y = clientes[TARGET].astype(int).copy()\n",
    "\n",
    "# 3A) EDA previa (resumen, correlaciones, crosstabs)\n",
    "print(\"\\n===================== EDA PREVIA (SIN FUGA) =====================\")\n",
    "print(f\"Shape de X: {X.shape} | y rate (1)= {y.mean():.3f}\")\n",
    "print(\"Rango de fechas (FechaEvento_dt):\",\n",
    "      str(clientes['FechaEvento_dt'].min().date()), \"→\", str(clientes['FechaEvento_dt'].max().date()))\n",
    "\n",
    "vc = y.value_counts().rename({0:'no_perdida', 1:'perdida'})\n",
    "print(\"\\n[Balance de clases]\")\n",
    "print(pd.concat([vc, (vc/vc.sum()).round(3).rename('pct')], axis=1).to_string())\n",
    "\n",
    "summary_rows = []\n",
    "for c in X.columns:\n",
    "    s = X[c]\n",
    "    dtype = s.dtype\n",
    "    pct_null = s.isnull().mean()*100\n",
    "    nuni = s.nunique(dropna=False)\n",
    "    # FIX: usar is_numeric_dtype para que no falle con CategoricalDtype\n",
    "    top5 = s.value_counts(dropna=False).head(5).to_dict() if (not is_numeric_dtype(s)) else {}\n",
    "    summary_rows.append({\n",
    "        'columna': c, 'dtype': str(dtype), '%nulos': round(pct_null,1),\n",
    "        'n_unicos': int(nuni), 'top5_valores': top5\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(['dtype','columna'])\n",
    "print(\"\\n[Resumen por columna] (primeras 30 filas)\")\n",
    "print(summary_df.head(30).to_string(index=False))\n",
    "summary_df.to_csv(ARTIF_DIR/\"eda_resumen_columnas.csv\", index=False)\n",
    "\n",
    "num_cols_for_corr = [c for c in X.columns if is_numeric_dtype(X[c]) and c != TARGET]\n",
    "if len(num_cols_for_corr) > 0:\n",
    "    corrs = X[num_cols_for_corr].corrwith(y).sort_values(ascending=False)\n",
    "    print(\"\\n[Correlación numéricas vs target] (top 20)\")\n",
    "    print(corrs.head(20).round(3).to_string())\n",
    "    print(\"\\n[Correlación numéricas vs target] (bottom 20)\")\n",
    "    print(corrs.tail(20).round(3).to_string())\n",
    "\n",
    "cat_cols_for_xtab = [c for c in X.columns if not is_numeric_dtype(X[c])]\n",
    "for c in cat_cols_for_xtab:\n",
    "    # sólo crosstab para cardinalidad moderada\n",
    "    if X[c].nunique(dropna=False) <= 25:\n",
    "        tab = pd.crosstab(X[c], y, normalize='index').rename(columns={0:'no_perdida',1:'perdida'}).round(3)\n",
    "        print(f\"\\n[Distribución {c} → proporción de pérdida por categoría]\")\n",
    "        print(tab.sort_values('perdida', ascending=False).to_string())\n",
    "\n",
    "print(\"\\n================= FIN EDA PREVIA (SIGUE MODELADO) =================\")\n",
    "\n",
    "# 3B) Limpiezas extra útiles antes de modelar\n",
    "# --- ELIMINAR COLUMNAS DUPLICADAS EN X ---\n",
    "dups_mask = X.columns.duplicated(keep='first')\n",
    "if dups_mask.any():\n",
    "    dups = pd.Series(X.columns)[dups_mask].tolist()\n",
    "    print(f\">>> Columnas duplicadas detectadas y removidas ({len(dups)}): {dups}\")\n",
    "    X = X.loc[:, ~X.columns.duplicated(keep='first')]\n",
    "else:\n",
    "    print(\">>> Sin columnas duplicadas en X.\")\n",
    "\n",
    "# --- PODA BAJA VARIANZA ---\n",
    "X, dropped_lowvar = prune_low_variance(X)\n",
    "\n",
    "print(\"\\n=== Resumen columnas finales (previas al modelado) ===\")\n",
    "print(f\"Numéricas: {len([c for c in X.columns if c in num_final])} | Categóricas: {len([c for c in X.columns if c in cat_final])}\")\n",
    "print(\"X shape:\", X.shape, \"| y rate (1):\", y.mean().round(3))\n",
    "print(\"FechaEvento_dt rango:\", str(clientes['FechaEvento_dt'].min().date()), \"→\", str(clientes['FechaEvento_dt'].max().date()))\n",
    "\n",
    "# ============================================================\n",
    "# 4) WALK-FORWARD CV — SIN MIRAR EL FUTURO\n",
    "# ============================================================\n",
    "cat_cols = [c for c in cat_final if c in X.columns]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "logit_cv, hgb_cv = walk_forward_cv(clientes, X, y, cat_cols, num_cols)\n",
    "\n",
    "print(\"\\nLogit — Walk-forward CV:\")\n",
    "print(logit_cv)\n",
    "print(f\"Logit — medias: ROC-AUC={logit_cv.roc_auc.mean():.3f} | PR-AUC={logit_cv.pr_auc.mean():.3f} | Brier={logit_cv.brier.mean():.3f}\")\n",
    "\n",
    "print(\"\\nHGB — Walk-forward CV:\")\n",
    "print(hgb_cv)\n",
    "print(f\"HGB — medias:   ROC-AUC={hgb_cv.roc_auc.mean():.3f} | PR-AUC={hgb_cv.pr_auc.mean():.3f} | Brier={hgb_cv.brier.mean():.3f}\")\n",
    "\n",
    "chosen = \"HGB\" if hgb_cv.pr_auc.mean() >= logit_cv.pr_auc.mean() else \"Logit\"\n",
    "print(f\"\\n>>> Modelo elegido por PR-AUC medio: {chosen}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) HOLDOUT FINAL (FUTURO) — EVALUAR Y BUSCAR PRECISIÓN >=0.80\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "\n",
    "# Split temporal\n",
    "cutoff = clientes['FechaEvento_dt'].quantile(0.8)\n",
    "train_idx = clientes['FechaEvento_dt'] <= cutoff\n",
    "hold_idx  = clientes['FechaEvento_dt'] >  cutoff\n",
    "X_tr, X_ho = X.loc[train_idx], X.loc[hold_idx]\n",
    "y_tr, y_ho = y.loc[train_idx], y.loc[hold_idx]\n",
    "\n",
    "print(f\"\\n>>> Holdout temporal: train={X_tr.shape}, holdout={X_ho.shape}, corte={pd.Timestamp(cutoff).date()}\")\n",
    "print(\">>> Checks anti-fuga:\")\n",
    "assert set(X_tr.index).isdisjoint(set(X_ho.index)), \"¡Solapamiento entre train y holdout!\"\n",
    "assert clientes.loc[X_tr.index,'FechaEvento_dt'].max() <= cutoff\n",
    "assert clientes.loc[X_ho.index,'FechaEvento_dt'].min() > cutoff\n",
    "print(\"OK sin fuga\\n\")\n",
    "\n",
    "# Entrena modelo elegido\n",
    "logit, hgb = build_pipelines(cat_cols, num_cols)\n",
    "sw = compute_sample_weight(\"balanced\", y_tr)\n",
    "hgb.fit(X_tr, y_tr, clf__sample_weight=sw)\n",
    "cal_hgb = CalibratedClassifierCV(hgb, method='isotonic', cv='prefit')\n",
    "cal_hgb.fit(X_ho, y_ho)\n",
    "proba_hgb = cal_hgb.predict_proba(X_ho)[:,1]\n",
    "\n",
    "# Métricas generales en holdout\n",
    "_ = evaluate_proba(y_ho, proba_hgb, name=\"HOLDOUT-HGB\")\n",
    "operating_points(y_ho, proba_hgb, name=\"HOLDOUT-HGB\")\n",
    "\n",
    "# -------- función para barrer umbrales\n",
    "def sweep_thresholds(y_true, y_proba, thresholds=(0.2,0.3,0.4,0.5)):\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        yhat = (y_proba >= t).astype(int)\n",
    "        TN, FP, FN, TP = confusion_matrix(y_true, yhat).ravel()\n",
    "        prec1 = TP/(TP+FP) if TP+FP>0 else 0\n",
    "        rec1  = TP/(TP+FN) if TP+FN>0 else 0\n",
    "        rows.append({\"thr\":t,\"prec1\":round(prec1,3),\"rec1\":round(rec1,3),\"TP\":TP,\"FP\":FP,\"FN\":FN,\"TN\":TN})\n",
    "    print(\"\\n[Barrido de umbrales clase 1]\")\n",
    "    print(pd.DataFrame(rows).to_string(index=False))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "tab_thr = sweep_thresholds(y_ho, proba_hgb)\n",
    "\n",
    "# -------- encontrar umbral que cumpla PRECISIÓN >= 0.80 para clase 1\n",
    "p, r, thr = precision_recall_curve(y_ho, proba_hgb)\n",
    "idx = np.where(p >= 0.80)[0]\n",
    "if len(idx) == 0:\n",
    "    print(\"\\n>>> No existe punto con precisión ≥0.80. Necesitas nuevas variables o aceptar menor recall.\")\n",
    "else:\n",
    "    k = idx[np.argmax(r[idx])]\n",
    "    thr_p80 = thr[k-1] if k>0 else 0.5\n",
    "    yhat = (proba_hgb >= thr_p80).astype(int)\n",
    "    TN, FP, FN, TP = confusion_matrix(y_ho, yhat).ravel()\n",
    "    prec1 = TP/(TP+FP) if TP+FP>0 else 0\n",
    "    rec1  = TP/(TP+FN) if TP+FN>0 else 0\n",
    "    print(f\"\\n[Objetivo] PRECISIÓN≥0.80 en morosos @thr={thr_p80:.3f}\")\n",
    "    print(f\"  TP={TP} FP={FP} FN={FN} TN={TN}\")\n",
    "    print(f\"  precision1={prec1:.3f} recall1={rec1:.3f} (morosos)\")\n",
    "    print(f\"  %alertados={(TP+FP)/len(y_ho):.3f} %morosos_detectados={TP/len(y_ho):.3f}\")\n",
    "\n",
    "# -------- guardar modelo calibrado\n",
    "bundle = {\n",
    "    \"model_calibrado\": cal_hgb,\n",
    "    \"num_final\": num_final,\n",
    "    \"cat_final\": cat_final,\n",
    "    \"columns_after_prune\": list(X.columns),\n",
    "    \"cutoff\": cutoff\n",
    "}\n",
    "joblib.dump(bundle, ARTIF_DIR/\"modelo_calibrado.joblib\")\n",
    "print(f\"\\n>>> Modelo calibrado guardado en {ARTIF_DIR/'modelo_calibrado.joblib'}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) CALIBRACIÓN POR DECILES EN HOLDOUT\n",
    "# ============================================================\n",
    "def decile_calibration(y_true, y_proba, name=\"holdout\"):\n",
    "    bins = pd.qcut(y_proba, q=10, duplicates='drop')\n",
    "    tab = pd.DataFrame({\"p\":y_proba, \"y\":y_true}).groupby(bins, observed=True).agg(\n",
    "        p_mean=('p','mean'),\n",
    "        y_rate=('y','mean'),\n",
    "        n=('p','size')\n",
    "    ).reset_index().rename(columns={\"p\":\"bin\"})\n",
    "    print(f\"\\n[{name}] Calibración por deciles (p_mean vs y_rate):\")\n",
    "    print(tab.to_string(index=False))\n",
    "    tab.to_csv(ARTIF_DIR/f\"calibracion_deciles_{name}.csv\", index=False)\n",
    "    return tab\n",
    "\n",
    "_ = decile_calibration(y_ho, proba_hgb, name=\"HOLDOUT-HGB\")\n",
    "\n",
    "# ============================================================\n",
    "# 7) FUNCIÓN DE SCORING FUTURO (para data no vista)\n",
    "# ============================================================\n",
    "def score_future(df_nuevo_raw: pd.DataFrame, artif_path=ARTIF_DIR/\"modelo_calibrado.joblib\", target_col=TARGET,\n",
    "                 export_csv=True):\n",
    "    \"\"\"Aplica el mismo pipeline a df_nuevo. Si trae label, evalúa; si no, solo predice.\"\"\"\n",
    "    bundle = joblib.load(artif_path)\n",
    "    model  = bundle[\"model_calibrado\"]\n",
    "    cols_ok = bundle[\"columns_after_prune\"]\n",
    "    num_final, cat_final = bundle[\"num_final\"], bundle[\"cat_final\"]\n",
    "\n",
    "    df_nuevo = parse_and_features(df_nuevo_raw)\n",
    "    Xn = df_nuevo[num_final + cat_final].copy()\n",
    "    # Alinear columnas esperadas + deduplicar por si acaso\n",
    "    Xn = Xn.reindex(columns=cols_ok, fill_value=np.nan)\n",
    "    Xn = Xn.loc[:, ~Xn.columns.duplicated(keep='first')]\n",
    "    proba1 = model.predict_proba(Xn)[:,1]\n",
    "    proba0 = 1 - proba1\n",
    "\n",
    "    # Resumen probabilidades (ambas clases)\n",
    "    print(\"\\n[SCORING] Resumen probabilidades:\")\n",
    "    print(f\"mean(p0)= {proba0.mean():.3f} | mean(p1)= {proba1.mean():.3f} | min/max p1= {proba1.min():.3f}/{proba1.max():.3f}\")\n",
    "\n",
    "    if target_col in df_nuevo.columns:\n",
    "        y_true = df_nuevo[target_col].astype(int)\n",
    "        _ = evaluate_proba(y_true, proba1, name=\"DF_NUEVO\")\n",
    "        operating_points(y_true, proba1, name=\"DF_NUEVO\")\n",
    "        _ = decile_calibration(y_true, proba1, name=\"DF_NUEVO\")\n",
    "    else:\n",
    "        print(\"df_nuevo sin target: se devuelven solo probabilidades.\")\n",
    "\n",
    "    out = pd.DataFrame({\"p_no_perdida\": proba0, \"p_perdida\": proba1})\n",
    "    if export_csv:\n",
    "        out_path = ARTIF_DIR/\"scoring_df_nuevo.csv\"\n",
    "        out.to_csv(out_path, index=False)\n",
    "        print(f\"Scoring exportado a {out_path}\")\n",
    "    return out\n",
    "\n",
    "# ------------- QUICK TEST -------------\n",
    "# Simular \"futuro real\" con el 20% más reciente:\n",
    "# df_nuevo = clientes.loc[clientes['FechaEvento_dt'] > cutoff].copy()\n",
    "# _ = score_future(df_nuevo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26df9f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SCORING] Resumen probabilidades:\n",
      "mean(p1)= 0.206 | min/max p1= 0.000/0.848\n",
      "[DF_NUEVO] ROC-AUC=0.838 | PR-AUC=0.555 | Brier=0.120\n",
      "[DF_NUEVO] Base rate y=1: 0.207 | mean(p1)=0.206\n",
      "[DF_NUEVO] best-F1=0.573 @ thr=0.292\n",
      "[DF_NUEVO] ConfMatrix @thr=0.292:\n",
      " [[18995  4296]\n",
      " [ 1934  4163]]\n",
      "[DF_NUEVO] Report @thr=0.292:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.908     0.816     0.859     23291\n",
      "           1      0.492     0.683     0.572      6097\n",
      "\n",
      "    accuracy                          0.788     29388\n",
      "   macro avg      0.700     0.749     0.716     29388\n",
      "weighted avg      0.821     0.788     0.800     29388\n",
      "\n",
      "Scoring exportado a artifacts_modelo/scoring_df_nuevo.csv\n"
     ]
    }
   ],
   "source": [
    "# ------------- QUICK TEST -------------\n",
    "# Simular \"futuro real\" con el 20% más reciente:\n",
    "df_nuevo = clientes.loc[clientes['FechaEvento_dt'] > cutoff].copy()\n",
    "_ = score_future(df_nuevo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9296f003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "---3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce0340c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> SHAPE crudo: (146939, 30)\n",
      ">>> Columnas (primeras 20): ['IdentificadorCliente', 'FechaEvento', 'UsabilidadCupo', 'CategoriaPrincipalCredito', 'DiasMaximosMoraCreditosGenerados', 'NumeroCreditosGPrevius', 'NumeroCreditosGCanalFPrevius', 'NumeroCreditosGEstadoActivosPrevius', 'NumeroCreditosGEstadoPagadosPrevius', 'NumeroCreditosGCanalVPrevius', 'NumeroCreditosLPrevius', 'NumeroCreditosLEstadoActivosPrevius', 'NumeroCreditosLEstadoPagadosPrevius', 'FechaVinculacionCliente', 'FechaPrimerUso', 'FechaUltimoUso', 'TotalPagosEfectuadosGlobalmentePrevius', 'TotalPagosEfectuadosLocalmentePrevius', 'CodigoAlmacenEntregaTC', 'CodigoMunicipioEntregaTC']\n",
      "\n",
      ">>> Construyendo features (sin fuga) ...\n",
      "\n",
      "===================== EDA PREVIA (SIN FUGA) =====================\n",
      "Shape de X: (146939, 65) | y rate (1)= 0.226\n",
      "Rango de fechas (FechaEvento_dt): 2022-05-01 → 2023-10-31\n",
      "\n",
      "[Balance de clases]\n",
      "                 count    pct\n",
      "PerdidaCartera               \n",
      "no_perdida      113803  0.774\n",
      "perdida          33136  0.226\n",
      "\n",
      "[Resumen por columna] (primeras 30 filas)\n",
      "                                  columna    dtype  %nulos  n_unicos                                                      top5_valores\n",
      "                              ScoreBucket category     0.0         4 {'alto': 41836, 'medio': 40654, 'bajo': 40652, 'sin_info': 23797}\n",
      "                             CupoAprobado  float64     0.4       115                                                                {}\n",
      "                       DiasDesdeUltimoUso  float64     0.0      1096                                                                {}\n",
      "                                     Edad  float64     0.0        76                                                                {}\n",
      "                      MesesDesdePrimerUso  float64     0.0      2901                                                                {}\n",
      "                    MesesDesdeVinculacion  float64     0.0      2366                                                                {}\n",
      "             NumeroCreditosGCanalFPrevius  float64     0.0        76                                                                {}\n",
      "             NumeroCreditosGCanalVPrevius  float64     0.0         6                                                                {}\n",
      "      NumeroCreditosGEstadoActivosPrevius  float64     0.0         6                                                                {}\n",
      "      NumeroCreditosGEstadoPagadosPrevius  float64     0.0        76                                                                {}\n",
      "                   NumeroCreditosGPrevius  float64     0.0        77                                                                {}\n",
      "      NumeroCreditosLEstadoActivosPrevius  float64     0.0         2                                                                {}\n",
      "      NumeroCreditosLEstadoPagadosPrevius  float64     0.0         4                                                                {}\n",
      "                   NumeroCreditosLPrevius  float64     0.0         5                                                                {}\n",
      "                   NumeroIntentosFallidos  float64     0.0        43                                                                {}\n",
      "                          ScoreCrediticio  float64     0.0       883                                                                {}\n",
      "   TotalPagosEfectuadosGlobalmentePrevius  float64     0.0       220                                                                {}\n",
      "    TotalPagosEfectuadosLocalmentePrevius  float64     0.0        12                                                                {}\n",
      "                           UsabilidadCupo  float64     0.0    101998                                                                {}\n",
      "                   creditos_activos_ratio  float64     0.0       554                                                                {}\n",
      "                         log_CupoAprobado  float64     0.0       114                                                                {}\n",
      "                 ratio_pagos_local_global  float64     0.0      1628                                                                {}\n",
      "                    Flag_CupoAprobado_NaN    int64     0.0         2                                                                {}\n",
      "           Flag_DiasDesdeUltimoUso_Capped    int64     0.0         2                                                                {}\n",
      "              Flag_DiasDesdeUltimoUso_NaN    int64     0.0         1                                                                {}\n",
      "Flag_DiasMaximosMoraCreditosGenerados_NaN    int64     0.0         2                                                                {}\n",
      "                            Flag_Edad_NaN    int64     0.0         2                                                                {}\n",
      "                            Flag_Edad_Out    int64     0.0         2                                                                {}\n",
      "             Flag_MesesDesdePrimerUso_NaN    int64     0.0         1                                                                {}\n",
      "        Flag_MesesDesdeVinculacion_Capped    int64     0.0         2                                                                {}\n",
      "\n",
      "[Correlación numéricas vs target] (top 20)\n",
      "NumeroCreditosGEstadoActivosPrevius                0.400\n",
      "creditos_activos_ratio                             0.396\n",
      "NumeroCreditosLEstadoActivosPrevius                0.216\n",
      "NumeroCreditosLPrevius                             0.201\n",
      "ratio_pagos_local_global                           0.182\n",
      "NumeroCreditosLEstadoPagadosPrevius                0.164\n",
      "Flag_NumeroCreditosGEstadoActivosPrevius_Capped    0.151\n",
      "Flag_PrimerUsoTemu                                 0.145\n",
      "Flag_TotalPagosEfectuadosLocalmentePrevius_NaN     0.137\n",
      "Flag_TotalPagosEfectuadosGlobalmentePrevius_NaN    0.137\n",
      "TotalPagosEfectuadosLocalmentePrevius              0.125\n",
      "Flag_DiasMaximosMoraCreditosGenerados_NaN          0.116\n",
      "UsabilidadCupo                                     0.114\n",
      "Flag_NumeroCreditosLEstadoActivosPrevius_Capped    0.101\n",
      "Flag_NumeroCreditosGPrevius_NaN                    0.101\n",
      "Flag_NumeroCreditosLEstadoPagadosPrevius_NaN       0.101\n",
      "Flag_NumeroCreditosLEstadoActivosPrevius_NaN       0.101\n",
      "Flag_NumeroCreditosGEstadoPagadosPrevius_NaN       0.101\n",
      "Flag_NumeroCreditosGEstadoActivosPrevius_NaN       0.101\n",
      "Flag_NumeroCreditosGCanalVPrevius_NaN              0.101\n",
      "\n",
      "[Correlación numéricas vs target] (bottom 20)\n",
      "Flag_NumeroCreditosGPrevius_Capped                   -0.026\n",
      "Flag_NumeroCreditosGCanalFPrevius_Capped             -0.027\n",
      "Flag_TotalPagosEfectuadosGlobalmentePrevius_Capped   -0.028\n",
      "Flag_NumeroCreditosGEstadoPagadosPrevius_Capped      -0.029\n",
      "Flag_PrimerUsoAntesVinc                              -0.031\n",
      "MesesDesdePrimerUso                                  -0.046\n",
      "DiasDesdeUltimoUso                                   -0.048\n",
      "ScoreSinInfo                                         -0.056\n",
      "Edad                                                 -0.069\n",
      "CupoAprobado                                         -0.084\n",
      "NumeroCreditosGPrevius                               -0.087\n",
      "NumeroCreditosGCanalFPrevius                         -0.088\n",
      "TotalPagosEfectuadosGlobalmentePrevius               -0.098\n",
      "MesesDesdeVinculacion                                -0.101\n",
      "log_CupoAprobado                                     -0.102\n",
      "NumeroCreditosGEstadoPagadosPrevius                  -0.112\n",
      "Flag_UltimoUsoPosterior                                 NaN\n",
      "Flag_MesesDesdeVinculacion_NaN                          NaN\n",
      "Flag_MesesDesdePrimerUso_NaN                            NaN\n",
      "Flag_DiasDesdeUltimoUso_NaN                             NaN\n",
      "\n",
      "[Distribución Genero → proporción de pérdida por categoría]\n",
      "PerdidaCartera  no_perdida  perdida\n",
      "Genero                             \n",
      "Masculino            0.753    0.247\n",
      "Desconocido          0.756    0.244\n",
      "Femenino             0.801    0.199\n",
      "\n",
      "[Distribución TipoMunicipioEntregaTC → proporción de pérdida por categoría]\n",
      "PerdidaCartera          no_perdida  perdida\n",
      "TipoMunicipioEntregaTC                     \n",
      "VIRTUAL                      0.722    0.278\n",
      "INTERMEDIO                   0.768    0.232\n",
      "PRINCIPAL                    0.796    0.204\n",
      "GRANDE                       0.816    0.184\n",
      "RURAL                        0.825    0.175\n",
      "PEQUEÑO                      0.837    0.163\n",
      "POBLADO                      0.839    0.161\n",
      "Desconocido                  0.940    0.060\n",
      "\n",
      "[Distribución CanalMunicipioEntregaTC → proporción de pérdida por categoría]\n",
      "PerdidaCartera           no_perdida  perdida\n",
      "CanalMunicipioEntregaTC                     \n",
      "Desconocido                   0.312    0.688\n",
      "Virtual                       0.722    0.278\n",
      "Fisico                        0.805    0.195\n",
      "\n",
      "[Distribución UsoAppWeb → proporción de pérdida por categoría]\n",
      "PerdidaCartera  no_perdida  perdida\n",
      "UsoAppWeb                          \n",
      "App                  0.763    0.237\n",
      "Web                  0.775    0.225\n",
      "Desconocido          0.807    0.193\n",
      "\n",
      "[Distribución CategoriaPrincipalCredito → proporción de pérdida por categoría]\n",
      "PerdidaCartera               no_perdida  perdida\n",
      "CategoriaPrincipalCredito                       \n",
      "alimentos-y-bebidas               0.455    0.545\n",
      "pines-virtuales                   0.675    0.325\n",
      "bebes12255                        0.689    0.311\n",
      "ropa-y-accesorios                 0.695    0.305\n",
      "camaras-y-accesorios              0.709    0.291\n",
      "otras-categorias                  0.723    0.277\n",
      "juegos-y-juguetes                 0.732    0.268\n",
      "relojes-y-joyas                   0.735    0.265\n",
      "electronica,-audio-y-video        0.739    0.261\n",
      "accesorios-para-vehiculos         0.746    0.254\n",
      "celulares-y-telefonos             0.747    0.253\n",
      "cuidado-personal                  0.748    0.252\n",
      "arte,-papeleria-y-merceria        0.749    0.251\n",
      "electrodomesticos                 0.750    0.250\n",
      "computacion                       0.758    0.242\n",
      "consolas-y-videojuegos            0.763    0.237\n",
      "herramientas-y-construccion       0.764    0.236\n",
      "OtrosRare                         0.769    0.231\n",
      "deportes-y-fitness                0.785    0.215\n",
      "belleza-y-cuidado-personal        0.787    0.213\n",
      "animales-y-mascotas               0.787    0.213\n",
      "hogar-y-muebles                   0.789    0.211\n",
      "salud-y-equipamiento-medico       0.805    0.195\n",
      "Desconocido                       0.840    0.160\n",
      "\n",
      "[Distribución ScoreBucket → proporción de pérdida por categoría]\n",
      "PerdidaCartera  no_perdida  perdida\n",
      "ScoreBucket                        \n",
      "bajo                 0.686    0.314\n",
      "medio                0.757    0.243\n",
      "sin_info             0.828    0.172\n",
      "alto                 0.847    0.153\n",
      "\n",
      "================= FIN EDA PREVIA (SIGUE MODELADO) =================\n",
      ">>> Sin columnas duplicadas en X.\n",
      ">>> Poda de columnas casi-constantes: ['Flag_UltimoUsoPosterior', 'Flag_MesesDesdeVinculacion_NaN', 'Flag_MesesDesdePrimerUso_NaN', 'Flag_DiasDesdeUltimoUso_NaN']\n",
      "\n",
      "=== Resumen columnas finales (previas al modelado) ===\n",
      "Numéricas: 54 | Categóricas: 7\n",
      "X shape: (146939, 61) | y rate (1): 0.226\n",
      "FechaEvento_dt rango: 2022-05-01 → 2023-10-31\n",
      "\n",
      ">>> Walk-forward CV (sin fuga):\n",
      "  - Split 1: train=(88163, 61), valid=(58776, 61), cutoff=2023-04-26\n",
      "[Logit+Cal (WF1)] ROC-AUC=0.558 | PR-AUC=0.284 | Brier=0.181\n",
      "[Logit+Cal (WF1)] Base rate y=1: 0.239 | mean(p1)=0.239 | mean(p0)=0.761\n",
      "[Logit+Cal (WF1)] best-F1=0.388 @ thr=0.191\n",
      "[Logit+Cal (WF1)] ConfMatrix @thr=0.191:\n",
      " [[ 2085 42651]\n",
      " [  403 13637]]\n",
      "[Logit+Cal (WF1)] Report @thr=0.191:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.838     0.047     0.088     44736\n",
      "           1      0.242     0.971     0.388     14040\n",
      "\n",
      "    accuracy                          0.267     58776\n",
      "   macro avg      0.540     0.509     0.238     58776\n",
      "weighted avg      0.696     0.267     0.160     58776\n",
      "\n",
      "[HGB+Cal (WF1)] ROC-AUC=0.849 | PR-AUC=0.648 | Brier=0.124\n",
      "[HGB+Cal (WF1)] Base rate y=1: 0.239 | mean(p1)=0.239 | mean(p0)=0.761\n",
      "[HGB+Cal (WF1)] best-F1=0.619 @ thr=0.291\n",
      "[HGB+Cal (WF1)] ConfMatrix @thr=0.291:\n",
      " [[36894  7842]\n",
      " [ 4230  9810]]\n",
      "[HGB+Cal (WF1)] Report @thr=0.291:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.897     0.825     0.859     44736\n",
      "           1      0.556     0.699     0.619     14040\n",
      "\n",
      "    accuracy                          0.795     58776\n",
      "   macro avg      0.726     0.762     0.739     58776\n",
      "weighted avg      0.816     0.795     0.802     58776\n",
      "\n",
      "  - Split 2: train=(102857, 61), valid=(44082, 61), cutoff=2023-06-11\n",
      "[Logit+Cal (WF2)] ROC-AUC=0.569 | PR-AUC=0.285 | Brier=0.176\n",
      "[Logit+Cal (WF2)] Base rate y=1: 0.230 | mean(p1)=0.230 | mean(p0)=0.770\n",
      "[Logit+Cal (WF2)] best-F1=0.376 @ thr=0.175\n",
      "[Logit+Cal (WF2)] ConfMatrix @thr=0.175:\n",
      " [[ 1564 32387]\n",
      " [  297  9834]]\n",
      "[Logit+Cal (WF2)] Report @thr=0.175:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.840     0.046     0.087     33951\n",
      "           1      0.233     0.971     0.376     10131\n",
      "\n",
      "    accuracy                          0.259     44082\n",
      "   macro avg      0.537     0.508     0.232     44082\n",
      "weighted avg      0.701     0.259     0.154     44082\n",
      "\n",
      "[HGB+Cal (WF2)] ROC-AUC=0.844 | PR-AUC=0.615 | Brier=0.124\n",
      "[HGB+Cal (WF2)] Base rate y=1: 0.230 | mean(p1)=0.230 | mean(p0)=0.770\n",
      "[HGB+Cal (WF2)] best-F1=0.604 @ thr=0.294\n",
      "[HGB+Cal (WF2)] ConfMatrix @thr=0.294:\n",
      " [[27730  6221]\n",
      " [ 3062  7069]]\n",
      "[HGB+Cal (WF2)] Report @thr=0.294:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.901     0.817     0.857     33951\n",
      "           1      0.532     0.698     0.604     10131\n",
      "\n",
      "    accuracy                          0.789     44082\n",
      "   macro avg      0.716     0.757     0.730     44082\n",
      "weighted avg      0.816     0.789     0.798     44082\n",
      "\n",
      "  - Split 3: train=(117551, 61), valid=(29388, 61), cutoff=2023-07-29\n",
      "[Logit+Cal (WF3)] ROC-AUC=0.589 | PR-AUC=0.275 | Brier=0.163\n",
      "[Logit+Cal (WF3)] Base rate y=1: 0.207 | mean(p1)=0.207 | mean(p0)=0.793\n",
      "[Logit+Cal (WF3)] best-F1=0.358 @ thr=0.227\n",
      "[Logit+Cal (WF3)] ConfMatrix @thr=0.227:\n",
      " [[13086 10205]\n",
      " [ 2545  3552]]\n",
      "[Logit+Cal (WF3)] Report @thr=0.227:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.837     0.562     0.672     23291\n",
      "           1      0.258     0.583     0.358      6097\n",
      "\n",
      "    accuracy                          0.566     29388\n",
      "   macro avg      0.548     0.572     0.515     29388\n",
      "weighted avg      0.717     0.566     0.607     29388\n",
      "\n",
      "[HGB+Cal (WF3)] ROC-AUC=0.839 | PR-AUC=0.557 | Brier=0.120\n",
      "[HGB+Cal (WF3)] Base rate y=1: 0.207 | mean(p1)=0.207 | mean(p0)=0.793\n",
      "[HGB+Cal (WF3)] best-F1=0.575 @ thr=0.265\n",
      "[HGB+Cal (WF3)] ConfMatrix @thr=0.265:\n",
      " [[18643  4648]\n",
      " [ 1786  4311]]\n",
      "[HGB+Cal (WF3)] Report @thr=0.265:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.913     0.800     0.853     23291\n",
      "           1      0.481     0.707     0.573      6097\n",
      "\n",
      "    accuracy                          0.781     29388\n",
      "   macro avg      0.697     0.754     0.713     29388\n",
      "weighted avg      0.823     0.781     0.795     29388\n",
      "\n",
      "\n",
      "Logit — Walk-forward CV:\n",
      "                         cutoff   roc_auc    pr_auc     brier       thr\n",
      "0 2023-04-26 00:30:52.808999936  0.558101  0.283581  0.180803  0.190750\n",
      "1 2023-06-11 16:14:11.783000064  0.569474  0.284715  0.175720  0.175010\n",
      "2 2023-07-29 19:52:43.941999872  0.588571  0.275172  0.162625  0.227073\n",
      "Logit — medias: ROC-AUC=0.572 | PR-AUC=0.281 | Brier=0.173\n",
      "\n",
      "HGB — Walk-forward CV:\n",
      "                         cutoff   roc_auc    pr_auc     brier       thr\n",
      "0 2023-04-26 00:30:52.808999936  0.848629  0.647578  0.123772  0.290625\n",
      "1 2023-06-11 16:14:11.783000064  0.843766  0.614984  0.124089  0.294118\n",
      "2 2023-07-29 19:52:43.941999872  0.838577  0.557450  0.120149  0.264672\n",
      "HGB — medias:   ROC-AUC=0.844 | PR-AUC=0.607 | Brier=0.123\n",
      "\n",
      ">>> Modelo elegido por PR-AUC medio: HGB\n",
      "\n",
      ">>> Holdout temporal: train=(117551, 61), holdout=(29388, 61), corte=2023-07-29\n",
      ">>> Checks anti-fuga OK: sin solapes y split temporal correcto.\n",
      "[HOLDOUT-Logit] ROC-AUC=0.589 | PR-AUC=0.275 | Brier=0.163\n",
      "[HOLDOUT-Logit] Base rate y=1: 0.207 | mean(p1)=0.207 | mean(p0)=0.793\n",
      "[HOLDOUT-Logit] best-F1=0.358 @ thr=0.227\n",
      "[HOLDOUT-Logit] ConfMatrix @thr=0.227:\n",
      " [[13086 10205]\n",
      " [ 2545  3552]]\n",
      "[HOLDOUT-Logit] Report @thr=0.227:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.837     0.562     0.672     23291\n",
      "           1      0.258     0.583     0.358      6097\n",
      "\n",
      "    accuracy                          0.566     29388\n",
      "   macro avg      0.548     0.572     0.515     29388\n",
      "weighted avg      0.717     0.566     0.607     29388\n",
      "\n",
      "\n",
      "[HOLDOUT-Logit] Puntos de operación sugeridos:\n",
      " - maxF1     : thr=0.227 | precision=0.258 | recall=0.583 | F1=0.358\n",
      " - prec>=0.6 : thr=0.254 | precision=1.000 | recall=0.000 | F1=0.000\n",
      " - rec>=0.7  : thr=0.214 | precision=0.235 | recall=0.701 | F1=0.352\n",
      "[HOLDOUT-HGB] ROC-AUC=0.839 | PR-AUC=0.557 | Brier=0.120\n",
      "[HOLDOUT-HGB] Base rate y=1: 0.207 | mean(p1)=0.207 | mean(p0)=0.793\n",
      "[HOLDOUT-HGB] best-F1=0.575 @ thr=0.265\n",
      "[HOLDOUT-HGB] ConfMatrix @thr=0.265:\n",
      " [[18643  4648]\n",
      " [ 1786  4311]]\n",
      "[HOLDOUT-HGB] Report @thr=0.265:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.913     0.800     0.853     23291\n",
      "           1      0.481     0.707     0.573      6097\n",
      "\n",
      "    accuracy                          0.781     29388\n",
      "   macro avg      0.697     0.754     0.713     29388\n",
      "weighted avg      0.823     0.781     0.795     29388\n",
      "\n",
      "\n",
      "[HOLDOUT-HGB] Puntos de operación sugeridos:\n",
      " - maxF1     : thr=0.265 | precision=0.504 | recall=0.669 | F1=0.575\n",
      " - prec>=0.6 : thr=0.448 | precision=0.666 | recall=0.327 | F1=0.439\n",
      " - rec>=0.7  : thr=0.250 | precision=0.481 | recall=0.707 | F1=0.573\n",
      "\n",
      "[HOLDOUT-HGB] Barrido de umbrales (métricas por clase y confusión):\n",
      "  thr  prec_1  rec_1  F1_1  prec_0  rec_0   TP   FP   FN    TN\n",
      "0.200   0.426  0.804 0.557   0.933  0.716 4905 6604 1192 16687\n",
      "0.250   0.477  0.714 0.572   0.914  0.795 4356 4783 1741 18508\n",
      "0.265   0.504  0.669 0.575   0.905  0.828 4081 4009 2016 19282\n",
      "0.300   0.513  0.654 0.575   0.902  0.838 3988 3780 2109 19511\n",
      "0.400   0.546  0.590 0.567   0.890  0.871 3600 2994 2497 20297\n",
      "0.500   0.693  0.300 0.419   0.840  0.965 1831  811 4266 22480\n",
      "\n",
      "[Punto auto] Máxima F1 clase 1: thr≈0.265 | precision=0.504 | recall=0.669 | F1=0.575\n",
      "\n",
      "[Tabla negocio] @thr=0.265\n",
      "                KPI  conteo  tasa_sobre_total\n",
      "    TP (detectados)    4311             0.147\n",
      "FP (falsas alertas)    4648             0.158\n",
      " FN (no detectados)    1786             0.061\n",
      "         TN (sanos)   18643             0.634\n",
      "\n",
      ">>> Artefactos guardados en artifacts_modelo/modelo_calibrado.joblib\n",
      "\n",
      "[HOLDOUT-HGB] Calibración por deciles (p_mean vs y_rate):\n",
      "           index   p_mean   y_rate    n\n",
      "(-0.001, 0.0157] 0.010896 0.010896 4497\n",
      "(0.0157, 0.0255] 0.025153 0.025153 1471\n",
      "(0.0255, 0.0436] 0.039778 0.039778 3067\n",
      "(0.0436, 0.0693] 0.064372 0.064372 2905\n",
      " (0.0693, 0.132] 0.103796 0.103796 2977\n",
      "  (0.132, 0.197] 0.164754 0.164754 2962\n",
      "  (0.197, 0.265] 0.241006 0.241006 3419\n",
      "  (0.265, 0.448] 0.409572 0.409572 5098\n",
      "  (0.448, 0.452] 0.452174 0.452174  115\n",
      "  (0.452, 0.846] 0.674661 0.674661 2877\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MODELO DE RIESGO — VALIDACIÓN TEMPORAL ESTRICTA (SIN FUGA)\n",
    "# ============================================================\n",
    "# Requiere: pandas, numpy, scikit-learn, joblib\n",
    "# Supone que 'clientes' (crudo) está disponible en memoria.\n",
    "#   - columnas de fecha: FechaEvento, FechaVinculacionCliente, FechaUltimoUso, FechaPrimerUso\n",
    "#   - target: PerdidaCartera (0/1)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    classification_report, confusion_matrix, precision_recall_curve\n",
    ")\n",
    "\n",
    "import joblib\n",
    "\n",
    "# -----------------------------\n",
    "# 0) CONFIG\n",
    "# -----------------------------\n",
    "TARGET = \"PerdidaCartera\"\n",
    "RANDOM_STATE = 42\n",
    "ARTIF_DIR = Path(\"./artifacts_modelo\"); ARTIF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "# ============================================================\n",
    "# 1) INGESTA (usa tu carga real). Aseguramos target y forma.\n",
    "# ============================================================\n",
    "# EJEMPLO (descomenta si lo necesitas):\n",
    "# clientes = pd.read_parquet(\"data/clientes.parquet\")\n",
    "\n",
    "assert TARGET in clientes.columns, f\"No encuentro columna target '{TARGET}' en 'clientes'\"\n",
    "print(\">>> SHAPE crudo:\", clientes.shape)\n",
    "print(\">>> Columnas (primeras 20):\", list(clientes.columns)[:20])\n",
    "\n",
    "# ============================================================\n",
    "# 2) FEATURE ENGINEERING — SIN FUGA\n",
    "# ============================================================\n",
    "def parse_and_features(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # --- Fechas\n",
    "    df['FechaEvento_dt'] = (\n",
    "        pd.to_datetime(df['FechaEvento'], errors='coerce', utc=True)\n",
    "          .dt.tz_convert(None)\n",
    "    )\n",
    "    df['FechaVinculacionCliente_dt'] = pd.to_datetime(\n",
    "        df['FechaVinculacionCliente'], errors='coerce',\n",
    "        origin='1899-12-30', unit='D'\n",
    "    )\n",
    "    df['FechaUltimoUso_dt'] = pd.to_datetime(\n",
    "        df['FechaUltimoUso'], errors='coerce',\n",
    "        origin='1899-12-30', unit='D'\n",
    "    )\n",
    "    df['FechaPrimerUso_dt'] = pd.to_datetime(\n",
    "        df['FechaPrimerUso'], errors='coerce',\n",
    "        origin='1904-01-01', unit='D'\n",
    "    )\n",
    "\n",
    "    # Corrección conservadora PrimerUso\n",
    "    df['Flag_PrimerUsoAntesVinc'] = (df['FechaPrimerUso_dt'] < df['FechaVinculacionCliente_dt']).astype(int)\n",
    "    mask_bad = df['FechaPrimerUso_dt'].isna() | (df['FechaPrimerUso_dt'] < df['FechaVinculacionCliente_dt'])\n",
    "    df.loc[mask_bad,  'FechaPrimerUso_corr'] = df.loc[mask_bad,  'FechaVinculacionCliente_dt']\n",
    "    df.loc[~mask_bad, 'FechaPrimerUso_corr'] = df.loc[~mask_bad, 'FechaPrimerUso_dt']\n",
    "\n",
    "    # Diferencias seguras\n",
    "    def safe_months(a, b):\n",
    "        d = (a - b).dt.days\n",
    "        d = d.where(d.notna(), 0); d = np.where(d < 0, 0, d)\n",
    "        return d / 30.0\n",
    "\n",
    "    def safe_days(a, b):\n",
    "        d = (a - b).dt.days\n",
    "        d = d.where(d.notna(), 0); d = np.where(d < 0, 0, d)\n",
    "        return d\n",
    "\n",
    "    df['MesesDesdeVinculacion'] = safe_months(df['FechaEvento_dt'], df['FechaVinculacionCliente_dt'])\n",
    "    df['MesesDesdePrimerUso']   = safe_months(df['FechaEvento_dt'], df['FechaPrimerUso_corr'])\n",
    "    df['DiasDesdeUltimoUso']    = safe_days(df['FechaEvento_dt'],  df['FechaUltimoUso_dt'])\n",
    "\n",
    "    # --- Usabilidad\n",
    "    df['UsabilidadCupo'] = pd.to_numeric(df['UsabilidadCupo'], errors='coerce')\n",
    "    df['Flag_Usab_NaN']    = df['UsabilidadCupo'].isna().astype(int)\n",
    "    df['Flag_Usab_Outlier']= ((df['UsabilidadCupo'] < 0) | (df['UsabilidadCupo'] > 2)).astype(int)\n",
    "    df.loc[(df['UsabilidadCupo'] < 0) | (df['UsabilidadCupo'] > 2), 'UsabilidadCupo'] = np.nan\n",
    "    df['UsabilidadCupo']   = df['UsabilidadCupo'].fillna(df['UsabilidadCupo'].median())\n",
    "\n",
    "    # --- Flags de fechas\n",
    "    df['Flag_UltimoUsoPosterior'] = (df['FechaUltimoUso_dt'] > df['FechaEvento_dt']).fillna(False).astype(int)\n",
    "\n",
    "    # --- Numéricas con nulos —> flags + imputación conservadora\n",
    "    num_cols_candidates = [\n",
    "        'DiasMaximosMoraCreditosGenerados',\n",
    "        'NumeroCreditosGPrevius','NumeroCreditosGCanalFPrevius','NumeroCreditosGCanalVPrevius',\n",
    "        'NumeroCreditosGEstadoActivosPrevius','NumeroCreditosGEstadoPagadosPrevius',\n",
    "        'NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius','NumeroCreditosLEstadoPagadosPrevius',\n",
    "        'TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroIntentosFallidos','CupoAprobado','ScoreCrediticio','Edad',\n",
    "        'MesesDesdeVinculacion','MesesDesdePrimerUso','DiasDesdeUltimoUso'\n",
    "    ]\n",
    "    for c in num_cols_candidates:\n",
    "        if c in df.columns:\n",
    "            df[f'Flag_{c}_NaN'] = df[c].isna().astype(int)\n",
    "            if c.startswith('NumeroCreditos') or c.startswith('TotalPagos'):\n",
    "                df[c] = df[c].fillna(0)\n",
    "            elif c in ['DiasMaximosMoraCreditosGenerados','Edad','NumeroIntentosFallidos']:\n",
    "                df[c] = df[c].fillna(0)\n",
    "\n",
    "    # --- Score & Cupo\n",
    "    df['ScoreSinInfo'] = (df['ScoreCrediticio'] == 0).astype(int)\n",
    "    if df['ScoreCrediticio'].isna().any():\n",
    "        med_pos = df.loc[df['ScoreCrediticio']>0, 'ScoreCrediticio'].median()\n",
    "        df['ScoreCrediticio'] = df['ScoreCrediticio'].fillna(med_pos)\n",
    "\n",
    "    df['log_CupoAprobado'] = np.log1p(df['CupoAprobado'])\n",
    "    df['log_CupoAprobado'] = df['log_CupoAprobado'].fillna(df['log_CupoAprobado'].median())\n",
    "\n",
    "    # --- Categóricas limpias\n",
    "    for c in ['CategoriaPrincipalCredito','UsoAppWeb','Genero','TipoMunicipioEntregaTC','CanalMunicipioEntregaTC']:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna('Desconocido')\n",
    "    df['Genero'] = df['Genero'].replace({27:'Desconocido'})\n",
    "    df['TipoMunicipioEntregaTC'] = df['TipoMunicipioEntregaTC'].replace({'PEQUEÃ‘O':'PEQUEÑO'}).fillna('Desconocido')\n",
    "\n",
    "    # --- Rango de Edad\n",
    "    df['Flag_Edad_Out'] = (~df['Edad'].between(18, 100, inclusive='both')).fillna(False).astype(int)\n",
    "    df.loc[df['Flag_Edad_Out']==1, 'Edad'] = np.nan\n",
    "    df['Edad'] = df['Edad'].fillna(df['Edad'].median())\n",
    "\n",
    "    # --- Features derivadas clave\n",
    "    if 'Flag_PrimerUsoTemu' not in df.columns and 'NumeroCreditosGPrevius' in df.columns:\n",
    "        df['Flag_PrimerUsoTemu'] = (df['NumeroCreditosGPrevius'] == 0).astype(int)\n",
    "\n",
    "    df['ratio_pagos_local_global'] = (\n",
    "        df['TotalPagosEfectuadosLocalmentePrevius'].fillna(0) /\n",
    "        (df['TotalPagosEfectuadosGlobalmentePrevius'].fillna(0) + 1.0)\n",
    "    )\n",
    "    df['creditos_activos_ratio'] = (\n",
    "        df['NumeroCreditosGEstadoActivosPrevius'].fillna(0) /\n",
    "        (df['NumeroCreditosGPrevius'].fillna(0) + 1.0)\n",
    "    )\n",
    "    for c in ['ratio_pagos_local_global','creditos_activos_ratio']:\n",
    "        df[c] = df[c].replace([np.inf,-np.inf], np.nan).fillna(0).clip(0,1)\n",
    "\n",
    "    df['Flag_CanalVirtual'] = (\n",
    "        (df['CanalMunicipioEntregaTC'].astype(str).str.lower()=='virtual') |\n",
    "        (df['TipoMunicipioEntregaTC'].astype(str).str.upper()=='VIRTUAL')\n",
    "    ).astype(int)\n",
    "\n",
    "    # --- Score negativo -> 0, más bucket\n",
    "    df['Flag_Score_Negativo'] = (df['ScoreCrediticio'] < 0).astype(int)\n",
    "    df.loc[df['ScoreCrediticio'] < 0, 'ScoreCrediticio'] = 0\n",
    "\n",
    "    df['ScoreBucket'] = 'sin_info'\n",
    "    mask_pos = df['ScoreCrediticio'] > 0\n",
    "    if mask_pos.sum() > 0:\n",
    "        b1, b2 = df.loc[mask_pos, 'ScoreCrediticio'].quantile([0.33, 0.66]).values\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] <= b1), 'ScoreBucket'] = 'bajo'\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] >  b1) & (df['ScoreCrediticio'] <= b2), 'ScoreBucket'] = 'medio'\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] >  b2), 'ScoreBucket'] = 'alto'\n",
    "    df['ScoreBucket'] = pd.Categorical(df['ScoreBucket'], categories=['sin_info','bajo','medio','alto'], ordered=True)\n",
    "\n",
    "    # --- Winsorización p99 + flags (colas largas)\n",
    "    def cap_with_flag(s, upper):\n",
    "        flag = (s > upper).astype(int)\n",
    "        return np.where(s > upper, upper, s), flag\n",
    "\n",
    "    cap_cols = [\n",
    "        'DiasDesdeUltimoUso','MesesDesdeVinculacion',\n",
    "        'TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroCreditosGPrevius','NumeroCreditosGCanalFPrevius','NumeroCreditosGCanalVPrevius',\n",
    "        'NumeroCreditosGEstadoActivosPrevius','NumeroCreditosGEstadoPagadosPrevius',\n",
    "        'NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius','NumeroCreditosLEstadoPagadosPrevius'\n",
    "    ]\n",
    "    for c in cap_cols:\n",
    "        if c in df.columns:\n",
    "            p99 = df[c].quantile(0.99)\n",
    "            capped, flag = cap_with_flag(df[c].fillna(0), p99)\n",
    "            df[c] = capped\n",
    "            df[f'Flag_{c}_Capped'] = flag\n",
    "\n",
    "    # Agrupar categorías raras de CategoriaPrincipalCredito (<0.1%)\n",
    "    if 'CategoriaPrincipalCredito' in df.columns:\n",
    "        vc = df['CategoriaPrincipalCredito'].astype(str).value_counts(dropna=False)\n",
    "        cutoff = df.shape[0] * 0.001\n",
    "        rare_levels = vc[vc < cutoff].index\n",
    "        df['CategoriaPrincipalCredito'] = df['CategoriaPrincipalCredito'].astype(str)\n",
    "        df.loc[df['CategoriaPrincipalCredito'].isin(rare_levels), 'CategoriaPrincipalCredito'] = 'OtrosRare'\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_feature_lists(df: pd.DataFrame):\n",
    "    drop_from_features = [\n",
    "        'IdentificadorCliente','FechaEvento','FechaVinculacionCliente','FechaPrimerUso','FechaUltimoUso',\n",
    "        'FechaEvento_dt','FechaVinculacionCliente_dt','FechaPrimerUso_dt','FechaUltimoUso_dt','FechaPrimerUso_corr',\n",
    "        'CodigoAlmacenEntregaTC','CodigoAlmacenEntregaTC_str','AlmacenTop20',\n",
    "        'CodigoMunicipioEntregaTC','MunicipioCat','MunicipioTop20','MesCompra',\n",
    "        'DiasMora'\n",
    "    ]\n",
    "    num_base = [\n",
    "        'UsabilidadCupo','MesesDesdeVinculacion','MesesDesdePrimerUso','DiasDesdeUltimoUso',\n",
    "        'NumeroCreditosGPrevius','NumeroCreditosGCanalFPrevius','NumeroCreditosGCanalVPrevius',\n",
    "        'NumeroCreditosGEstadoActivosPrevius','NumeroCreditosGEstadoPagadosPrevius',\n",
    "        'NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius','NumeroCreditosLEstadoPagadosPrevius',\n",
    "        'TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroIntentosFallidos','ScoreCrediticio','ScoreSinInfo','CupoAprobado','log_CupoAprobado','Edad',\n",
    "        'Flag_Usab_NaN','Flag_Usab_Outlier','Flag_PrimerUsoAntesVinc','Flag_UltimoUsoPosterior','Flag_Edad_Out',\n",
    "        'ratio_pagos_local_global','creditos_activos_ratio','Flag_Score_Negativo'\n",
    "    ]\n",
    "    num_dyn = [c for c in df.columns if c.startswith('Flag_') and (c.endswith('_NaN') or c.endswith('_Capped'))]\n",
    "    num_final = [c for c in (num_base + num_dyn) if c in df.columns and c not in drop_from_features]\n",
    "\n",
    "    cat_final = [\n",
    "        'Genero','TipoMunicipioEntregaTC','CanalMunicipioEntregaTC','UsoAppWeb','CategoriaPrincipalCredito',\n",
    "        'Flag_PrimerUsoTemu','ScoreBucket'\n",
    "    ]\n",
    "    cat_final = [c for c in cat_final if c in df.columns and c not in drop_from_features]\n",
    "\n",
    "    # Evitar solape entre num y cat + forzar unicidad y orden estable\n",
    "    overlap = set(num_final) & set(cat_final)\n",
    "    if overlap:\n",
    "        print(f\">>> Aviso: {len(overlap)} columnas estaban en num y cat. Se quitan de num: {sorted(overlap)}\")\n",
    "        num_final = [c for c in num_final if c not in overlap]\n",
    "\n",
    "    num_final = list(dict.fromkeys(num_final))\n",
    "    cat_final = list(dict.fromkeys(cat_final))\n",
    "    return num_final, cat_final\n",
    "\n",
    "\n",
    "def prune_low_variance(X: pd.DataFrame):\n",
    "    low_var = [c for c in X.columns if X[c].nunique(dropna=False) <= 1]\n",
    "    if low_var:\n",
    "        print(\">>> Poda de columnas casi-constantes:\", low_var)\n",
    "        return X.drop(columns=low_var, errors='ignore'), low_var\n",
    "    else:\n",
    "        print(\">>> Poda de columnas casi-constantes: ninguna\")\n",
    "        return X, []\n",
    "\n",
    "\n",
    "def build_pipelines(cat_cols, num_cols):\n",
    "    pre_logit = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('imp', SimpleImputer(strategy='most_frequent')),\n",
    "                ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "            ]), cat_cols),\n",
    "            ('num', Pipeline(steps=[\n",
    "                ('imp', SimpleImputer(strategy='median'))\n",
    "            ]), num_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    logit = Pipeline(steps=[\n",
    "        ('pre', pre_logit),\n",
    "        ('clf', LogisticRegression(\n",
    "            solver='saga', penalty='l2', class_weight='balanced',\n",
    "            max_iter=800, n_jobs=-1, random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    pre_hgb = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), cat_cols),\n",
    "            ('num', 'passthrough', num_cols),\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    hgb = Pipeline(steps=[\n",
    "        ('pre', pre_hgb),\n",
    "        ('clf', HistGradientBoostingClassifier(\n",
    "            learning_rate=0.07, max_leaf_nodes=31, l2_regularization=1.0,\n",
    "            random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ])\n",
    "    return logit, hgb\n",
    "\n",
    "\n",
    "def evaluate_proba(y_true, y_proba, name=\"model\"):\n",
    "    # Métricas de probas + resumen de ambos lados\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    ap  = average_precision_score(y_true, y_proba)\n",
    "    br  = brier_score_loss(y_true, y_proba)\n",
    "    print(f\"[{name}] ROC-AUC={auc:.3f} | PR-AUC={ap:.3f} | Brier={br:.3f}\")\n",
    "    print(f\"[{name}] Base rate y=1: {y_true.mean():.3f} | mean(p1)={np.mean(y_proba):.3f} | mean(p0)={np.mean(1-y_proba):.3f}\")\n",
    "    p, r, thr = precision_recall_curve(y_true, y_proba)\n",
    "    f1 = 2*p*r/(p+r+1e-12)\n",
    "    j  = np.argmax(f1)\n",
    "    best_thr = thr[j-1] if j>0 and j-1 < len(thr) else 0.5\n",
    "    y_hat = (y_proba >= best_thr).astype(int)\n",
    "    print(f\"[{name}] best-F1={f1[j]:.3f} @ thr={best_thr:.3f}\")\n",
    "    print(f\"[{name}] ConfMatrix @thr={best_thr:.3f}:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    print(f\"[{name}] Report @thr={best_thr:.3f}:\\n\", classification_report(y_true, y_hat, digits=3))\n",
    "    return dict(roc_auc=auc, pr_auc=ap, brier=br, thr=best_thr)\n",
    "\n",
    "\n",
    "def operating_points(y_true, y_proba, name=\"model\"):\n",
    "    p, r, thr = precision_recall_curve(y_true, y_proba)\n",
    "    f1 = 2*p*r/(p+r+1e-12)\n",
    "    pts = {}\n",
    "    # máx F1\n",
    "    j = np.argmax(f1); pts['maxF1'] = (thr[j-1] if j>0 else 0.5, p[j], r[j], f1[j])\n",
    "    # precisión >= 0.6\n",
    "    idx = np.where(p>=0.6)[0]\n",
    "    if len(idx)>0:\n",
    "        k = idx[np.argmax(r[idx])]\n",
    "        pts['prec>=0.6'] = (thr[k-1] if k>0 else 0.5, p[k], r[k], f1[k])\n",
    "    # recall >= 0.7\n",
    "    idx = np.where(r>=0.7)[0]\n",
    "    if len(idx)>0:\n",
    "        k = idx[np.argmax(p[idx])]\n",
    "        pts['rec>=0.7'] = (thr[k-1] if k>0 else 0.5, p[k], r[k], f1[k])\n",
    "\n",
    "    print(f\"\\n[{name}] Puntos de operación sugeridos:\")\n",
    "    for kk,(t,pp,rr,ff) in pts.items():\n",
    "        print(f\" - {kk:10s}: thr={t:.3f} | precision={pp:.3f} | recall={rr:.3f} | F1={ff:.3f}\")\n",
    "    return pts\n",
    "\n",
    "\n",
    "def walk_forward_cv(df, X, y, cat_cols, num_cols, cut_fracs=(0.6,0.7,0.8)):\n",
    "    \"\"\"Valida SIN FUGA con 3 cortes temporales (ajustable).\"\"\"\n",
    "    cutoffs = df['FechaEvento_dt'].quantile(list(cut_fracs)).values\n",
    "    rows_l, rows_h = [], []\n",
    "    logit, hgb = build_pipelines(cat_cols, num_cols)\n",
    "\n",
    "    print(\"\\n>>> Walk-forward CV (sin fuga):\")\n",
    "    for i, c in enumerate(cutoffs, 1):\n",
    "        tr_idx = df['FechaEvento_dt'] <= c\n",
    "        va_idx = df['FechaEvento_dt'] >  c\n",
    "        X_tr, X_va = X.loc[tr_idx], X.loc[va_idx]\n",
    "        y_tr, y_va = y.loc[tr_idx], y.loc[va_idx]\n",
    "        print(f\"  - Split {i}: train={X_tr.shape}, valid={X_va.shape}, cutoff={pd.Timestamp(c).date()}\")\n",
    "\n",
    "        # Logit + calibración\n",
    "        logit.fit(X_tr, y_tr)\n",
    "        cal_logit = CalibratedClassifierCV(logit, method='sigmoid', cv='prefit')\n",
    "        cal_logit.fit(X_va, y_va)\n",
    "        proba_l = cal_logit.predict_proba(X_va)[:,1]\n",
    "        m_l = evaluate_proba(y_va, proba_l, name=f\"Logit+Cal (WF{i})\")\n",
    "        rows_l.append({\"cutoff\": c, **m_l})\n",
    "\n",
    "        # HGB + sample_weight + calibración\n",
    "        sw = compute_sample_weight(\"balanced\", y_tr)\n",
    "        hgb.fit(X_tr, y_tr, clf__sample_weight=sw)\n",
    "        cal_hgb = CalibratedClassifierCV(hgb, method='isotonic', cv='prefit')\n",
    "        cal_hgb.fit(X_va, y_va)\n",
    "        proba_h = cal_hgb.predict_proba(X_va)[:,1]\n",
    "        m_h = evaluate_proba(y_va, proba_h, name=f\"HGB+Cal (WF{i})\")\n",
    "        rows_h.append({\"cutoff\": c, **m_h})\n",
    "\n",
    "    return pd.DataFrame(rows_l), pd.DataFrame(rows_h)\n",
    "\n",
    "# ============================================================\n",
    "# 3) CONSTRUIR DATASET LIMPIO + LISTAS DE FEATURES + EDA\n",
    "# ============================================================\n",
    "print(\"\\n>>> Construyendo features (sin fuga) ...\")\n",
    "clientes = parse_and_features(clientes)\n",
    "num_final, cat_final = build_feature_lists(clientes)\n",
    "\n",
    "X = clientes[num_final + cat_final].copy()\n",
    "y = clientes[TARGET].astype(int).copy()\n",
    "\n",
    "# 3A) EDA previa (resumen, correlaciones, crosstabs)\n",
    "print(\"\\n===================== EDA PREVIA (SIN FUGA) =====================\")\n",
    "print(f\"Shape de X: {X.shape} | y rate (1)= {y.mean():.3f}\")\n",
    "print(\"Rango de fechas (FechaEvento_dt):\",\n",
    "      str(clientes['FechaEvento_dt'].min().date()), \"→\", str(clientes['FechaEvento_dt'].max().date()))\n",
    "\n",
    "vc = y.value_counts().rename({0:'no_perdida', 1:'perdida'})\n",
    "print(\"\\n[Balance de clases]\")\n",
    "print(pd.concat([vc, (vc/vc.sum()).round(3).rename('pct')], axis=1).to_string())\n",
    "\n",
    "summary_rows = []\n",
    "for c in X.columns:\n",
    "    s = X[c]\n",
    "    dtype = s.dtype\n",
    "    pct_null = s.isnull().mean()*100\n",
    "    nuni = s.nunique(dropna=False)\n",
    "    # FIX: usar is_numeric_dtype para que no falle con CategoricalDtype\n",
    "    top5 = s.value_counts(dropna=False).head(5).to_dict() if (not is_numeric_dtype(s)) else {}\n",
    "    summary_rows.append({\n",
    "        'columna': c, 'dtype': str(dtype), '%nulos': round(pct_null,1),\n",
    "        'n_unicos': int(nuni), 'top5_valores': top5\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(['dtype','columna'])\n",
    "print(\"\\n[Resumen por columna] (primeras 30 filas)\")\n",
    "print(summary_df.head(30).to_string(index=False))\n",
    "summary_df.to_csv(ARTIF_DIR/\"eda_resumen_columnas.csv\", index=False)\n",
    "\n",
    "num_cols_for_corr = [c for c in X.columns if is_numeric_dtype(X[c]) and c != TARGET]\n",
    "if len(num_cols_for_corr) > 0:\n",
    "    corrs = X[num_cols_for_corr].corrwith(y).sort_values(ascending=False)\n",
    "    print(\"\\n[Correlación numéricas vs target] (top 20)\")\n",
    "    print(corrs.head(20).round(3).to_string())\n",
    "    print(\"\\n[Correlación numéricas vs target] (bottom 20)\")\n",
    "    print(corrs.tail(20).round(3).to_string())\n",
    "\n",
    "cat_cols_for_xtab = [c for c in X.columns if not is_numeric_dtype(X[c])]\n",
    "for c in cat_cols_for_xtab:\n",
    "    # sólo crosstab para cardinalidad moderada\n",
    "    if X[c].nunique(dropna=False) <= 25:\n",
    "        tab = pd.crosstab(X[c], y, normalize='index').rename(columns={0:'no_perdida',1:'perdida'}).round(3)\n",
    "        print(f\"\\n[Distribución {c} → proporción de pérdida por categoría]\")\n",
    "        print(tab.sort_values('perdida', ascending=False).to_string())\n",
    "\n",
    "print(\"\\n================= FIN EDA PREVIA (SIGUE MODELADO) =================\")\n",
    "\n",
    "# 3B) Limpiezas extra útiles antes de modelar\n",
    "# --- ELIMINAR COLUMNAS DUPLICADAS EN X ---\n",
    "dups_mask = X.columns.duplicated(keep='first')\n",
    "if dups_mask.any():\n",
    "    dups = pd.Series(X.columns)[dups_mask].tolist()\n",
    "    print(f\">>> Columnas duplicadas detectadas y removidas ({len(dups)}): {dups}\")\n",
    "    X = X.loc[:, ~X.columns.duplicated(keep='first')]\n",
    "else:\n",
    "    print(\">>> Sin columnas duplicadas en X.\")\n",
    "\n",
    "# --- PODA BAJA VARIANZA ---\n",
    "X, dropped_lowvar = prune_low_variance(X)\n",
    "\n",
    "print(\"\\n=== Resumen columnas finales (previas al modelado) ===\")\n",
    "print(f\"Numéricas: {len([c for c in X.columns if c in num_final])} | Categóricas: {len([c for c in X.columns if c in cat_final])}\")\n",
    "print(\"X shape:\", X.shape, \"| y rate (1):\", y.mean().round(3))\n",
    "print(\"FechaEvento_dt rango:\", str(clientes['FechaEvento_dt'].min().date()), \"→\", str(clientes['FechaEvento_dt'].max().date()))\n",
    "\n",
    "# ============================================================\n",
    "# 4) WALK-FORWARD CV — SIN MIRAR EL FUTURO\n",
    "# ============================================================\n",
    "cat_cols = [c for c in cat_final if c in X.columns]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "logit_cv, hgb_cv = walk_forward_cv(clientes, X, y, cat_cols, num_cols)\n",
    "\n",
    "print(\"\\nLogit — Walk-forward CV:\")\n",
    "print(logit_cv)\n",
    "print(f\"Logit — medias: ROC-AUC={logit_cv.roc_auc.mean():.3f} | PR-AUC={logit_cv.pr_auc.mean():.3f} | Brier={logit_cv.brier.mean():.3f}\")\n",
    "\n",
    "print(\"\\nHGB — Walk-forward CV:\")\n",
    "print(hgb_cv)\n",
    "print(f\"HGB — medias:   ROC-AUC={hgb_cv.roc_auc.mean():.3f} | PR-AUC={hgb_cv.pr_auc.mean():.3f} | Brier={hgb_cv.brier.mean():.3f}\")\n",
    "\n",
    "chosen = \"HGB\" if hgb_cv.pr_auc.mean() >= logit_cv.pr_auc.mean() else \"Logit\"\n",
    "print(f\"\\n>>> Modelo elegido por PR-AUC medio: {chosen}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) HOLDOUT FINAL (FUTURO) — 20% MÁS RECIENTE + CHEQUEOS\n",
    "# ============================================================\n",
    "cutoff = clientes['FechaEvento_dt'].quantile(0.8)\n",
    "train_idx = clientes['FechaEvento_dt'] <= cutoff\n",
    "hold_idx  = clientes['FechaEvento_dt'] >  cutoff\n",
    "X_tr, X_ho = X.loc[train_idx], X.loc[hold_idx]\n",
    "y_tr, y_ho = y.loc[train_idx], y.loc[hold_idx]\n",
    "print(f\"\\n>>> Holdout temporal: train={X_tr.shape}, holdout={X_ho.shape}, corte={pd.Timestamp(cutoff).date()}\")\n",
    "\n",
    "# Chequeos anti-fuga (muy importantes)\n",
    "idx_train = set(X_tr.index); idx_hold = set(X_ho.index)\n",
    "assert idx_train.isdisjoint(idx_hold), \"Solapamiento entre train y holdout\"\n",
    "assert clientes.loc[X_tr.index, 'FechaEvento_dt'].max() <= cutoff, \"Train tiene fechas > cutoff\"\n",
    "assert clientes.loc[X_ho.index, 'FechaEvento_dt'].min() >  cutoff, \"Holdout tiene fechas <= cutoff\"\n",
    "print(\">>> Checks anti-fuga OK: sin solapes y split temporal correcto.\")\n",
    "\n",
    "logit, hgb = build_pipelines(cat_cols, num_cols)\n",
    "\n",
    "# LOGIT + calibración\n",
    "logit.fit(X_tr, y_tr)\n",
    "cal_logit = CalibratedClassifierCV(logit, method='sigmoid', cv='prefit')\n",
    "cal_logit.fit(X_ho, y_ho)\n",
    "proba_logit = cal_logit.predict_proba(X_ho)[:,1]\n",
    "_ = evaluate_proba(y_ho, proba_logit, name=\"HOLDOUT-Logit\")\n",
    "operating_points(y_ho, proba_logit, name=\"HOLDOUT-Logit\")\n",
    "\n",
    "# HGB + sample_weight + calibración (modelo principal)\n",
    "sw = compute_sample_weight(\"balanced\", y_tr)\n",
    "hgb.fit(X_tr, y_tr, clf__sample_weight=sw)\n",
    "cal_hgb = CalibratedClassifierCV(hgb, method='isotonic', cv='prefit')\n",
    "cal_hgb.fit(X_ho, y_ho)\n",
    "proba_hgb = cal_hgb.predict_proba(X_ho)[:,1]\n",
    "m_hold = evaluate_proba(y_ho, proba_hgb, name=\"HOLDOUT-HGB\")\n",
    "operating_points(y_ho, proba_hgb, name=\"HOLDOUT-HGB\")\n",
    "\n",
    "# --- Barrido de umbrales con métricas por clase + confusiones\n",
    "def sweep_thresholds(y_true, y_proba, thresholds=(0.20, 0.25, 0.265, 0.30, 0.40, 0.50)):\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        yhat = (y_proba >= t).astype(int)\n",
    "        TN, FP, FN, TP = confusion_matrix(y_true, yhat).ravel()\n",
    "        prec1 = TP / (TP+FP) if TP+FP>0 else 0.0\n",
    "        rec1  = TP / (TP+FN) if TP+FN>0 else 0.0\n",
    "        f1    = (2*prec1*rec1/(prec1+rec1)) if (prec1+rec1)>0 else 0.0\n",
    "        prec0 = TN / (TN+FN) if TN+FN>0 else 0.0   # precisión clase 0\n",
    "        rec0  = TN / (TN+FP) if TN+FP>0 else 0.0   # recall clase 0\n",
    "        rows.append({\n",
    "            \"thr\": t,\n",
    "            \"prec_1\": round(prec1,3), \"rec_1\": round(rec1,3), \"F1_1\": round(f1,3),\n",
    "            \"prec_0\": round(prec0,3), \"rec_0\": round(rec0,3),\n",
    "            \"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN\n",
    "        })\n",
    "    tab = pd.DataFrame(rows)\n",
    "    print(\"\\n[HOLDOUT-HGB] Barrido de umbrales (métricas por clase y confusión):\")\n",
    "    print(tab.to_string(index=False))\n",
    "    tab.to_csv(ARTIF_DIR/\"holdout_umbral_sweep.csv\", index=False)\n",
    "    return tab\n",
    "\n",
    "tab_thr = sweep_thresholds(y_ho, proba_hgb)\n",
    "\n",
    "# --- Punto de operación por máxima F1 (automático)\n",
    "p, r, thr = precision_recall_curve(y_ho, proba_hgb)\n",
    "f1 = 2*p*r/(p+r+1e-12)\n",
    "j  = np.argmax(f1)\n",
    "thr_star = thr[j-1] if j>0 and j-1<len(thr) else 0.5\n",
    "print(f\"\\n[Punto auto] Máxima F1 clase 1: thr≈{thr_star:.3f} | precision={p[j]:.3f} | recall={r[j]:.3f} | F1={f1[j]:.3f}\")\n",
    "\n",
    "# --- Tabla de negocio (tasas sobre total)\n",
    "def business_table(y_true, y_proba, thr):\n",
    "    yhat = (y_proba >= thr).astype(int)\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, yhat).ravel()\n",
    "    n = len(y_true)\n",
    "    out = pd.DataFrame({\n",
    "        \"KPI\": [\"TP (detectados)\",\"FP (falsas alertas)\",\"FN (no detectados)\",\"TN (sanos)\"],\n",
    "        \"conteo\": [TP, FP, FN, TN],\n",
    "        \"tasa_sobre_total\": [round(TP/n,3), round(FP/n,3), round(FN/n,3), round(TN/n,3)]\n",
    "    })\n",
    "    print(f\"\\n[Tabla negocio] @thr={thr:.3f}\")\n",
    "    print(out.to_string(index=False))\n",
    "    out.to_csv(ARTIF_DIR/\"holdout_tabla_negocio.csv\", index=False)\n",
    "    return out\n",
    "\n",
    "_ = business_table(y_ho, proba_hgb, thr_star)\n",
    "\n",
    "# Guardar bundle de artefactos (modelo calibrado + metadatos)\n",
    "bundle = {\n",
    "    \"model_calibrado\": cal_hgb if chosen==\"HGB\" else cal_logit,\n",
    "    \"chosen\": chosen,\n",
    "    \"num_final\": num_final,\n",
    "    \"cat_final\": cat_final,\n",
    "    \"columns_after_prune\": list(X.columns),\n",
    "    \"cutoff\": cutoff\n",
    "}\n",
    "joblib.dump(bundle, ARTIF_DIR/\"modelo_calibrado.joblib\")\n",
    "print(f\"\\n>>> Artefactos guardados en {ARTIF_DIR/'modelo_calibrado.joblib'}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) CALIBRACIÓN POR DECILES EN HOLDOUT\n",
    "# ============================================================\n",
    "def decile_calibration(y_true, y_proba, name=\"holdout\"):\n",
    "    bins = pd.qcut(y_proba, q=10, duplicates='drop')\n",
    "    tab = pd.DataFrame({\"p\":y_proba, \"y\":y_true}).groupby(bins, observed=True).agg(\n",
    "        p_mean=('p','mean'),\n",
    "        y_rate=('y','mean'),\n",
    "        n=('p','size')\n",
    "    ).reset_index().rename(columns={\"p\":\"bin\"})\n",
    "    print(f\"\\n[{name}] Calibración por deciles (p_mean vs y_rate):\")\n",
    "    print(tab.to_string(index=False))\n",
    "    tab.to_csv(ARTIF_DIR/f\"calibracion_deciles_{name}.csv\", index=False)\n",
    "    return tab\n",
    "\n",
    "_ = decile_calibration(y_ho, proba_hgb, name=\"HOLDOUT-HGB\")\n",
    "\n",
    "# ============================================================\n",
    "# 7) FUNCIÓN DE SCORING FUTURO (para data no vista)\n",
    "# ============================================================\n",
    "def score_future(df_nuevo_raw: pd.DataFrame, artif_path=ARTIF_DIR/\"modelo_calibrado.joblib\", target_col=TARGET,\n",
    "                 export_csv=True):\n",
    "    \"\"\"Aplica el mismo pipeline a df_nuevo. Si trae label, evalúa; si no, solo predice.\"\"\"\n",
    "    bundle = joblib.load(artif_path)\n",
    "    model  = bundle[\"model_calibrado\"]\n",
    "    cols_ok = bundle[\"columns_after_prune\"]\n",
    "    num_final, cat_final = bundle[\"num_final\"], bundle[\"cat_final\"]\n",
    "\n",
    "    df_nuevo = parse_and_features(df_nuevo_raw)\n",
    "    Xn = df_nuevo[num_final + cat_final].copy()\n",
    "    # Alinear columnas esperadas + deduplicar por si acaso\n",
    "    Xn = Xn.reindex(columns=cols_ok, fill_value=np.nan)\n",
    "    Xn = Xn.loc[:, ~Xn.columns.duplicated(keep='first')]\n",
    "    proba1 = model.predict_proba(Xn)[:,1]\n",
    "    proba0 = 1 - proba1\n",
    "\n",
    "    # Resumen probabilidades (ambas clases)\n",
    "    print(\"\\n[SCORING] Resumen probabilidades:\")\n",
    "    print(f\"mean(p0)= {proba0.mean():.3f} | mean(p1)= {proba1.mean():.3f} | min/max p1= {proba1.min():.3f}/{proba1.max():.3f}\")\n",
    "\n",
    "    if target_col in df_nuevo.columns:\n",
    "        y_true = df_nuevo[target_col].astype(int)\n",
    "        _ = evaluate_proba(y_true, proba1, name=\"DF_NUEVO\")\n",
    "        operating_points(y_true, proba1, name=\"DF_NUEVO\")\n",
    "        _ = decile_calibration(y_true, proba1, name=\"DF_NUEVO\")\n",
    "    else:\n",
    "        print(\"df_nuevo sin target: se devuelven solo probabilidades.\")\n",
    "\n",
    "    out = pd.DataFrame({\"p_no_perdida\": proba0, \"p_perdida\": proba1})\n",
    "    if export_csv:\n",
    "        out_path = ARTIF_DIR/\"scoring_df_nuevo.csv\"\n",
    "        out.to_csv(out_path, index=False)\n",
    "        print(f\"Scoring exportado a {out_path}\")\n",
    "    return out\n",
    "\n",
    "# ------------- QUICK TEST -------------\n",
    "# Simular \"futuro real\" con el 20% más reciente:\n",
    "# df_nuevo = clientes.loc[clientes['FechaEvento_dt'] > cutoff].copy()\n",
    "# _ = score_future(df_nuevo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2702b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "---2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3adabb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> SHAPE crudo: (146939, 81)\n",
      ">>> Columnas (primeras 20): ['IdentificadorCliente', 'FechaEvento', 'UsabilidadCupo', 'CategoriaPrincipalCredito', 'DiasMaximosMoraCreditosGenerados', 'NumeroCreditosGPrevius', 'NumeroCreditosGCanalFPrevius', 'NumeroCreditosGEstadoActivosPrevius', 'NumeroCreditosGEstadoPagadosPrevius', 'NumeroCreditosGCanalVPrevius', 'NumeroCreditosLPrevius', 'NumeroCreditosLEstadoActivosPrevius', 'NumeroCreditosLEstadoPagadosPrevius', 'FechaVinculacionCliente', 'FechaPrimerUso', 'FechaUltimoUso', 'TotalPagosEfectuadosGlobalmentePrevius', 'TotalPagosEfectuadosLocalmentePrevius', 'CodigoAlmacenEntregaTC', 'CodigoMunicipioEntregaTC']\n",
      "\n",
      ">>> Construyendo features (sin fuga) ...\n",
      ">>> Sin columnas duplicadas en X.\n",
      ">>> Poda de columnas casi-constantes: ['Flag_Usab_NaN', 'Flag_Usab_Outlier', 'Flag_UltimoUsoPosterior', 'Flag_Edad_Out', 'Flag_Score_Negativo', 'Flag_DiasMaximosMoraCreditosGenerados_NaN', 'Flag_NumeroCreditosGPrevius_NaN', 'Flag_NumeroCreditosGCanalFPrevius_NaN', 'Flag_NumeroCreditosGCanalVPrevius_NaN', 'Flag_NumeroCreditosGEstadoActivosPrevius_NaN', 'Flag_NumeroCreditosGEstadoPagadosPrevius_NaN', 'Flag_NumeroCreditosLPrevius_NaN', 'Flag_NumeroCreditosLEstadoActivosPrevius_NaN', 'Flag_NumeroCreditosLEstadoPagadosPrevius_NaN', 'Flag_TotalPagosEfectuadosGlobalmentePrevius_NaN', 'Flag_TotalPagosEfectuadosLocalmentePrevius_NaN', 'Flag_NumeroIntentosFallidos_NaN', 'Flag_ScoreCrediticio_NaN', 'Flag_Edad_NaN', 'Flag_MesesDesdeVinculacion_NaN', 'Flag_MesesDesdePrimerUso_NaN', 'Flag_DiasDesdeUltimoUso_NaN', 'Flag_TotalPagosEfectuadosGlobalmentePrevius_Capped', 'Flag_TotalPagosEfectuadosLocalmentePrevius_Capped', 'Flag_NumeroCreditosGPrevius_Capped', 'Flag_NumeroCreditosGCanalFPrevius_Capped', 'Flag_NumeroCreditosGCanalVPrevius_Capped', 'Flag_NumeroCreditosGEstadoActivosPrevius_Capped', 'Flag_NumeroCreditosGEstadoPagadosPrevius_Capped', 'Flag_NumeroCreditosLPrevius_Capped', 'Flag_NumeroCreditosLEstadoActivosPrevius_Capped', 'Flag_NumeroCreditosLEstadoPagadosPrevius_Capped']\n",
      "\n",
      "=== Resumen columnas finales (previas al modelado) ===\n",
      "Numéricas: 26 | Categóricas: 7\n",
      "X shape: (146939, 33) | y rate (1): 0.226\n",
      "FechaEvento_dt rango: 2022-05-01 → 2023-10-31\n",
      "\n",
      ">>> Walk-forward CV (sin fuga):\n",
      "  - Split 1: train=(88163, 33), valid=(58776, 33), cutoff=2023-04-26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 399\u001b[0m\n\u001b[1;32m    396\u001b[0m cat_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cat_final \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m    397\u001b[0m num_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cat_cols]\n\u001b[0;32m--> 399\u001b[0m logit_cv, hgb_cv \u001b[38;5;241m=\u001b[39m walk_forward_cv(clientes, X, y, cat_cols, num_cols)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLogit — Walk-forward CV:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28mprint\u001b[39m(logit_cv)\n",
      "Cell \u001b[0;32mIn[18], line 348\u001b[0m, in \u001b[0;36mwalk_forward_cv\u001b[0;34m(df, X, y, cat_cols, num_cols, cut_fracs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_tr\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, valid=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_va\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, cutoff=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpd\u001b[38;5;241m.\u001b[39mTimestamp(c)\u001b[38;5;241m.\u001b[39mdate()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# Logit + calibración\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m logit\u001b[38;5;241m.\u001b[39mfit(X_tr, y_tr)\n\u001b[1;32m    349\u001b[0m cal_logit \u001b[38;5;241m=\u001b[39m CalibratedClassifierCV(logit, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprefit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    350\u001b[0m cal_logit\u001b[38;5;241m.\u001b[39mfit(X_va, y_va)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:416\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    415\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 416\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:370\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    368\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    371\u001b[0m     cloned_transformer,\n\u001b[1;32m    372\u001b[0m     X,\n\u001b[1;32m    373\u001b[0m     y,\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    375\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    376\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 950\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:743\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m--> 743\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:670\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[0;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[1;32m    664\u001b[0m transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(\n\u001b[1;32m    666\u001b[0m         fitted\u001b[38;5;241m=\u001b[39mfitted, replace_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, column_as_strings\u001b[38;5;241m=\u001b[39mcolumn_as_strings\n\u001b[1;32m    667\u001b[0m     )\n\u001b[1;32m    668\u001b[0m )\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    671\u001b[0m         delayed(func)(\n\u001b[1;32m    672\u001b[0m             transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[1;32m    673\u001b[0m             X\u001b[38;5;241m=\u001b[39m_safe_indexing(X, column, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    674\u001b[0m             y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    675\u001b[0m             weight\u001b[38;5;241m=\u001b[39mweight,\n\u001b[1;32m    676\u001b[0m             message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    677\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(name, idx, \u001b[38;5;28mlen\u001b[39m(transformers)),\n\u001b[1;32m    678\u001b[0m         )\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, trans, column, weight) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transformers, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    680\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 950\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:472\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    470\u001b[0m fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit_transform(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\u001b[38;5;241m.\u001b[39mtransform(Xt)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/impute/_base.py:405\u001b[0m, in \u001b[0;36mSimpleImputer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatistics_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_fit(\n\u001b[1;32m    401\u001b[0m             X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing_values, fill_value\n\u001b[1;32m    402\u001b[0m         )\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatistics_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_fit(\n\u001b[1;32m    406\u001b[0m         X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing_values, fill_value\n\u001b[1;32m    407\u001b[0m     )\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/impute/_base.py:471\u001b[0m, in \u001b[0;36mSimpleImputer._dense_fit\u001b[0;34m(self, X, strategy, missing_values, fill_value)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# Median\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 471\u001b[0m     median_masked \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mmedian(masked_X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;66;03m# Avoid the warning \"Warning: converting a masked element to nan.\"\u001b[39;00m\n\u001b[1;32m    473\u001b[0m     median \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mgetdata(median_masked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/ma/extras.py:735\u001b[0m, in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m m\n\u001b[0;32m--> 735\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ureduce(a, func\u001b[38;5;241m=\u001b[39m_median, keepdims\u001b[38;5;241m=\u001b[39mkeepdims, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout,\n\u001b[1;32m    736\u001b[0m                 overwrite_input\u001b[38;5;241m=\u001b[39moverwrite_input)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/function_base.py:3752\u001b[0m, in \u001b[0;36m_ureduce\u001b[0;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[1;32m   3749\u001b[0m             index_out \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, ) \u001b[38;5;241m*\u001b[39m nd\n\u001b[1;32m   3750\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out[(\u001b[38;5;28mEllipsis\u001b[39m, ) \u001b[38;5;241m+\u001b[39m index_out]\n\u001b[0;32m-> 3752\u001b[0m r \u001b[38;5;241m=\u001b[39m func(a, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/ma/extras.py:754\u001b[0m, in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m    752\u001b[0m         asorted \u001b[38;5;241m=\u001b[39m a\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 754\u001b[0m     asorted \u001b[38;5;241m=\u001b[39m sort(a, axis\u001b[38;5;241m=\u001b[39maxis, fill_value\u001b[38;5;241m=\u001b[39mfill_value)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    757\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/ma/core.py:7012\u001b[0m, in \u001b[0;36msort\u001b[0;34m(a, axis, kind, order, endwith, fill_value)\u001b[0m\n\u001b[1;32m   7009\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   7011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, MaskedArray):\n\u001b[0;32m-> 7012\u001b[0m     a\u001b[38;5;241m.\u001b[39msort(axis\u001b[38;5;241m=\u001b[39maxis, kind\u001b[38;5;241m=\u001b[39mkind, order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m   7013\u001b[0m            endwith\u001b[38;5;241m=\u001b[39mendwith, fill_value\u001b[38;5;241m=\u001b[39mfill_value)\n\u001b[1;32m   7014\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   7015\u001b[0m     a\u001b[38;5;241m.\u001b[39msort(axis\u001b[38;5;241m=\u001b[39maxis, kind\u001b[38;5;241m=\u001b[39mkind, order\u001b[38;5;241m=\u001b[39morder)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/ma/core.py:5741\u001b[0m, in \u001b[0;36mMaskedArray.sort\u001b[0;34m(self, axis, kind, order, endwith, fill_value)\u001b[0m\n\u001b[1;32m   5736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   5738\u001b[0m sidx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margsort(axis\u001b[38;5;241m=\u001b[39maxis, kind\u001b[38;5;241m=\u001b[39mkind, order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m   5739\u001b[0m                     fill_value\u001b[38;5;241m=\u001b[39mfill_value, endwith\u001b[38;5;241m=\u001b[39mendwith)\n\u001b[0;32m-> 5741\u001b[0m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtake_along_axis(\u001b[38;5;28mself\u001b[39m, sidx, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MODELO DE RIESGO — VALIDACIÓN TEMPORAL ESTRICTA (SIN FUGA)\n",
    "# ============================================================\n",
    "# Requiere: pandas, numpy, scikit-learn, joblib\n",
    "# Supone que 'clientes' (crudo) está disponible en memoria.\n",
    "#   - columnas de fecha: FechaEvento, FechaVinculacionCliente, FechaUltimoUso, FechaPrimerUso\n",
    "#   - target: PerdidaCartera (0/1)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, brier_score_loss,\n",
    "                             classification_report, confusion_matrix, precision_recall_curve)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# -----------------------------\n",
    "# 0) CONFIG\n",
    "# -----------------------------\n",
    "TARGET = \"PerdidaCartera\"\n",
    "RANDOM_STATE = 42\n",
    "ARTIF_DIR = Path(\"./artifacts_modelo\"); ARTIF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "# ============================================================\n",
    "# 1) INGESTA (usa tu carga real). Aseguramos target y forma.\n",
    "# ============================================================\n",
    "# EJEMPLO (descomenta si lo necesitas):\n",
    "# clientes = pd.read_parquet(\"data/clientes.parquet\")\n",
    "\n",
    "assert TARGET in clientes.columns, f\"No encuentro columna target '{TARGET}' en 'clientes'\"\n",
    "print(\">>> SHAPE crudo:\", clientes.shape)\n",
    "print(\">>> Columnas (primeras 20):\", list(clientes.columns)[:20])\n",
    "\n",
    "# ============================================================\n",
    "# 2) FEATURE ENGINEERING — SIN FUGA\n",
    "# ============================================================\n",
    "def parse_and_features(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # --- Fechas\n",
    "    df['FechaEvento_dt'] = (\n",
    "        pd.to_datetime(df['FechaEvento'], errors='coerce', utc=True)\n",
    "          .dt.tz_convert(None)\n",
    "    )\n",
    "    df['FechaVinculacionCliente_dt'] = pd.to_datetime(\n",
    "        df['FechaVinculacionCliente'], errors='coerce',\n",
    "        origin='1899-12-30', unit='D'\n",
    "    )\n",
    "    df['FechaUltimoUso_dt'] = pd.to_datetime(\n",
    "        df['FechaUltimoUso'], errors='coerce',\n",
    "        origin='1899-12-30', unit='D'\n",
    "    )\n",
    "    df['FechaPrimerUso_dt'] = pd.to_datetime(\n",
    "        df['FechaPrimerUso'], errors='coerce',\n",
    "        origin='1904-01-01', unit='D'\n",
    "    )\n",
    "\n",
    "    # Corrección conservadora PrimerUso\n",
    "    df['Flag_PrimerUsoAntesVinc'] = (df['FechaPrimerUso_dt'] < df['FechaVinculacionCliente_dt']).astype(int)\n",
    "    mask_bad = df['FechaPrimerUso_dt'].isna() | (df['FechaPrimerUso_dt'] < df['FechaVinculacionCliente_dt'])\n",
    "    df.loc[mask_bad,  'FechaPrimerUso_corr'] = df.loc[mask_bad,  'FechaVinculacionCliente_dt']\n",
    "    df.loc[~mask_bad, 'FechaPrimerUso_corr'] = df.loc[~mask_bad, 'FechaPrimerUso_dt']\n",
    "\n",
    "    # Diferencias seguras\n",
    "    def safe_months(a, b):\n",
    "        d = (a - b).dt.days\n",
    "        d = d.where(d.notna(), 0); d = np.where(d < 0, 0, d)\n",
    "        return d / 30.0\n",
    "\n",
    "    def safe_days(a, b):\n",
    "        d = (a - b).dt.days\n",
    "        d = d.where(d.notna(), 0); d = np.where(d < 0, 0, d)\n",
    "        return d\n",
    "\n",
    "    df['MesesDesdeVinculacion'] = safe_months(df['FechaEvento_dt'], df['FechaVinculacionCliente_dt'])\n",
    "    df['MesesDesdePrimerUso']   = safe_months(df['FechaEvento_dt'], df['FechaPrimerUso_corr'])\n",
    "    df['DiasDesdeUltimoUso']    = safe_days(df['FechaEvento_dt'],  df['FechaUltimoUso_dt'])\n",
    "\n",
    "    # --- Usabilidad\n",
    "    df['UsabilidadCupo'] = pd.to_numeric(df['UsabilidadCupo'], errors='coerce')\n",
    "    df['Flag_Usab_NaN']    = df['UsabilidadCupo'].isna().astype(int)\n",
    "    df['Flag_Usab_Outlier']= ((df['UsabilidadCupo'] < 0) | (df['UsabilidadCupo'] > 2)).astype(int)\n",
    "    df.loc[(df['UsabilidadCupo'] < 0) | (df['UsabilidadCupo'] > 2), 'UsabilidadCupo'] = np.nan\n",
    "    df['UsabilidadCupo']   = df['UsabilidadCupo'].fillna(df['UsabilidadCupo'].median())\n",
    "\n",
    "    # --- Flags de fechas\n",
    "    df['Flag_UltimoUsoPosterior'] = (df['FechaUltimoUso_dt'] > df['FechaEvento_dt']).fillna(False).astype(int)\n",
    "\n",
    "    # --- Numéricas con nulos —> flags + imputación conservadora\n",
    "    num_cols_candidates = [\n",
    "        'DiasMaximosMoraCreditosGenerados',\n",
    "        'NumeroCreditosGPrevius','NumeroCreditosGCanalFPrevius','NumeroCreditosGCanalVPrevius',\n",
    "        'NumeroCreditosGEstadoActivosPrevius','NumeroCreditosGEstadoPagadosPrevius',\n",
    "        'NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius','NumeroCreditosLEstadoPagadosPrevius',\n",
    "        'TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroIntentosFallidos','CupoAprobado','ScoreCrediticio','Edad',\n",
    "        'MesesDesdeVinculacion','MesesDesdePrimerUso','DiasDesdeUltimoUso'\n",
    "    ]\n",
    "    for c in num_cols_candidates:\n",
    "        if c in df.columns:\n",
    "            df[f'Flag_{c}_NaN'] = df[c].isna().astype(int)\n",
    "            if c.startswith('NumeroCreditos') or c.startswith('TotalPagos'):\n",
    "                df[c] = df[c].fillna(0)\n",
    "            elif c in ['DiasMaximosMoraCreditosGenerados','Edad','NumeroIntentosFallidos']:\n",
    "                df[c] = df[c].fillna(0)\n",
    "\n",
    "    # --- Score & Cupo\n",
    "    df['ScoreSinInfo'] = (df['ScoreCrediticio'] == 0).astype(int)\n",
    "    if df['ScoreCrediticio'].isna().any():\n",
    "        med_pos = df.loc[df['ScoreCrediticio']>0, 'ScoreCrediticio'].median()\n",
    "        df['ScoreCrediticio'] = df['ScoreCrediticio'].fillna(med_pos)\n",
    "\n",
    "    df['log_CupoAprobado'] = np.log1p(df['CupoAprobado'])\n",
    "    df['log_CupoAprobado'] = df['log_CupoAprobado'].fillna(df['log_CupoAprobado'].median())\n",
    "\n",
    "    # --- Categóricas limpias\n",
    "    for c in ['CategoriaPrincipalCredito','UsoAppWeb','Genero','TipoMunicipioEntregaTC','CanalMunicipioEntregaTC']:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna('Desconocido')\n",
    "    df['Genero'] = df['Genero'].replace({27:'Desconocido'})\n",
    "    df['TipoMunicipioEntregaTC'] = df['TipoMunicipioEntregaTC'].replace({'PEQUEÃ‘O':'PEQUEÑO'}).fillna('Desconocido')\n",
    "\n",
    "    # --- Rango de Edad\n",
    "    df['Flag_Edad_Out'] = (~df['Edad'].between(18, 100, inclusive='both')).fillna(False).astype(int)\n",
    "    df.loc[df['Flag_Edad_Out']==1, 'Edad'] = np.nan\n",
    "    df['Edad'] = df['Edad'].fillna(df['Edad'].median())\n",
    "\n",
    "    # --- Features derivadas clave\n",
    "    if 'Flag_PrimerUsoTemu' not in df.columns and 'NumeroCreditosGPrevius' in df.columns:\n",
    "        df['Flag_PrimerUsoTemu'] = (df['NumeroCreditosGPrevius'] == 0).astype(int)\n",
    "\n",
    "    df['ratio_pagos_local_global'] = (\n",
    "        df['TotalPagosEfectuadosLocalmentePrevius'].fillna(0) /\n",
    "        (df['TotalPagosEfectuadosGlobalmentePrevius'].fillna(0) + 1.0)\n",
    "    )\n",
    "    df['creditos_activos_ratio'] = (\n",
    "        df['NumeroCreditosGEstadoActivosPrevius'].fillna(0) /\n",
    "        (df['NumeroCreditosGPrevius'].fillna(0) + 1.0)\n",
    "    )\n",
    "    for c in ['ratio_pagos_local_global','creditos_activos_ratio']:\n",
    "        df[c] = df[c].replace([np.inf,-np.inf], np.nan).fillna(0).clip(0,1)\n",
    "\n",
    "    df['Flag_CanalVirtual'] = (\n",
    "        (df['CanalMunicipioEntregaTC'].astype(str).str.lower()=='virtual') |\n",
    "        (df['TipoMunicipioEntregaTC'].astype(str).str.upper()=='VIRTUAL')\n",
    "    ).astype(int)\n",
    "\n",
    "    # --- Score negativo -> 0, más bucket\n",
    "    df['Flag_Score_Negativo'] = (df['ScoreCrediticio'] < 0).astype(int)\n",
    "    df.loc[df['ScoreCrediticio'] < 0, 'ScoreCrediticio'] = 0\n",
    "\n",
    "    df['ScoreBucket'] = 'sin_info'\n",
    "    mask_pos = df['ScoreCrediticio'] > 0\n",
    "    if mask_pos.sum() > 0:\n",
    "        b1, b2 = df.loc[mask_pos, 'ScoreCrediticio'].quantile([0.33, 0.66]).values\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] <= b1), 'ScoreBucket'] = 'bajo'\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] >  b1) & (df['ScoreCrediticio'] <= b2), 'ScoreBucket'] = 'medio'\n",
    "        df.loc[mask_pos & (df['ScoreCrediticio'] >  b2), 'ScoreBucket'] = 'alto'\n",
    "    df['ScoreBucket'] = pd.Categorical(df['ScoreBucket'], categories=['sin_info','bajo','medio','alto'], ordered=True)\n",
    "\n",
    "    # --- Winsorización p99 + flags (colas largas)\n",
    "    def cap_with_flag(s, upper):\n",
    "        flag = (s > upper).astype(int)\n",
    "        return np.where(s > upper, upper, s), flag\n",
    "\n",
    "    cap_cols = [\n",
    "        'DiasDesdeUltimoUso','MesesDesdeVinculacion',\n",
    "        'TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroCreditosGPrevius','NumeroCreditosGCanalFPrevius','NumeroCreditosGCanalVPrevius',\n",
    "        'NumeroCreditosGEstadoActivosPrevius','NumeroCreditosGEstadoPagadosPrevius',\n",
    "        'NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius','NumeroCreditosLEstadoPagadosPrevius'\n",
    "    ]\n",
    "    for c in cap_cols:\n",
    "        if c in df.columns:\n",
    "            p99 = df[c].quantile(0.99)\n",
    "            capped, flag = cap_with_flag(df[c].fillna(0), p99)\n",
    "            df[c] = capped\n",
    "            df[f'Flag_{c}_Capped'] = flag\n",
    "\n",
    "    # Agrupar categorías raras de CategoriaPrincipalCredito (<0.1%)\n",
    "    if 'CategoriaPrincipalCredito' in df.columns:\n",
    "        vc = df['CategoriaPrincipalCredito'].astype(str).value_counts(dropna=False)\n",
    "        cutoff = df.shape[0] * 0.001\n",
    "        rare_levels = vc[vc < cutoff].index\n",
    "        df['CategoriaPrincipalCredito'] = df['CategoriaPrincipalCredito'].astype(str)\n",
    "        df.loc[df['CategoriaPrincipalCredito'].isin(rare_levels), 'CategoriaPrincipalCredito'] = 'OtrosRare'\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_feature_lists(df: pd.DataFrame):\n",
    "    drop_from_features = [\n",
    "        'IdentificadorCliente','FechaEvento','FechaVinculacionCliente','FechaPrimerUso','FechaUltimoUso',\n",
    "        'FechaEvento_dt','FechaVinculacionCliente_dt','FechaPrimerUso_dt','FechaUltimoUso_dt','FechaPrimerUso_corr',\n",
    "        'CodigoAlmacenEntregaTC','CodigoAlmacenEntregaTC_str','AlmacenTop20',\n",
    "        'CodigoMunicipioEntregaTC','MunicipioCat','MunicipioTop20','MesCompra',\n",
    "        'DiasMora'\n",
    "    ]\n",
    "    num_base = [\n",
    "        'UsabilidadCupo','MesesDesdeVinculacion','MesesDesdePrimerUso','DiasDesdeUltimoUso',\n",
    "        'NumeroCreditosGPrevius','NumeroCreditosGCanalFPrevius','NumeroCreditosGCanalVPrevius',\n",
    "        'NumeroCreditosGEstadoActivosPrevius','NumeroCreditosGEstadoPagadosPrevius',\n",
    "        'NumeroCreditosLPrevius','NumeroCreditosLEstadoActivosPrevius','NumeroCreditosLEstadoPagadosPrevius',\n",
    "        'TotalPagosEfectuadosGlobalmentePrevius','TotalPagosEfectuadosLocalmentePrevius',\n",
    "        'NumeroIntentosFallidos','ScoreCrediticio','ScoreSinInfo','CupoAprobado','log_CupoAprobado','Edad',\n",
    "        'Flag_Usab_NaN','Flag_Usab_Outlier','Flag_PrimerUsoAntesVinc','Flag_UltimoUsoPosterior','Flag_Edad_Out',\n",
    "        'ratio_pagos_local_global','creditos_activos_ratio','Flag_Score_Negativo'\n",
    "    ]\n",
    "    num_dyn = [c for c in df.columns if c.startswith('Flag_') and (c.endswith('_NaN') or c.endswith('_Capped'))]\n",
    "    num_final = [c for c in (num_base + num_dyn) if c in df.columns and c not in drop_from_features]\n",
    "\n",
    "    cat_final = [\n",
    "        'Genero','TipoMunicipioEntregaTC','CanalMunicipioEntregaTC','UsoAppWeb','CategoriaPrincipalCredito',\n",
    "        'Flag_PrimerUsoTemu','ScoreBucket'\n",
    "    ]\n",
    "    cat_final = [c for c in cat_final if c in df.columns and c not in drop_from_features]\n",
    "\n",
    "    # --- Evitar solape entre num y cat + forzar unicidad y orden estable\n",
    "    overlap = set(num_final) & set(cat_final)\n",
    "    if overlap:\n",
    "        print(f\">>> Aviso: {len(overlap)} columnas estaban en num y cat. Se quitan de num: {sorted(overlap)}\")\n",
    "        num_final = [c for c in num_final if c not in overlap]\n",
    "\n",
    "    num_final = list(dict.fromkeys(num_final))\n",
    "    cat_final = list(dict.fromkeys(cat_final))\n",
    "    return num_final, cat_final\n",
    "\n",
    "\n",
    "def prune_low_variance(X: pd.DataFrame):\n",
    "    low_var = [c for c in X.columns if X[c].nunique(dropna=False) <= 1]\n",
    "    if low_var:\n",
    "        print(\">>> Poda de columnas casi-constantes:\", low_var)\n",
    "        return X.drop(columns=low_var, errors='ignore'), low_var\n",
    "    else:\n",
    "        print(\">>> Poda de columnas casi-constantes: ninguna\")\n",
    "        return X, []\n",
    "\n",
    "\n",
    "def build_pipelines(cat_cols, num_cols):\n",
    "    pre_logit = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('imp', SimpleImputer(strategy='most_frequent')),\n",
    "                ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "            ]), cat_cols),\n",
    "            ('num', Pipeline(steps=[\n",
    "                ('imp', SimpleImputer(strategy='median'))\n",
    "            ]), num_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    logit = Pipeline(steps=[\n",
    "        ('pre', pre_logit),\n",
    "        ('clf', LogisticRegression(\n",
    "            solver='saga', penalty='l2', class_weight='balanced',\n",
    "            max_iter=800, n_jobs=-1, random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    pre_hgb = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), cat_cols),\n",
    "            ('num', 'passthrough', num_cols),\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    hgb = Pipeline(steps=[\n",
    "        ('pre', pre_hgb),\n",
    "        ('clf', HistGradientBoostingClassifier(\n",
    "            learning_rate=0.07, max_leaf_nodes=31, l2_regularization=1.0,\n",
    "            random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ])\n",
    "    return logit, hgb\n",
    "\n",
    "\n",
    "def evaluate_proba(y_true, y_proba, name=\"model\"):\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    ap  = average_precision_score(y_true, y_proba)\n",
    "    br  = brier_score_loss(y_true, y_proba)\n",
    "    print(f\"[{name}] ROC-AUC={auc:.3f} | PR-AUC={ap:.3f} | Brier={br:.3f}\")\n",
    "    p, r, thr = precision_recall_curve(y_true, y_proba)\n",
    "    f1 = 2*p*r/(p+r+1e-12)\n",
    "    j  = np.argmax(f1)\n",
    "    best_thr = thr[j-1] if j>0 and j-1 < len(thr) else 0.5\n",
    "    y_hat = (y_proba >= best_thr).astype(int)\n",
    "    print(f\"[{name}] best-F1={f1[j]:.3f} @ thr={best_thr:.3f}\")\n",
    "    print(f\"[{name}] ConfMatrix @thr={best_thr:.3f}:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    print(f\"[{name}] Report @thr={best_thr:.3f}:\\n\", classification_report(y_true, y_hat, digits=3))\n",
    "    return dict(roc_auc=auc, pr_auc=ap, brier=br, thr=best_thr)\n",
    "\n",
    "\n",
    "def operating_points(y_true, y_proba, name=\"model\"):\n",
    "    p, r, thr = precision_recall_curve(y_true, y_proba)\n",
    "    f1 = 2*p*r/(p+r+1e-12)\n",
    "    pts = {}\n",
    "    # máx F1\n",
    "    j = np.argmax(f1); pts['maxF1'] = (thr[j-1] if j>0 else 0.5, p[j], r[j], f1[j])\n",
    "    # precisión >= 0.6\n",
    "    idx = np.where(p>=0.6)[0]\n",
    "    if len(idx)>0:\n",
    "        k = idx[np.argmax(r[idx])]\n",
    "        pts['prec>=0.6'] = (thr[k-1] if k>0 else 0.5, p[k], r[k], f1[k])\n",
    "    # recall >= 0.7\n",
    "    idx = np.where(r>=0.7)[0]\n",
    "    if len(idx)>0:\n",
    "        k = idx[np.argmax(p[idx])]\n",
    "        pts['rec>=0.7'] = (thr[k-1] if k>0 else 0.5, p[k], r[k], f1[k])\n",
    "\n",
    "    print(f\"\\n[{name}] Puntos de operación sugeridos:\")\n",
    "    for kk,(t,pp,rr,ff) in pts.items():\n",
    "        print(f\" - {kk:10s}: thr={t:.3f} | precision={pp:.3f} | recall={rr:.3f} | F1={ff:.3f}\")\n",
    "    return pts\n",
    "\n",
    "\n",
    "def walk_forward_cv(df, X, y, cat_cols, num_cols, cut_fracs=(0.6,0.7,0.8)):\n",
    "    \"\"\"Valida SIN FUGA con 3 cortes temporales (ajustable).\"\"\"\n",
    "    cutoffs = df['FechaEvento_dt'].quantile(list(cut_fracs)).values\n",
    "    rows_l, rows_h = [], []\n",
    "    logit, hgb = build_pipelines(cat_cols, num_cols)\n",
    "\n",
    "    print(\"\\n>>> Walk-forward CV (sin fuga):\")\n",
    "    for i, c in enumerate(cutoffs, 1):\n",
    "        tr_idx = df['FechaEvento_dt'] <= c\n",
    "        va_idx = df['FechaEvento_dt'] >  c\n",
    "        X_tr, X_va = X.loc[tr_idx], X.loc[va_idx]\n",
    "        y_tr, y_va = y.loc[tr_idx], y.loc[va_idx]\n",
    "        print(f\"  - Split {i}: train={X_tr.shape}, valid={X_va.shape}, cutoff={pd.Timestamp(c).date()}\")\n",
    "\n",
    "        # Logit + calibración\n",
    "        logit.fit(X_tr, y_tr)\n",
    "        cal_logit = CalibratedClassifierCV(logit, method='sigmoid', cv='prefit')\n",
    "        cal_logit.fit(X_va, y_va)\n",
    "        proba_l = cal_logit.predict_proba(X_va)[:,1]\n",
    "        m_l = evaluate_proba(y_va, proba_l, name=f\"Logit+Cal (WF{i})\")\n",
    "        rows_l.append({\"cutoff\": c, **m_l})\n",
    "\n",
    "        # HGB + sample_weight + calibración\n",
    "        sw = compute_sample_weight(\"balanced\", y_tr)\n",
    "        hgb.fit(X_tr, y_tr, clf__sample_weight=sw)\n",
    "        cal_hgb = CalibratedClassifierCV(hgb, method='isotonic', cv='prefit')\n",
    "        cal_hgb.fit(X_va, y_va)\n",
    "        proba_h = cal_hgb.predict_proba(X_va)[:,1]\n",
    "        m_h = evaluate_proba(y_va, proba_h, name=f\"HGB+Cal (WF{i})\")\n",
    "        rows_h.append({\"cutoff\": c, **m_h})\n",
    "\n",
    "    return pd.DataFrame(rows_l), pd.DataFrame(rows_h)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) CONSTRUIR DATASET LIMPIO + LISTAS DE FEATURES\n",
    "# ============================================================\n",
    "print(\"\\n>>> Construyendo features (sin fuga) ...\")\n",
    "clientes = parse_and_features(clientes)\n",
    "num_final, cat_final = build_feature_lists(clientes)\n",
    "\n",
    "X = clientes[num_final + cat_final].copy()\n",
    "y = clientes[TARGET].astype(int).copy()\n",
    "\n",
    "# --- FIX CRÍTICO: ELIMINAR COLUMNAS DUPLICADAS EN X ---\n",
    "dups_mask = X.columns.duplicated(keep='first')\n",
    "if dups_mask.any():\n",
    "    dups = pd.Series(X.columns)[dups_mask].tolist()\n",
    "    print(f\">>> Columnas duplicadas detectadas y removidas ({len(dups)}): {dups}\")\n",
    "    X = X.loc[:, ~X.columns.duplicated(keep='first')]\n",
    "else:\n",
    "    print(\">>> Sin columnas duplicadas en X.\")\n",
    "\n",
    "X, dropped_lowvar = prune_low_variance(X)\n",
    "\n",
    "print(\"\\n=== Resumen columnas finales (previas al modelado) ===\")\n",
    "print(f\"Numéricas: {len([c for c in X.columns if c in num_final])} | Categóricas: {len([c for c in X.columns if c in cat_final])}\")\n",
    "print(\"X shape:\", X.shape, \"| y rate (1):\", y.mean().round(3))\n",
    "print(\"FechaEvento_dt rango:\", str(clientes['FechaEvento_dt'].min().date()), \"→\", str(clientes['FechaEvento_dt'].max().date()))\n",
    "\n",
    "# ============================================================\n",
    "# 4) WALK-FORWARD CV — SIN MIRAR EL FUTURO\n",
    "# ============================================================\n",
    "cat_cols = [c for c in cat_final if c in X.columns]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "logit_cv, hgb_cv = walk_forward_cv(clientes, X, y, cat_cols, num_cols)\n",
    "\n",
    "print(\"\\nLogit — Walk-forward CV:\")\n",
    "print(logit_cv)\n",
    "print(f\"Logit — medias: ROC-AUC={logit_cv.roc_auc.mean():.3f} | PR-AUC={logit_cv.pr_auc.mean():.3f} | Brier={logit_cv.brier.mean():.3f}\")\n",
    "\n",
    "print(\"\\nHGB — Walk-forward CV:\")\n",
    "print(hgb_cv)\n",
    "print(f\"HGB — medias:   ROC-AUC={hgb_cv.roc_auc.mean():.3f} | PR-AUC={hgb_cv.pr_auc.mean():.3f} | Brier={hgb_cv.brier.mean():.3f}\")\n",
    "\n",
    "chosen = \"HGB\" if hgb_cv.pr_auc.mean() >= logit_cv.pr_auc.mean() else \"Logit\"\n",
    "print(f\"\\n>>> Modelo elegido por PR-AUC medio: {chosen}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) HOLDOUT FINAL (FUTURO) — 20% MÁS RECIENTE\n",
    "# ============================================================\n",
    "cutoff = clientes['FechaEvento_dt'].quantile(0.8)\n",
    "train_idx = clientes['FechaEvento_dt'] <= cutoff\n",
    "hold_idx  = clientes['FechaEvento_dt'] >  cutoff\n",
    "X_tr, X_ho = X.loc[train_idx], X.loc[hold_idx]\n",
    "y_tr, y_ho = y.loc[train_idx], y.loc[hold_idx]\n",
    "print(f\"\\n>>> Holdout temporal: train={X_tr.shape}, holdout={X_ho.shape}, corte={pd.Timestamp(cutoff).date()}\")\n",
    "\n",
    "logit, hgb = build_pipelines(cat_cols, num_cols)\n",
    "\n",
    "# LOGIT + calibración\n",
    "logit.fit(X_tr, y_tr)\n",
    "cal_logit = CalibratedClassifierCV(logit, method='sigmoid', cv='prefit')\n",
    "cal_logit.fit(X_ho, y_ho)\n",
    "proba_logit = cal_logit.predict_proba(X_ho)[:,1]\n",
    "_ = evaluate_proba(y_ho, proba_logit, name=\"HOLDOUT-Logit\")\n",
    "operating_points(y_ho, proba_logit, name=\"HOLDOUT-Logit\")\n",
    "\n",
    "# HGB + sample_weight + calibración\n",
    "sw = compute_sample_weight(\"balanced\", y_tr)\n",
    "hgb.fit(X_tr, y_tr, clf__sample_weight=sw)\n",
    "cal_hgb = CalibratedClassifierCV(hgb, method='isotonic', cv='prefit')\n",
    "cal_hgb.fit(X_ho, y_ho)\n",
    "proba_hgb = cal_hgb.predict_proba(X_ho)[:,1]\n",
    "m_hold = evaluate_proba(y_ho, proba_hgb, name=\"HOLDOUT-HGB\")\n",
    "operating_points(y_ho, proba_hgb, name=\"HOLDOUT-HGB\")\n",
    "\n",
    "# Guardar bundle de artefactos\n",
    "bundle = {\n",
    "    \"model_calibrado\": cal_hgb if chosen==\"HGB\" else cal_logit,\n",
    "    \"chosen\": chosen,\n",
    "    \"num_final\": num_final,\n",
    "    \"cat_final\": cat_final,\n",
    "    \"columns_after_prune\": list(X.columns),\n",
    "    \"cutoff\": cutoff\n",
    "}\n",
    "joblib.dump(bundle, ARTIF_DIR/\"modelo_calibrado.joblib\")\n",
    "print(f\"\\n>>> Artefactos guardados en {ARTIF_DIR/'modelo_calibrado.joblib'}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) (OPCIONAL) CALIBRACIÓN POR DECILES EN HOLDOUT\n",
    "# ============================================================\n",
    "def decile_calibration(y_true, y_proba, name=\"holdout\"):\n",
    "    bins = pd.qcut(y_proba, q=10, duplicates='drop')\n",
    "    tab = pd.DataFrame({\"p\":y_proba, \"y\":y_true}).groupby(bins, observed=True).agg(\n",
    "        p_mean=('p','mean'),\n",
    "        y_rate=('y','mean'),\n",
    "        n=('p','size')\n",
    "    ).reset_index().rename(columns={\"p\":\"bin\"})\n",
    "    print(f\"\\n[{name}] Calibración por deciles (p_mean vs y_rate):\")\n",
    "    print(tab.to_string(index=False))\n",
    "    return tab\n",
    "\n",
    "_ = decile_calibration(y_ho, proba_hgb, name=\"HOLDOUT-HGB\")\n",
    "\n",
    "# ============================================================\n",
    "# 7) (OPCIONAL) FUNCIÓN DE SCORING FUTURO\n",
    "# ============================================================\n",
    "def score_future(df_nuevo_raw: pd.DataFrame, artif_path=ARTIF_DIR/\"modelo_calibrado.joblib\", target_col=TARGET):\n",
    "    \"\"\"Aplica el mismo pipeline a df_nuevo. Si trae label, evalúa; si no, solo predice.\"\"\"\n",
    "    bundle = joblib.load(artif_path)\n",
    "    model  = bundle[\"model_calibrado\"]\n",
    "    cols_ok = bundle[\"columns_after_prune\"]\n",
    "    num_final, cat_final = bundle[\"num_final\"], bundle[\"cat_final\"]\n",
    "\n",
    "    df_nuevo = parse_and_features(df_nuevo_raw)\n",
    "    Xn = df_nuevo[num_final + cat_final].copy()\n",
    "    # Alinear columnas esperadas + deduplicar por si acaso\n",
    "    Xn = Xn.reindex(columns=cols_ok, fill_value=np.nan)\n",
    "    Xn = Xn.loc[:, ~Xn.columns.duplicated(keep='first')]\n",
    "    proba = model.predict_proba(Xn)[:,1]\n",
    "\n",
    "    if target_col in df_nuevo.columns:\n",
    "        y_true = df_nuevo[target_col].astype(int)\n",
    "        _ = evaluate_proba(y_true, proba, name=\"DF_NUEVO\")\n",
    "        operating_points(y_true, proba, name=\"DF_NUEVO\")\n",
    "        _ = decile_calibration(y_true, proba, name=\"DF_NUEVO\")\n",
    "    else:\n",
    "        print(\"df_nuevo sin target: se devuelven solo probabilidades.\")\n",
    "    out = pd.DataFrame({\"proba_perdida\": proba})\n",
    "    out_path = ARTIF_DIR/\"scoring_df_nuevo.csv\"\n",
    "    out.to_csv(out_path, index=False)\n",
    "    print(f\"Scoring exportado a {out_path}\")\n",
    "    return out\n",
    "\n",
    "# ------------- QUICK TEST -------------\n",
    "# Simular \"futuro real\" con el 20% más reciente (ya evaluado arriba):\n",
    "# df_nuevo = clientes.loc[clientes['FechaEvento_dt'] > cutoff].copy()\n",
    "# _ = score_future(df_nuevo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d5a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44eaf11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
